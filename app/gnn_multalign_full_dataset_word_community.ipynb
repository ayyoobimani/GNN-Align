{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('multalign_graph': conda)"
  },
  "interpreter": {
   "hash": "e87ea973a27bc0df729a3cffeb7c69a8289035b60e563cd26286fedfe440ad07"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch, sys\n",
    "sys.path.insert(0, '../')\n",
    "from my_utils import gpu_utils\n",
    "import importlib, gc\n",
    "from my_utils.alignment_features import *\n",
    "import my_utils.alignment_features as afeatures\n",
    "importlib.reload(afeatures)\n",
    "import gnn_utils.graph_utils as gutils"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/mounts/Users/student/ayyoob/anaconda3/envs/multalign_graph/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# !pip install torch-geometric\n",
    "# !pip install tensorboardX\n",
    "\n",
    "# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "# !unzip ngrok-stable-linux-amd64.zip\n",
    "\n",
    "#  print(torch.version.cuda)\n",
    "#  print(torch.__version__)    \n",
    "\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from my_utils import align_utils as autils, utils\n",
    "import argparse\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "\n",
    "# set random seed\n",
    "config_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc-ui-demo/config_pbc.ini\"\n",
    "utils.setup(config_file)\n",
    "\n",
    "params = argparse.Namespace()\n",
    "\n",
    "\n",
    "params.gold_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-grc-fin-gold-alignments_train.txt\"\n",
    "pros, surs = autils.load_gold(params.gold_file)\n",
    "all_verses = list(pros.keys())\n",
    "params.gold_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-heb-fin-gold-alignments_train.txt\"\n",
    "pros, surs = autils.load_gold(params.gold_file)\n",
    "all_verses.extend(list(pros.keys()))\n",
    "all_verses = list(set(all_verses))\n",
    "print(len(all_verses))\n",
    "\n",
    "params.editions_file =  \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi_lang_list.txt\"\n",
    "editions, langs = autils.load_simalign_editions(params.editions_file)\n",
    "current_editions = [editions[lang] for lang in langs]\n",
    "\n",
    "def get_pruned_verse_alignments(args):\n",
    "    verse, current_editions = args\n",
    "    \n",
    "    verse_aligns_inter = autils.get_verse_alignments(verse)\n",
    "    verse_aligns_gdfa = autils.get_verse_alignments(verse, gdfa=True)\n",
    "\n",
    "    autils.prune_non_necessary_alignments(verse_aligns_inter, current_editions)\n",
    "    autils.prune_non_necessary_alignments(verse_aligns_gdfa, current_editions)\n",
    "\n",
    "    gc.collect()\n",
    "    return verse_aligns_inter, verse_aligns_gdfa\n",
    "    \n",
    "\n",
    "verse_alignments_inter = {}\n",
    "verse_alignments_gdfa = {}\n",
    "args = []\n",
    "for i,verse in enumerate(all_verses):\n",
    "    args.append((verse, current_editions[:]))\n",
    "\n",
    "#print('going to get alignments')\n",
    "#with Pool(20) as p:\n",
    "#    all_res = p.map(get_pruned_verse_alignments, args)\n",
    "\n",
    "#for i,verse in enumerate(all_verses):\n",
    "#    verse_aligns_inter, verse_aligns_gdfa = all_res[i]\n",
    "    \n",
    "    #verse_alignments_inter[verse] = verse_aligns_inter\n",
    "    #verse_alignments_gdfa[verse] = verse_aligns_gdfa\n",
    "\n",
    "#utils.LOG.info(\"done reading alignments\")\n",
    "#torch.save(verse_alignments_inter, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_train_inter.pickle\")\n",
    "#torch.save(verse_alignments_gdfa, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_train_gdfa.pickle\")\n",
    "#utils.LOG.info('done saving pruned alignments')\n",
    "\n",
    "print('reading inter verse alignments')\n",
    "verse_alignments_inter = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_train_inter_8000.pickle\")\n",
    "gc.collect()\n",
    "print('done reading inter verse alignments')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "24159\n",
      "reading inter verse alignments\n",
      "done reading inter verse alignments\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_channels, hidden_channels)\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.lin3(x)\n",
    "        return x\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#importlib.reload(afeatures)\n",
    "class Encoder2(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Encoder2, self).__init__()\n",
    "        self.conv1 = pyg_nn.GATConv(in_channels, 2*out_channels)\n",
    "        self.conv2 = pyg_nn.GATConv(2 * out_channels , out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.elu(self.conv1(x, edge_index, ))\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features, n_head = 2, edge_feature_dim = 0,):\n",
    "        super(Encoder, self).__init__()\n",
    "        #self.lin = nn.Linear(in_channels, out_channels)\n",
    "        self.conv1 = pyg_nn.GATConv(in_channels, 2*out_channels, heads= n_head)\n",
    "        self.conv2 = pyg_nn.GATConv(2 * n_head *  out_channels , out_channels, heads= 1)\n",
    "        #self.conv3 = pyg_nn.GATConv(2 * n_head *  out_channels , out_channels, heads= n_head)\n",
    "        #self.f_embedding = nn.Linear(in_channels, in_channels)\n",
    "        self.fin_lin = nn.Linear(out_channels, out_channels)\n",
    "        \n",
    "\n",
    "        self.feature_encoder = afeatures.FeatureEncoding(features, word_vectors)\n",
    "        #self.already_inited = False\n",
    "        #self.prev_edge_index = None\n",
    "        #self.prev_edge_attr = None\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.feature_encoder(x, dev)\n",
    "        #x = F.relu(self.f_embedding(x))\n",
    "        #if not self.already_inited or self.prev_edge_index.data_ptr() != edge_index.data_ptr():\n",
    "        #    edge_index_np = edge_index.cpu().numpy()\n",
    "        #    val_indices = x_edge_np[edge_index_np[0, :], edge_index_np[1, :]]\n",
    "        #    vals = x_edge_vals[val_indices, :]\n",
    "        #    vals = vals.reshape((vals.shape[1], vals.shape[2]))\n",
    "        #    self.prev_edge_attr = vals.to(dev)\n",
    "        #    self.prev_edge_index = edge_index\n",
    "        #    self.already_inited = True\n",
    "        #x = self.lin(x)\n",
    "        x = F.elu(self.conv1(x, edge_index, ))\n",
    "        #x = self.conv_gin(x, edge_index)\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        return F.relu(self.fin_lin(x))#, self.conv3(x, edge_index)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def clean_memory():\n",
    "    gc.collect()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def train(epoch):\n",
    "    global optimizer\n",
    "    total_loss = 0\n",
    "    cluster_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    #for i in tqdm(range(int(train_pos_edge_index_permed.shape[1]/batch_size)+1)):\n",
    "    for i,batch_ in enumerate(tqdm(data_loader)):\n",
    "        for verse in batch_:\n",
    "            if verse in masked_verses:\n",
    "                continue\n",
    "            batch = batch_[verse]\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            x = batch['x'].to(dev)\n",
    "            edge_index = batch['edge_index'].to(dev)\n",
    "            if torch.max(edge_index) >= x.shape[0]:\n",
    "                print(torch.max(edge_index), x.shape)\n",
    "                print(batch)\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                z = model.encode(x, edge_index)\n",
    "            except Exception as e:\n",
    "                global sag, khar, gav\n",
    "                sag, khar, gav =  (i, batch_, verse)\n",
    "                print(e)\n",
    "                1/0\n",
    "            #z1 = encoder2(z, torch.tensor(batch['intra_sent_edges'], dtype=torch.long).to(dev))\n",
    "            #z = torch.cat((z,z1), dim=1)\n",
    "            #for j in range(5):\n",
    "            #    discriminator_optimizer.zero_grad()\n",
    "            #    discriminator_loss = model.discriminator_loss(z) / (int(train_pos_edge_index_permed.shape[1]/batch_size)+1)\n",
    "            #    discriminator_loss.backward()\n",
    "            #    discriminator_optimizer.step()\n",
    "            \n",
    "            pos = torch.tensor(batch['pos'], dtype=torch.long).to(dev)\n",
    "            neg = torch.tensor(batch['neg'], dtype=torch.long).to(dev)\n",
    "            #nodes = torch.tensor(list(batch['nodes']), dtype=torch.long).to(dev)\n",
    "\n",
    "            loss1 = model.recon_loss( z, pos, neg) #TODO try providing better neg edges\n",
    "            #ortho_loss, mincut_loss, entropy_loss = model.decoder.clustering_loss(z, nodes, batch['adjacency'])\n",
    "            \n",
    "            loss =   loss1 * pos.shape[1] #+ ortho_loss + mincut_loss #+ 0.05 * entropy_loss #* pos.shape[1]/train_neg_edge_index.shape[1] #+ model.reg_loss(z)/(int(train_pos_edge_index_permed.shape[1]/batch_size)+1)# + (1 / x.shape[0]) * model.kl_loss()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() \n",
    "            cluster_loss += loss1\n",
    "\n",
    "            \n",
    "            if i  % 10000 == 9999:\n",
    "                #alignment_test(epoch, test_dataset.edge_index, editf1, editf2, test_verses, test_nodes_map,\n",
    "                #        dev, model, x_test, pros, surs, verse_alignments_inter, verse_alignments_gdfa, writer, gnn_dataset.verse_info)\n",
    "\n",
    "                clean_memory()\n",
    "                \n",
    "                eval_utils.alignment_test(epoch, grc_test_dataset.edge_index, editf_fin, editf_grc, grc_test_verses[:], grc_test_dataset.nodes_map,\n",
    "                        dev, model, grc_test_dataset.x, pros_grc, surs_grc, grc_test_verse_alignments_inter, grc_test_verse_alignments_gdfa, writer, gnn_dataset_grc.verse_info)\n",
    "\n",
    "                eval_utils.alignment_test(epoch, heb_test_dataset.edge_index, editf_fin, editf_heb, heb_test_verses[:], heb_test_dataset.nodes_map,\n",
    "                                        dev, model, heb_test_dataset.x, pros_heb, surs_heb, heb_test_verse_alignments_inter, heb_test_verse_alignments_gdfa, writer, gnn_dataset_heb.verse_info)\n",
    "\n",
    "                eval_utils.alignment_test(epoch, blinker_test_dataset.edge_index, editf12, editf22, blinker_verses, blinker_test_dataset.nodes_map,\n",
    "                            dev, model, blinker_test_dataset.x, pros_blinker, surs_blinker, blinker_verse_alignments_inter, blinker_verse_alignments_gdfa, writer, gnn_dataset_blinker.verse_info)\n",
    "                \n",
    "                clean_memory()   \n",
    "                # decoder.set_objective('sequence_prediction')\n",
    "                # auc, ap = test(edge_index_seq_sent, edge_index_seq_sent_neg, epoch)\n",
    "                # print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))\n",
    "                # if epoch > 4:\n",
    "                #     decoder.set_objective('link_prediction')\n",
    "                model.train()\n",
    "            \n",
    "            #if (i+1)*batch_size > train_pos_edge_index.shape[1]:\n",
    "            #    break\n",
    "\n",
    "            #if i % 51 == 0:\n",
    "            #    clean_memory\n",
    "    \n",
    "    writer.add_scalar(\"loss\", total_loss, epoch)\n",
    "    print(f\"train loss: {total_loss}\")\n",
    "    print(f\"cluster loss: {cluster_loss}\")\n",
    "\n",
    "def test(pos_edge_index, neg_edge_index, epoch):\n",
    "    model.eval()\n",
    "    tot_auc = tot_ap = 0\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(x_test, torch.cat((train_pos_edge_index, neg_edge_index), dim=1).to(dev))\n",
    "        \n",
    "        neg_pos_coeff = neg_edge_index.shape[1]/ pos_edge_index.shape[1]\n",
    "        for i in (range(int(pos_edge_index.shape[1]/batch_size)+1)):\n",
    "            auc,ap = model.test(z, pos_edge_index[:, i*batch_size:(i+1)*batch_size].to(dev),\n",
    "                                neg_edge_index[:, int(i*batch_size*neg_pos_coeff):int((i+1)*batch_size*neg_pos_coeff)].to(dev))\n",
    "\n",
    "            tot_auc += auc * pos_edge_index[:, i*batch_size:(i+1)*batch_size].shape[1]\n",
    "            tot_ap += ap *  pos_edge_index[:, i*batch_size:(i+1)*batch_size].shape[1]\n",
    "\n",
    "\n",
    "    return tot_auc/pos_edge_index.shape[1], tot_ap/pos_edge_index.shape[1]\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "EPS = 1e-15\n",
    "\n",
    "def _diag(x):\n",
    "    eye = torch.eye(x.size(0)).type_as(x)\n",
    "    out = eye * x.unsqueeze(1).expand(x.size(0), x.size(0))\n",
    "    return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, edge_features, n_cluster=32):\n",
    "        super(Decoder, self).__init__()\n",
    "        #self.feature_encoder = afeatures.FeatureEncoding(edge_features)\n",
    "        self.features_size = sum([x.out_dim for x in edge_features])\n",
    "        self.representataion_size = (input_size - self.features_size)\n",
    "\n",
    "        self.transfer = nn.Sequential(nn.Linear(input_size, hidden_size*2), nn.ReLU(), nn.Dropout(drop_out),\n",
    "                            #nn.Linear(hidden_size*2, hidden_size), nn.ReLU(), nn.Dropout(drop_out),\n",
    "                            nn.Linear(hidden_size*2, 1))\n",
    "\n",
    "        #self.transfer = nn.Sequential(nn.ELU(), nn.Linear(n_cluster*2, 1), nn.ELU())\n",
    "\n",
    "        #self.n_cluster = n_cluster                \n",
    "        #self.cluster = nn.Sequential(nn.Linear(int((input_size - len(edge_features))/2), hidden_size*2), nn.ELU(), nn.Linear(hidden_size*2, 2*n_cluster))\n",
    "        #self.actual_cluster = nn.Linear(2*n_cluster, n_cluster)\n",
    "        #self.cos = nn.CosineSimilarity(dim=1)        \n",
    "        #self.dist = nn.PairwiseDistance()\n",
    "        #self.gnn_transform = nn.Sequential(nn.Linear(self.representataion_size, hidden_size), nn.ReLU(), nn.Dropout(drop_out))\n",
    "        self.counter = 0\n",
    "\n",
    "        self.objective = 'link_prediction'\n",
    "    def forward(self, z, edge_index, sigmoid = True):\n",
    "        if self.features_size > 0:\n",
    "            if self.objective == 'link_prediction':\n",
    "                edge_index_np = edge_index.cpu().numpy()\n",
    "                val_indices = x_edge_np[edge_index_np[0, :], edge_index_np[1, :]]\n",
    "                val_indices = np.squeeze(np.asarray(val_indices))\n",
    "                vals = x_edge_vals2[val_indices, :]\n",
    "            elif self.objective == 'sequence_prediction':\n",
    "                vals = torch.zeros((edge_index.shape[1], self.features_size)).to(dev)\n",
    "\n",
    "\n",
    "            features = self.feature_encoder(vals.to(dev), dev)\n",
    "            #features = vals.to(dev)\n",
    "            h1 = z[edge_index[0, :]]\n",
    "            h2 = z[edge_index[1, :]]\n",
    "\n",
    "            self.counter += 1\n",
    "\n",
    "            #rep = self.gnn_transform(torch.cat((h1, h2), dim=1))\n",
    "            res = self.transfer(torch.cat((self.cluster(h1), self.cluster(h2), features), dim=1))\n",
    "            #res = self.transfer(features)\n",
    "        else:\n",
    "            h1 = z[edge_index[0, :]]\n",
    "            h2 = z[edge_index[1, :]]\n",
    "\n",
    "            \n",
    "            res = self.transfer(torch.cat((h1, h2), dim=-1))\n",
    "\n",
    "            #res = self.transfer(torch.cat((self.cluster(h1), self.cluster(h2)), dim=1))\n",
    "            #res = torch.sum(torch.pow(F.softmax(self.cluster(h1)/1, dim=1) - F.softmax(self.cluster(h2)/1, dim=1), 2), dim=1)\n",
    "            #res = self.cos(self.cluster(h1), self.cluster(h2))\n",
    "            #res = - self.dist(self.cluster(h1), self.cluster(h2))\n",
    "            #print(res)\n",
    "        res = torch.sigmoid(res) if sigmoid else res\n",
    "        return res\n",
    "\n",
    "    def set_objective(self, objective):\n",
    "        self.objective = objective\n",
    "        \n",
    "    def clustering_loss(self, z, nodes, adjacency):\n",
    "        s = self.actual_cluster(torch.relu(self.cluster(z[nodes])))\n",
    "        s = torch.softmax(s, dim=-1)\n",
    "        entropy_loss = (-s * torch.log(s + EPS)).sum(dim=-1).mean()\n",
    "\n",
    "        ss = torch.matmul(s.transpose(0, 1), s)\n",
    "        i_s = torch.eye(self.n_cluster).type_as(ss)\n",
    "        ortho_loss = torch.norm(\n",
    "            ss / torch.norm(ss, dim=(-1, -2), keepdim=True) -\n",
    "            i_s / torch.norm(i_s), dim=(-1, -2))\n",
    "        ortho_loss = torch.mean(ortho_loss)\n",
    "\n",
    "        adjacency = adjacency.to(dev).float()\n",
    "        out_adj = torch.matmul(s.transpose(0, 1),torch.sparse.mm(adjacency, s))\n",
    "        # MinCUT regularization.\n",
    "        mincut_num = torch.trace(out_adj)\n",
    "        #d_flat = torch.einsum('ij->i', adjacency) # FIXME since I don't consider the whole adjacency matrix this could be a source of problem\n",
    "        d_flat = torch.sparse.sum(adjacency, dim=1).to_dense()\n",
    "        d = _diag(d_flat)\n",
    "        mincut_den = torch.trace(\n",
    "            torch.matmul(torch.matmul(s.transpose(0, 1), d), s))\n",
    "        mincut_loss = -(mincut_num / mincut_den)\n",
    "        mincut_loss = torch.mean(mincut_loss)\n",
    "\n",
    "        return ortho_loss, mincut_loss, entropy_loss\n",
    "    \n",
    "    def get_alignments(self, z, edge_index):\n",
    "        h1 = z[edge_index[0, :]]\n",
    "        h2 = z[edge_index[1, :]]\n",
    "        \n",
    "        h1 = torch.softmax(self.cluster(h1), dim=1)\n",
    "        h2 = torch.softmax(self.cluster(h2), dim=1)\n",
    "\n",
    "        h1_max = torch.argmax(h1, dim=1)\n",
    "        h2_max = torch.argmax(h2, dim=1)\n",
    "\n",
    "        h1_cluster = torch.zeros(*h1.shape)\n",
    "        h2_cluster = torch.zeros(*h2.shape)\n",
    "\n",
    "        h1_cluster[range(h1.size(0)), h1_max] = 1\n",
    "        h2_cluster[range(h2.size(0)), h2_max] = 1\n",
    "        res = torch.max(h1_cluster * h2_cluster, dim=1).values\n",
    "\n",
    "        #res = h1 * h2\n",
    "        #res = torch.sum(res, dim = 1)\n",
    "        return torch.unsqueeze(res, dim=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import pickle\n",
    "\n",
    "train_verses = all_verses[:]\n",
    "test_verses = all_verses[:] \n",
    "editf1 = 'fin-x-bible-helfi'\n",
    "editf2 = \"heb-x-bible-helfi\"\n",
    "\n",
    "\n",
    "if 'jpn-x-bible-newworld' in  current_editions[:]:\n",
    "     current_editions.remove('jpn-x-bible-newworld')\n",
    "if 'grc-x-bible-unaccented' in  current_editions[:]:\n",
    "     current_editions.remove('grc-x-bible-unaccented')\n",
    "\n",
    "train_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_helfi_train_community_word_8000.pickle\", map_location=torch.device('cpu'))\n",
    "#train_dataset, train_nodes_map = create_dataset(train_verses, verse_alignments_inter, small_editions)\n",
    "features = train_dataset.features\n",
    "train_nodes_map = train_dataset.nodes_map\n",
    "#edge_index_intra_sent = train_dataset.edge_index_intra_sent\n",
    "#test_edge_index_intra_sent = edge_index_intra_sent\n",
    "\n",
    "# test_dataset, test_nodes_map = create_dataset(test_verses, verse_alignments_inter, small_editions)\n",
    "test_dataset, test_nodes_map = train_dataset, train_nodes_map\n",
    "test_verses = train_verses\n",
    "print(train_dataset.x.shape)\n",
    "\n",
    "# gutils.augment_features(test_dataset)\n",
    "# x_edge, features_edge = gutils.create_edge_attribs(train_nodes_map, train_verses, small_editions, verse_alignments_inter, train_dataset.x.shape[0])\n",
    "# with open(\"./dataset.pickle\", 'wb') as of:\n",
    "#     pickle.dump(train_dataset, of)\n",
    "gc.collect()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10331043, 10])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec.load(\"/mounts/work/ayyoob/models/w2v/word2vec_helfi_langs_15e.model\")\n",
    "\n",
    "print(w2v_model.wv.vectors.shape)\n",
    "\n",
    "word_vectors = torch.from_numpy(w2v_model.wv.vectors).float()\n",
    "\n",
    "print(word_vectors.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2354770, 100)\n",
      "torch.Size([2354770, 100])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# edges_intra_sent, edges_seq_sent = get_inter_sentence_connections(train_dataset.nodes_map)\n",
    "# edge_index_seq_sent = torch.tensor(edges_seq_sent, dtype=torch.long)\n",
    "# train_dataset.edge_index_seq_sent = edge_index_seq_sent\n",
    "# torch.cuda.set_device(int(free_gpu1))\n",
    "# edge_index_intra_sent = torch.tensor(edges_intra_sent, dtype=torch.long).to(dev)\n",
    "# train_dataset.edge_index_intra_sent = edge_index_intra_sent\n",
    "# test_edge_index_intra_sent = train_dataset.edge_index_intra_sent\n",
    "# print(train_dataset.edge_index_intra_sent.shape)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import pickle\n",
    "import torch\n",
    "## with open(\"./features_edge.pickle\", 'wb') as of:\n",
    "##     pickle.dump(features_edge, of)\n",
    "\n",
    "## print('done first')\n",
    "## with open(\"/mounts/work/ayyoob/models/gnn//x_edge.pickle\", 'wb') as of:\n",
    "##     pickle.dump(x_edge, of)\n",
    "\n",
    "#with open(\"./features_edge.pickle\", 'rb') as inf:\n",
    "#    features_edge = pickle.load(inf)\n",
    "\n",
    "\n",
    "## indices = [[],[]]\n",
    "## values = []\n",
    "\n",
    "## print('going to create sparse matrix representation')\n",
    "## for i in range(len(ss)):\n",
    "##     print(i)\n",
    "##     for j in range(len(ss)):\n",
    "##         if ss[i][j] != None and ss[i][j] != []:\n",
    "##             indices[0].append(i)\n",
    "##             indices[1].append(j)\n",
    "##             values.append(ss[i][j])\n",
    "\n",
    "\n",
    "## with open(\"./edge_attribs_sparse_indices.pickle\", 'wb') as of:\n",
    "##     pickle.dump(indices, of)\n",
    "\n",
    "## with open(\"./edge_attribs_sparse_values.pickle\", 'wb') as of:\n",
    "##     pickle.dump(values, of)\n",
    "\n",
    "## print('loading indices')\n",
    "## with open(\"./edge_attribs_sparse_indices.pickle\", 'rb') as inf:\n",
    "##     indices = pickle.load(inf)\n",
    "\n",
    "## print('loading values')\n",
    "## with open(\"./edge_attribs_sparse_values.pickle\", 'rb') as inf:\n",
    "##     values = pickle.load(inf)\n",
    "\n",
    "## print('creating sparse tensor')\n",
    "## s = torch.sparse_coo_tensor(indices, values, (67800, 67800, len(ff)), dtype=torch.float16)\n",
    "## print('saving sparse matrix')\n",
    "## torch.save(s, \"/mounts/work/ayyoob/models/gnn/edge_attribs_tensor16.pickle\")\n",
    "\n",
    "#print('loading sparse matrix')\n",
    "#x_edge = torch.load(\"/mounts/work/ayyoob/models/gnn/edge_attribs_tensor.pickle\")\n",
    "\n",
    "\n",
    "#train_dataset.features_edge = features_edge\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "#x_edge = x_edge.coalesce()\n",
    "#torch.cuda.set_device(1)\n",
    "#x_edge_vals = x_edge.values()\n",
    "#indices_np = x_edge.indices().numpy()\n",
    "#print(indices_np.shape)\n",
    "#x_edge_np = csr_matrix((np.arange(indices_np.shape[1]), (indices_np[0, :], indices_np[1,:])), shape=(67800, 67800))\n",
    "\n",
    "##x_edge_vals = x_edge_vals.cpu()\n",
    "##maxes = torch.max(x_edge_vals,0)\n",
    "##mins = torch.min(x_edge_vals,0)\n",
    "##x_edge_vals_d = torch.div(x_edge_vals, maxes.values)\n",
    "\n",
    "#print('creating targets')\n",
    "#targets = torch.zeros(indices_np.shape[1], dtype=torch.int64)\n",
    "#pos_indices = x_edge_np[train_dataset.edge_index.cpu().numpy()[0,:], train_dataset.edge_index.cpu().numpy()[1,:]]\n",
    "#pos_indices = np.squeeze(np.asarray(pos_indices))\n",
    "#targets[pos_indices] = 1\n",
    "#print(\"done\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    " # run on delta, extract w2v features\n",
    "#sys.path.insert(0, '../')\n",
    "#import pickle\n",
    "#from gensim.models import Word2Vec\n",
    "#from app.document_retrieval import DocumentRetriever\n",
    "#from my_utils import utils\n",
    "#config_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc-ui-demo/config_pbc.ini\"\n",
    "#utils.setup(config_file)\n",
    "#import torch\n",
    "#import my_utils.alignment_features as feat_utils\n",
    "\n",
    "#doc_retriever = DocumentRetriever()\n",
    "\n",
    "#model_w2v = Word2Vec.load(\"word2vec_83langs_15epoch.model\")\n",
    "#train_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_en_fr_full.pickle\")\n",
    "#nodes_map = train_dataset.nodes_map\n",
    "\n",
    "#x = [[] for i in range(train_dataset.x.shape[0])]\n",
    "#for edition_f in nodes_map:\n",
    "#    utils.LOG.info(f\"processing edition {edition_f}\")\n",
    "#    for verse in nodes_map[edition_f]:         #toknom nodecount\n",
    "#        line = doc_retriever.retrieve_document(f'{verse}@{edition_f}')\n",
    "#        line = line.strip().split()\n",
    "\n",
    "#        for tok in nodes_map[edition_f][verse]:\n",
    "#            w_emb = model_w2v.wv[f'{edition_f[:3]}:{line[tok]}']\n",
    "#            x[nodes_map[edition_f][verse][tok]].extend(w_emb)\n",
    "\n",
    "#x = torch.tensor(x, dtype=torch.float)\n",
    "#train_dataset.x = torch.cat((train_dataset.x, x), dim=1)\n",
    "#train_dataset.features.append(feat_utils.ForwardFeature(50, 100, 'W2v'))\n",
    "\n",
    "#print(x.shape, train_dataset.x.shape, len(train_dataset.features))\n",
    "\n",
    "#torch.save(train_dataset, \"/mounts/work/ayyoob/models/gnn/dataset_en_fr_full.pickle\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "## Add node embedding features\n",
    "#importlib.reload(gutils)\n",
    "#x_,features_ = gutils.get_embedding_node_features(train_dataset.nodes_map, train_verses, small_editions, verse_alignments_inter, x_edge_np, x_edge_vals.cpu().numpy())\n",
    "#train_dataset.x = torch.cat((train_dataset.x,x_), dim=1)\n",
    "#train_dataset.features.extend(features_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "blinker_test_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_blinker_full_community_word.pickle\", map_location=torch.device('cpu'))\n",
    "editf12 = \"eng-x-bible-mixed\"\n",
    "editf22 = 'fra-x-bible-louissegond'\n",
    "\n",
    "test_gold_eng_fra = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/eng_fra_pbc/eng-fra.gold\"\n",
    "\n",
    "pros_blinker, surs_blinker = autils.load_gold(test_gold_eng_fra)\n",
    "blinker_verses = list(pros_blinker.keys())\n",
    "\n",
    "#blinker_verse_alignments_inter = {}\n",
    "#blinker_verse_alignments_gdfa = {}\n",
    "#args = []\n",
    "#for i,verse in enumerate(blinker_verses):\n",
    "#    args.append((verse, current_editions))\n",
    "\n",
    "#with Pool(20) as p:\n",
    "#    all_res = p.map(get_pruned_verse_alignments, args)\n",
    "\n",
    "#for i,verse in enumerate(blinker_verses):\n",
    "#    verse_aligns_inter, verse_aligns_gdfa = all_res[i]\n",
    "    \n",
    "#    blinker_verse_alignments_inter[verse] = verse_aligns_inter\n",
    "#    blinker_verse_alignments_gdfa[verse] = verse_aligns_gdfa\n",
    "\n",
    "utils.LOG.info(\"done reading alignments\")\n",
    "#torch.save(blinker_verse_alignments_inter, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_inter.pickle\")\n",
    "#torch.save(blinker_verse_alignments_gdfa, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_gdfa.pickle\")\n",
    "utils.LOG.info('done saving pruned alignments')\n",
    "\n",
    "print('reading inter verse alignments')\n",
    "blinker_verse_alignments_inter = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_inter.pickle\")\n",
    "blinker_verse_alignments_gdfa = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_gdfa.pickle\")\n",
    "gc.collect()\n",
    "print('done reading inter verse alignments')\n",
    "\n",
    "verses_map = {}\n",
    "\n",
    "for edit in blinker_test_dataset.nodes_map:\n",
    "    for verse in blinker_test_dataset.nodes_map[edit]:\n",
    "        if verse not in verses_map:\n",
    "            for tok in blinker_test_dataset.nodes_map[edit][verse]:\n",
    "                verses_map[verse] = blinker_test_dataset.nodes_map[edit][verse][tok]\n",
    "                break\n",
    "\n",
    "sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "blinker_verses = [item[0] for item in sorted_verses]\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-11 12:00:30,633 - analytics - INFO - done reading alignments\n",
      "2021-09-11 12:00:30,635 - analytics - INFO - done saving pruned alignments\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "reading inter verse alignments\n",
      "done reading inter verse alignments\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "helfi_heb_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_helfi_heb_test_community_word.pickle\", map_location=torch.device('cpu'))\n",
    "editf_fin = \"fin-x-bible-helfi\"\n",
    "editf_heb = 'heb-x-bible-helfi'\n",
    "\n",
    "test_gold_helfi_heb = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-heb-fin-gold-alignments_test.txt\"\n",
    "\n",
    "pros_heb, surs_heb = autils.load_gold(test_gold_helfi_heb)\n",
    "heb_verses = list(pros_blinker.keys())\n",
    "\n",
    "#blinker_verse_alignments_inter = {}\n",
    "#blinker_verse_alignments_gdfa = {}\n",
    "#args = []\n",
    "#for i,verse in enumerate(blinker_verses):\n",
    "#    args.append((verse, current_editions))\n",
    "\n",
    "#with Pool(20) as p:\n",
    "#    all_res = p.map(get_pruned_verse_alignments, args)\n",
    "\n",
    "#for i,verse in enumerate(blinker_verses):\n",
    "#    verse_aligns_inter, verse_aligns_gdfa = all_res[i]\n",
    "    \n",
    "#    blinker_verse_alignments_inter[verse] = verse_aligns_inter\n",
    "#    blinker_verse_alignments_gdfa[verse] = verse_aligns_gdfa\n",
    "\n",
    "utils.LOG.info(\"done reading alignments\")\n",
    "#torch.save(blinker_verse_alignments_inter, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_inter.pickle\")\n",
    "#torch.save(blinker_verse_alignments_gdfa, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_gdfa.pickle\")\n",
    "utils.LOG.info('done saving pruned alignments')\n",
    "\n",
    "print('reading inter verse alignments')\n",
    "blinker_verse_alignments_inter = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_inter.pickle\")\n",
    "blinker_verse_alignments_gdfa = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_gdfa.pickle\")\n",
    "gc.collect()\n",
    "print('done reading inter verse alignments')\n",
    "\n",
    "verses_map = {}\n",
    "\n",
    "for edit in blinker_test_dataset.nodes_map:\n",
    "    for verse in blinker_test_dataset.nodes_map[edit]:\n",
    "        if verse not in verses_map:\n",
    "            for tok in blinker_test_dataset.nodes_map[edit][verse]:\n",
    "                verses_map[verse] = blinker_test_dataset.nodes_map[edit][verse][tok]\n",
    "                break\n",
    "\n",
    "sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "blinker_verses = [item[0] for item in sorted_verses]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-11 12:00:46,029 - analytics - INFO - done reading alignments\n",
      "2021-09-11 12:00:46,030 - analytics - INFO - done saving pruned alignments\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "reading inter verse alignments\n",
      "done reading inter verse alignments\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "#importlib.reload(afeatures)\n",
    "grc_test_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_helfi_grc_test_community_word.pickle\", map_location=torch.device('cpu'))\n",
    "editf_fin = \"fin-x-bible-helfi\"\n",
    "editf_grc = 'grc-x-bible-helfi'\n",
    "\n",
    "test_gold_grc = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-grc-fin-gold-alignments_test.txt\"\n",
    "\n",
    "pros_grc, surs_grc = autils.load_gold(test_gold_grc)\n",
    "grc_verses = list(pros_grc.keys())\n",
    "\n",
    "\n",
    "grc_test_verse_alignments_inter = {}\n",
    "grc_test_verse_alignments_gdfa = {}\n",
    "gc.collect()\n",
    "#args = []\n",
    "#for i,verse in enumerate(grc_verses):\n",
    "#    args.append((verse, current_editions))\n",
    "\n",
    "#with Pool(20) as p:\n",
    "#    all_res = p.map(get_pruned_verse_alignments, args)\n",
    "\n",
    "#for i,verse in enumerate(grc_verses):\n",
    "#    verse_aligns_inter, verse_aligns_gdfa = all_res[i]\n",
    "    \n",
    "#    grc_test_verse_alignments_inter[verse] = verse_aligns_inter\n",
    "#    grc_test_verse_alignments_gdfa[verse] = verse_aligns_gdfa\n",
    "\n",
    "utils.LOG.info(\"done reading alignments\")\n",
    "#torch.save(grc_test_verse_alignments_inter, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_grc_inter.pickle\")\n",
    "#torch.save(grc_test_verse_alignments_gdfa, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_grc_gdfa.pickle\")\n",
    "utils.LOG.info('done saving pruned alignments')\n",
    "\n",
    "print('reading inter verse alignments')\n",
    "grc_test_verse_alignments_inter = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_grc_inter.pickle\")\n",
    "grc_test_verse_alignments_gdfa = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_grc_gdfa.pickle\")\n",
    "gc.collect()\n",
    "print('done reading inter verse alignments')\n",
    "\n",
    "verses_map = {}\n",
    "\n",
    "for edit in grc_test_dataset.nodes_map:\n",
    "    for verse in grc_test_dataset.nodes_map[edit]:\n",
    "        if verse not in verses_map:\n",
    "            for tok in grc_test_dataset.nodes_map[edit][verse]:\n",
    "                verses_map[verse] = grc_test_dataset.nodes_map[edit][verse][tok]\n",
    "                break\n",
    "\n",
    "sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "grc_test_verses = [item[0] for item in sorted_verses]\n",
    "\n",
    "gc.collect()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-11 12:00:59,874 - analytics - INFO - done reading alignments\n",
      "2021-09-11 12:00:59,876 - analytics - INFO - done saving pruned alignments\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "reading inter verse alignments\n",
      "done reading inter verse alignments\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "heb_test_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_helfi_heb_test_community_word.pickle\", map_location=torch.device('cpu'))\n",
    "editf_fin = \"fin-x-bible-helfi\"\n",
    "editf_heb = 'heb-x-bible-helfi'\n",
    "\n",
    "test_gold_heb = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-heb-fin-gold-alignments_test.txt\"\n",
    "\n",
    "pros_heb, surs_heb = autils.load_gold(test_gold_heb)\n",
    "heb_verses = list(pros_heb.keys())\n",
    "\n",
    "\n",
    "heb_test_verse_alignments_inter = {}\n",
    "heb_test_verse_alignments_gdfa = {}\n",
    "#args = []\n",
    "#for i,verse in enumerate(heb_verses):\n",
    "#    args.append((verse, current_editions))\n",
    "\n",
    "#with Pool(20) as p:\n",
    "#    all_res = p.map(get_pruned_verse_alignments, args)\n",
    "\n",
    "#for i,verse in enumerate(heb_verses):\n",
    "#    verse_aligns_inter, verse_aligns_gdfa = all_res[i]\n",
    "    \n",
    "#    heb_test_verse_alignments_inter[verse] = verse_aligns_inter\n",
    "#    heb_test_verse_alignments_gdfa[verse] = verse_aligns_gdfa\n",
    "\n",
    "#utils.LOG.info(\"done reading alignments\")\n",
    "#torch.save(heb_test_verse_alignments_inter, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_heb_inter.pickle\")\n",
    "#torch.save(heb_test_verse_alignments_gdfa, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_heb_gdfa.pickle\")\n",
    "#utils.LOG.info('done saving pruned alignments')\n",
    "\n",
    "print('reading inter verse alignments')\n",
    "heb_test_verse_alignments_inter = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_heb_inter.pickle\")\n",
    "heb_test_verse_alignments_gdfa = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_heb_gdfa.pickle\")\n",
    "gc.collect()\n",
    "print('done reading inter verse alignments')\n",
    "\n",
    "verses_map = {}\n",
    "\n",
    "for edit in heb_test_dataset.nodes_map:\n",
    "    for verse in heb_test_dataset.nodes_map[edit]:\n",
    "        if verse not in verses_map:\n",
    "            for tok in heb_test_dataset.nodes_map[edit][verse]:\n",
    "                verses_map[verse] = heb_test_dataset.nodes_map[edit][verse][tok]\n",
    "                break\n",
    "\n",
    "sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "heb_test_verses = [item[0] for item in sorted_verses]\n",
    "gc.collect()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "reading inter verse alignments\n",
      "done reading inter verse alignments\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "verses_map = {}\n",
    "\n",
    "for edit in train_dataset.nodes_map:\n",
    "    for verse in train_dataset.nodes_map[edit]:\n",
    "        if verse not in verses_map:\n",
    "            for tok in train_dataset.nodes_map[edit][verse]:\n",
    "                verses_map[verse] = train_dataset.nodes_map[edit][verse][tok]\n",
    "                break\n",
    "\n",
    "sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "all_verses = [item[0] for item in sorted_verses]\n",
    "\n",
    "long_verses = set()\n",
    "\n",
    "for edit in train_dataset.nodes_map.keys():\n",
    "    for verse in train_dataset.nodes_map[edit]:\n",
    "        to_print = False\n",
    "        for tok in train_dataset.nodes_map[edit][verse]:\n",
    "            if tok > 150:\n",
    "                to_print = True\n",
    "        if to_print == True:\n",
    "            long_verses.add(verse)\n",
    "\n",
    "\n",
    "train_verses = all_verses[:]\n",
    "\n",
    "masked_verses = list(long_verses)\n",
    "masked_verses.extend(blinker_verses)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "class GNNDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset, verses, edit_files, alignments, group_size = 360):\n",
    "        self.verses = list(verses)\n",
    "        self.edit_files = list(edit_files)\n",
    "        self.nodes_map = dataset.nodes_map\n",
    "\n",
    "        self.items = self.calculate_size(self.nodes_map, self.verses, self.edit_files, group_size)\n",
    "        self.alignments = alignments\n",
    "        self.verse_info = {}\n",
    "        self.calculate_verse_stats(verses, edit_files, alignments, dataset)\n",
    "    \n",
    "    def calculate_size(self, nodes_map, verses, edit_files, group_size):\n",
    "        res = []\n",
    "        item = []\n",
    "        self.not_presented = []\n",
    "        for verse in verses:\n",
    "            if len(item) > 0:\n",
    "                res.append(item)\n",
    "                item = []\n",
    "            for i,editf1 in enumerate(edit_files):\n",
    "                if editf1 not in nodes_map:\n",
    "                    self.not_presented.append(editf1)\n",
    "                    continue\n",
    "                if verse in nodes_map[editf1]:\n",
    "                    for editf2 in edit_files[i+1:]:\n",
    "                        if editf2 not in nodes_map:\n",
    "                            self.not_presented.append(editf2)\n",
    "                            continue\n",
    "                        if verse in nodes_map[editf2]:\n",
    "                            item.append((verse, editf1, editf2))\n",
    "                            if len(item) >= group_size:\n",
    "                                res.append(item)\n",
    "                                item = []\n",
    "        \n",
    "        if len(item)>0:\n",
    "            res.append(item)\n",
    "        \n",
    "        print(f\"not presented: {set(self.not_presented)}\")\n",
    "\n",
    "        return res\n",
    "\n",
    "    def calculate_verse_stats(self,verses, edition_files, alignments, dataset):\n",
    "        min_edge = 0\n",
    "        for verse in tqdm(verses):\n",
    "            min_nodes = 99999999999999\n",
    "            max_nodes = 0\n",
    "            #utils.LOG.info(f\"adding {verse}\")\n",
    "            edges_tmp = [[],[]]\n",
    "            x_tmp = []\n",
    "            features = []\n",
    "            for i,editf1 in enumerate(edition_files):\n",
    "                for j,editf2 in enumerate(edition_files[i+1:]):\n",
    "                    aligns = autils.get_aligns(editf1, editf2, alignments[verse])\n",
    "                    if aligns != None:\n",
    "                        for align in aligns:\n",
    "                            try:\n",
    "                                n1,_ = gutils.node_nom(verse, editf1, align[0], None, dataset.nodes_map, x_tmp, edition_files, features)\n",
    "                                n2,_ = gutils.node_nom(verse, editf2, align[1], None, dataset.nodes_map, x_tmp, edition_files, features)\n",
    "                                edges_tmp[0].extend([n1, n2])\n",
    "\n",
    "                                max_nodes = max(n1, n2, max_nodes)\n",
    "                                min_nodes = min(n1, n2, min_nodes)\n",
    "                            except Exception as e:\n",
    "                                print(editf1, editf2, verse)\n",
    "                                raise(e)\n",
    "\n",
    "            self.verse_info[verse] = {}\n",
    "\n",
    "            self.verse_info[verse]['padding'] = min_nodes\n",
    "\n",
    "            self.verse_info[verse]['x'] = dataset.x[min_nodes:max_nodes+1,:]\n",
    "            \n",
    "            self.verse_info[verse]['edge_index'] = dataset.edge_index[:, min_edge : min_edge + len(edges_tmp[0])] - min_nodes\n",
    "\n",
    "            if torch.min(self.verse_info[verse]['edge_index']) != 0:\n",
    "                print(verse, min_nodes, max_nodes, min_edge, len(edges_tmp[0]))\n",
    "                print(torch.min(self.verse_info[verse]['edge_index']))\n",
    "            \n",
    "            if self.verse_info[verse]['x'].shape[0] != torch.max(self.verse_info[verse]['edge_index']) + 1 :\n",
    "                print(verse, min_nodes, max_nodes, min_edge, len(edges_tmp[0]))\n",
    "                print(torch.min(self.verse_info[verse]['edge_index']))\n",
    "            \n",
    "            min_edge = min_edge + len(edges_tmp[0])\n",
    "\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        #return self.length\n",
    "        return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        item = self.items[idx]\n",
    "\n",
    "        res_pos = [[],[]]\n",
    "        res_neg = [[],[]]\n",
    "        nodes = set()\n",
    "        for instance in item:\n",
    "            verse, editf1, editf2 = instance\n",
    "            aligns = autils.get_aligns(editf1, editf2, self.alignments[verse]) \n",
    "            if aligns != None:\n",
    "                for align in aligns:\n",
    "                    p1, p2 = align\n",
    "                    n1 = self.nodes_map[editf1][verse][p1] - self.verse_info[verse]['padding']\n",
    "                    n2 = self.nodes_map[editf2][verse][p2] - self.verse_info[verse]['padding']\n",
    "                    res_pos[0].extend([n1,n2])\n",
    "                    res_pos[1].extend([n2,n1])\n",
    "\n",
    "                    n2_ = random.choice( list(self.nodes_map[editf2][verse].values()) ) - self.verse_info[verse]['padding']\n",
    "                    n1_ = random.choice( list(self.nodes_map[editf1][verse].values()) ) - self.verse_info[verse]['padding']\n",
    "                    \n",
    "                    if n2_ != n2:\n",
    "                        res_neg[0].extend([n1, n2_])\n",
    "                        res_neg[1].extend([n2_, n1])\n",
    "                    \n",
    "                    if n1_ != n1:\n",
    "                        res_neg[0].extend([n1_, n2])\n",
    "                        res_neg[1].extend([n2, n1_])\n",
    "                    \n",
    "                    #nodes.update([n1, n2, n1_, n2_])\n",
    "                \n",
    "        \n",
    "        return {'pos':res_pos, 'neg':res_neg, 'nodes':nodes, 'verse':verse, 'editf1':editf1, 'editf2':editf2}\n",
    "\n",
    "def collate_fun(input):\n",
    "    res = {}\n",
    "    #all_edits = {}\n",
    "    for item in input:\n",
    "        verse = item['verse'] \n",
    "        if verse not in res:\n",
    "            res[verse] = {'pos': [[],[]], 'neg' : [[],[]],\n",
    "                 'x':gnn_dataset.verse_info[verse]['x'], 'edge_index':gnn_dataset.verse_info[verse]['edge_index']\n",
    "                 ,'intra_sent_edges':[[],[]]}\n",
    "        \n",
    "        res[verse]['pos'][0].extend(item['pos'][0])\n",
    "        res[verse]['pos'][1].extend(item['pos'][1])\n",
    "\n",
    "        res[verse]['neg'][0].extend(item['neg'][0])\n",
    "        res[verse]['neg'][1].extend(item['neg'][1])\n",
    "\n",
    "        #if verse not in all_edits:\n",
    "        #    all_edits[verse] = []\n",
    "\n",
    "        #if item['editf1'] not in all_edits[verse]:\n",
    "        #    e = eval_utils.get_all_edges(verse, item['editf1'], train_dataset.nodes_map, gnn_dataset.verse_info)\n",
    "        #    res[verse]['intra_sent_edges'][0].extend(e[0])\n",
    "        #    res[verse]['intra_sent_edges'][1].extend(e[1])\n",
    "\n",
    "        #if item['editf2'] not in all_edits[verse]:\n",
    "        #    e = eval_utils.get_all_edges(verse, item['editf2'], train_dataset.nodes_map, gnn_dataset.verse_info)\n",
    "        #    res[verse]['intra_sent_edges'][0].extend(e[0])\n",
    "        #    res[verse]['intra_sent_edges'][1].extend(e[1])\n",
    "\n",
    "    #nodes = list(nodes)\n",
    "    #mapping = {node:pos for pos, node in enumerate(nodes)}\n",
    "    ##indices = [[i for i in range(len(res_pos[0]))],[i for i in range(len(res_pos[1]))]]\n",
    "    #indices = [[],[]]\n",
    "    ##adjacency = torch.zeros((len(nodes), len(nodes)), dtype=torch.float)\n",
    "    #for i in range(len(res_pos[0])):\n",
    "    ##    adjacency[mapping[res_pos[0][i]], mapping[res_pos[1][i]]] = 1\n",
    "    #    indices[0].append(mapping[res_pos[0][i]])\n",
    "    #    indices[1].append(mapping[res_pos[1][i]])\n",
    "\n",
    "    #adjacency = torch.sparse_coo_tensor(indices, [1 for i in range(len(res_pos[0]))], (len(nodes), len(nodes)))\n",
    "    return res\n",
    "\n",
    "\n",
    "gnn_dataset_train = GNNDataset(train_dataset, train_verses, current_editions, verse_alignments_inter)\n",
    "gnn_dataset_blinker = GNNDataset(blinker_test_dataset, blinker_verses, current_editions, blinker_verse_alignments_inter)\n",
    "gnn_dataset_heb = GNNDataset(heb_test_dataset, heb_test_verses, current_editions, heb_test_verse_alignments_inter)\n",
    "gnn_dataset_grc = GNNDataset(grc_test_dataset, grc_test_verses, current_editions, grc_test_verse_alignments_inter)\n",
    "\n",
    "len(gnn_dataset_train)\n",
    "gc.collect()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 2/8000 [00:00<07:45, 17.18it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "not presented: set()\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 8000/8000 [07:07<00:00, 18.71it/s]\n",
      "  2%|         | 4/250 [00:00<00:07, 32.15it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "not presented: set()\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 250/250 [00:13<00:00, 18.01it/s]\n",
      "  0%|          | 3/2225 [00:00<01:44, 21.27it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "not presented: {'prs-x-bible-goodnews', 'nep-x-bible-revised', 'sqi-x-bible', 'est-x-bible-newworld2009', 'bre-x-bible', 'mar-x-bible', 'pan-x-bible-newworld', 'kan-x-bible-newworld', 'rus-x-bible-slovozhizny2006', 'eus-x-bible-batua', 'por-x-bible-versaointernacional', 'ind-x-bible-easy2005', 'kir-x-bible-2005', 'tat-x-bible', 'ron-x-bible-2006', 'kat-x-bible', 'hat-x-bible-newworld2007', 'nno-x-bible-2011', 'mal-x-bible', 'tam-x-bible-newworld', 'urd-x-bible-revised2010', 'nob-x-bible-2011', 'hun-x-bible-2012', 'cat-x-bible-evangelica', 'msa-x-bible-klinkert', 'mkd-x-bible-2004', 'min-x-bible', 'chv-x-bible', 'lav-x-bible', 'mya-x-bible-newworld', 'isl-x-bible', 'tgk-x-bible', 'bak-x-bible', 'grc-x-bible-helfi', 'hin-x-bible-newworld', 'vie-x-bible-newworld', 'war-x-bible-newworld', 'sun-x-bible-formal', 'bel-x-bible-bokun', 'jav-x-bible-2006'}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 2225/2225 [01:15<00:00, 29.34it/s]\n",
      "  0%|          | 2/783 [00:00<01:03, 12.25it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "not presented: {'heb-x-bible-helfi', 'plt-x-bible-newworld'}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 783/783 [01:22<00:00,  9.54it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "from gnn_utils import eval_utils\n",
    "train_dataset.train_mask = train_dataset.val_mask = train_dataset.test_mask = train_dataset.y = None\n",
    "test_dataset.train_mask = test_dataset.val_mask = test_dataset.test_mask = test_dataset.y = None\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "features = train_dataset.features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def save_model(model):\n",
    "    model.encoder.feature_encoder.feature_types[0] = afeatures.OneHotFeature(20, 83, 'editf')\n",
    "    model.encoder.feature_encoder.feature_types[1] = afeatures.OneHotFeature(32, 150, 'position')\n",
    "    model.encoder.feature_encoder.feature_types[2] = afeatures.FloatFeature(4, 'degree_centrality')\n",
    "    model.encoder.feature_encoder.feature_types[3] = afeatures.FloatFeature(4, 'closeness_centrality')\n",
    "    model.encoder.feature_encoder.feature_types[4] = afeatures.FloatFeature(4, 'betweenness_centrality')\n",
    "    model.encoder.feature_encoder.feature_types[5] = afeatures.FloatFeature(4, 'load_centrality')\n",
    "    model.encoder.feature_encoder.feature_types[6] = afeatures.FloatFeature(4, 'harmonic_centrality')\n",
    "    model.encoder.feature_encoder.feature_types[7] = afeatures.OneHotFeature(32, 250, 'greedy_modularity_community')\n",
    "    model.encoder.feature_encoder.feature_types[8] = afeatures.OneHotFeature(32, 250, 'community_2')\n",
    "    model.encoder.feature_encoder.feature_types[9] = afeatures.MappingFeature(100, 'word')\n",
    "    torch.save(model, '/mounts/work/ayyoob/models/gnn/checkpoint/gnn_256_flggll_word_halfTrain_nofeatlinear_encoderlineear_decoderonelayer' + datetime.now().strftime(\"%Y%m%d-%H%M%S-\") + '.pickle')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "from tqdm import tqdm\n",
    "features_edge = [] #TODO remove me\n",
    "#features_edge = train_dataset.features_edge[:]\n",
    "#x_edge_vals2 = x_edge_vals[:, :]\n",
    "#features = train_dataset.features\n",
    "gnn_dataset = gnn_dataset_train\n",
    "data_loader = DataLoader(gnn_dataset_train, batch_size=1, collate_fn=collate_fun, shuffle=True)\n",
    "\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "drop_out = 0\n",
    "pos_noise = 0.0\n",
    "neg_noise = 0.0\n",
    "n_head = 1\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "channels = 256\n",
    "\n",
    "in_dim = sum(t.out_dim for t in features)\n",
    "decoder_in_dim = n_head * channels * 2 + sum(t.out_dim for t in features_edge) \n",
    "print('edge features size: ', sum(t.out_dim for t in features_edge))\n",
    "#discriminator = Discriminator(channels*n_head, channels * (n_head+1), channels*n_head)\n",
    "#discriminator_optimizer = torch.optim.AdamW(discriminator.parameters(), lr=0.0007)\n",
    "#encoder2 = Encoder2(channels, int(channels/2)).to(dev)\n",
    "decoder = Decoder(decoder_in_dim, int(decoder_in_dim/2), features_edge, n_cluster=64)\n",
    "model = pyg_nn.GAE(Encoder(in_dim, channels, features, n_head, edge_feature_dim=len(features_edge)), decoder).to(dev)\n",
    "#model.encoder2 = encoder2\n",
    "#model = pyg_nn.GAE(DeeperGCN(in_dim, len(features_edge), channels, 10, features), decoder=decoder).to(dev)\n",
    "#model = pyg_nn.GAE(Encoder(in_dim, channels, features, n_head)).to(dev)\n",
    "\n",
    "print(\"sending input to gpu\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "optimizer.add_param_group({'params': word_vectors})\n",
    "\n",
    "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S-\") + f\"samett-{channels}chs-feat{train_dataset.num_node_features}-\")\n",
    "\n",
    "torch.set_printoptions(edgeitems=5)\n",
    "print(\"model params - decoder params - conv1\", sum(p.numel() for p in model.parameters()), sum(p.numel() for p in decoder.parameters()))\n",
    "\n",
    "for epoch in range(1, 2):\n",
    "    print(f\"\\n----------------epoch {epoch} ---------------\")\n",
    "    \n",
    "    #if epoch % 1 == 0:\n",
    "    #    train_neg_edge_index = gutils.get_negative_edges(train_verses, small_editions, train_dataset.nodes_map,  verse_alignments_inter).to(dev)\n",
    "        #edge_index_seq_sent_neg = get_negative_edges_seq(train_dataset.nodes_map).to(dev)\n",
    "\n",
    "    train(epoch)\n",
    "    save_model(model)\n",
    "    clean_memory()\n",
    "    if epoch % 1 == 0:\n",
    "        #alignment_test(epoch, test_dataset.edge_index, editf1, editf2, test_verses[:30], test_nodes_map,\n",
    "        #    dev, model, x_test, pros, surs, verse_alignments_inter, verse_alignments_gdfa, writer, gnn_dataset.verse_info)\n",
    "        #eval_utils.alignment_test(epoch, test_dataset.edge_index, editf1, editf2, test_verses[:], test_nodes_map,\n",
    "        #    dev, model, x_test, pros, surs, verse_alignments_inter, verse_alignments_gdfa, writer, gnn_dataset.verse_info)\n",
    "\n",
    "        eval_utils.alignment_test(epoch, grc_test_dataset.edge_index, editf_fin, editf_grc, grc_test_verses[:], grc_test_dataset.nodes_map,\n",
    "                        dev, model, grc_test_dataset.x, pros_grc, surs_grc, grc_test_verse_alignments_inter, grc_test_verse_alignments_gdfa, writer, gnn_dataset_grc.verse_info)\n",
    "\n",
    "        eval_utils.alignment_test(epoch, heb_test_dataset.edge_index, editf_fin, editf_heb, heb_test_verses[:], heb_test_dataset.nodes_map,\n",
    "                                dev, model, heb_test_dataset.x, pros_heb, surs_heb, heb_test_verse_alignments_inter, heb_test_verse_alignments_gdfa, writer, gnn_dataset_heb.verse_info)\n",
    "\n",
    "        eval_utils.alignment_test(epoch, blinker_test_dataset.edge_index, editf12, editf22, blinker_verses, blinker_test_dataset.nodes_map,\n",
    "                    dev, model, blinker_test_dataset.x, pros_blinker, surs_blinker, blinker_verse_alignments_inter, blinker_verse_alignments_gdfa, writer, gnn_dataset_blinker.verse_info)\n",
    "        # auc, ap = test(edge_index_seq_sent, edge_index_seq_sent_neg, epoch)\n",
    "        # print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))\n",
    "    \n",
    "    clean_memory()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "edge features size:  0\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "i = sag\n",
    "batch = khar\n",
    "verse = gav\n",
    "print(i, verse)\n",
    "\n",
    "keys = list(gnn_dataset.verse_info.keys())\n",
    "\n",
    "gnn_dataset.verse_info[verse]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_loader_blinker = DataLoader(gnn_dataset_blinker, batch_size=1, collate_fn=collate_fun, shuffle=True)\n",
    "data_loader_heb = DataLoader(gnn_dataset_heb, batch_size=1, collate_fn=collate_fun, shuffle=True)\n",
    "data_loader_grc = DataLoader(gnn_dataset_grc, batch_size=1, collate_fn=collate_fun, shuffle=True)\n",
    "\n",
    "clean_memory()\n",
    "data_loader = data_loader_blinker\n",
    "gnn_dataset = gnn_dataset_blinker\n",
    "train(1)\n",
    "clean_memory()\n",
    "eval_utils.alignment_test(epoch, blinker_test_dataset.edge_index, editf12, editf22, blinker_verses[:], blinker_test_dataset.nodes_map,\n",
    "                    dev, model, blinker_test_dataset.x, pros_blinker, surs_blinker, blinker_verse_alignments_inter, blinker_verse_alignments_gdfa, writer, gnn_dataset_blinker.verse_info)\n",
    "clean_memory()\n",
    "\n",
    "data_loader = data_loader_grc\n",
    "gnn_dataset = gnn_dataset_grc\n",
    "train(1)\n",
    "clean_memory()\n",
    "eval_utils.alignment_test(epoch, grc_test_dataset.edge_index, editf_fin, editf_grc, grc_test_verses[:], grc_test_dataset.nodes_map,\n",
    "                        dev, model, grc_test_dataset.x, pros_grc, surs_grc, grc_test_verse_alignments_inter, grc_test_verse_alignments_gdfa, writer, gnn_dataset_grc.verse_info)\n",
    "clean_memory()\n",
    "\n",
    "data_loader = data_loader_heb\n",
    "gnn_dataset = gnn_dataset_heb\n",
    "train(1)\n",
    "clean_memory()\n",
    "eval_utils.alignment_test(epoch, heb_test_dataset.edge_index, editf_fin, editf_heb, heb_test_verses[:], heb_test_dataset.nodes_map,\n",
    "                        dev, model, heb_test_dataset.x, pros_heb, surs_heb, heb_test_verse_alignments_inter, heb_test_verse_alignments_gdfa, writer, gnn_dataset_heb.verse_info)\n",
    "clean_memory()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 1180/1180 [00:34<00:00, 34.34it/s]\n",
      "  0%|          | 1/250 [00:00<00:41,  5.97it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train loss: 0\n",
      "cluster loss: 0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 250/250 [00:13<00:00, 18.44it/s]\n",
      "  0%|          | 0/7000 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.783, rec: 0.755, F1: 0.769, AER: 0.231\n",
      "argmax prec: 0.942, rec: 0.673, F1: 0.785, AER: 0.212\n",
      "resnorm prec: 0.712, rec: 0.76, F1: 0.735, AER: 0.266\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.927, rec: 0.708, F1: 0.803, AER: 0.194\n",
      "my_gd_gdfa prec: 0.867, rec: 0.785, F1: 0.824, AER: 0.174\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 7000/7000 [15:10<00:00,  7.69it/s]\n",
      "  0%|          | 2/783 [00:00<00:41, 18.97it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train loss: 5168247.646852493\n",
      "cluster loss: 692.6781005859375\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 783/783 [00:33<00:00, 23.33it/s]\n",
      "  0%|          | 1/6635 [00:00<12:27,  8.87it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.644, rec: 0.793, F1: 0.711, AER: 0.289\n",
      "argmax prec: 0.852, rec: 0.695, F1: 0.766, AER: 0.235\n",
      "resnorm prec: 0.674, rec: 0.772, F1: 0.72, AER: 0.281\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.837, rec: 0.728, F1: 0.779, AER: 0.221\n",
      "my_gd_gdfa prec: 0.735, rec: 0.785, F1: 0.759, AER: 0.241\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 6635/6635 [11:39<00:00,  9.48it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train loss: 3193099.0618701577\n",
      "cluster loss: 467.2447814941406\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 2225/2225 [01:27<00:00, 25.51it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.513, rec: 0.745, F1: 0.608, AER: 0.393\n",
      "argmax prec: 0.779, rec: 0.611, F1: 0.685, AER: 0.315\n",
      "resnorm prec: 0.564, rec: 0.732, F1: 0.637, AER: 0.363\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.763, rec: 0.658, F1: 0.707, AER: 0.293\n",
      "my_gd_gdfa prec: 0.611, rec: 0.695, F1: 0.65, AER: 0.35\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gnn_utils import eval_utils\n",
    "importlib.reload(eval_utils)\n",
    "\n",
    "clean_memory()\n",
    "eval_utils.alignment_test(epoch, heb_test_dataset.edge_index, editf_fin, editf_heb, heb_test_verses, heb_test_dataset.nodes_map,\n",
    "                        dev, model, heb_test_dataset.x, pros_heb, surs_heb, heb_test_verse_alignments_inter, heb_test_verse_alignments_gdfa, writer, gnn_dataset_heb.verse_info)\n",
    "\n",
    "eval_utils.alignment_test(epoch, grc_test_dataset.edge_index, editf_fin, editf_grc, grc_test_verses, grc_test_dataset.nodes_map,\n",
    "                        dev, model, grc_test_dataset.x, pros_grc, surs_grc, grc_test_verse_alignments_inter, grc_test_verse_alignments_gdfa, writer, gnn_dataset_grc.verse_info)\n",
    "\n",
    "eval_utils.alignment_test(epoch, blinker_test_dataset.edge_index, editf12, editf22, blinker_verses, blinker_test_dataset.nodes_map,\n",
    "                    dev, model, blinker_test_dataset.x, pros_blinker, surs_blinker, blinker_verse_alignments_inter, blinker_verse_alignments_gdfa, writer, gnn_dataset_blinker.verse_info)\n",
    "\n",
    "clean_memory()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.521, rec: 0.757, F1: 0.617, AER: 0.383\n",
      "argmax prec: 0.781, rec: 0.618, F1: 0.69, AER: 0.31\n",
      "resnorm prec: 0.513, rec: 0.758, F1: 0.612, AER: 0.388\n",
      "itermax2-.9 prec: 0.657, rec: 0.702, F1: 0.679, AER: 0.321\n",
      "itermax2-.95 prec: 0.655, rec: 0.701, F1: 0.677, AER: 0.323\n",
      "itermax2-.8 prec: 0.66, rec: 0.702, F1: 0.68, AER: 0.32\n",
      "my_gd prec: 0.759, rec: 0.663, F1: 0.708, AER: 0.292\n",
      "my_gd_gdfa prec: 0.607, rec: 0.701, F1: 0.651, AER: 0.349\n",
      "new1 prec: 0.808, rec: 0.616, F1: 0.699, AER: 0.3\n",
      "new_mygd prec: 0.753, rec: 0.667, F1: 0.707, AER: 0.293\n",
      "new_mygd_gdfa prec: 0.604, rec: 0.704, F1: 0.65, AER: 0.35\n",
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.643, rec: 0.792, F1: 0.71, AER: 0.29\n",
      "argmax prec: 0.85, rec: 0.7, F1: 0.768, AER: 0.233\n",
      "resnorm prec: 0.582, rec: 0.791, F1: 0.671, AER: 0.329\n",
      "itermax2-.9 prec: 0.736, rec: 0.756, F1: 0.746, AER: 0.254\n",
      "itermax2-.95 prec: 0.733, rec: 0.755, F1: 0.744, AER: 0.256\n",
      "itermax2-.8 prec: 0.738, rec: 0.756, F1: 0.747, AER: 0.253\n",
      "my_gd prec: 0.824, rec: 0.728, F1: 0.773, AER: 0.227\n",
      "my_gd_gdfa prec: 0.727, rec: 0.785, F1: 0.755, AER: 0.245\n",
      "new1 prec: 0.866, rec: 0.69, F1: 0.768, AER: 0.232\n",
      "new_mygd prec: 0.821, rec: 0.73, F1: 0.773, AER: 0.227\n",
      "new_mygd_gdfa prec: 0.724, rec: 0.786, F1: 0.754, AER: 0.246\n",
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.789, rec: 0.756, F1: 0.772, AER: 0.227\n",
      "argmax prec: 0.94, rec: 0.675, F1: 0.786, AER: 0.211\n",
      "resnorm prec: 0.656, rec: 0.784, F1: 0.714, AER: 0.29\n",
      "itermax2-.9 prec: 0.852, rec: 0.727, F1: 0.785, AER: 0.213\n",
      "itermax2-.95 prec: 0.851, rec: 0.728, F1: 0.785, AER: 0.213\n",
      "itermax2-.8 prec: 0.854, rec: 0.727, F1: 0.785, AER: 0.212\n",
      "my_gd prec: 0.918, rec: 0.713, F1: 0.803, AER: 0.194\n",
      "my_gd_gdfa prec: 0.863, rec: 0.791, F1: 0.825, AER: 0.173\n",
      "new1 prec: 0.942, rec: 0.668, F1: 0.782, AER: 0.215\n",
      "new_mygd prec: 0.913, rec: 0.714, F1: 0.801, AER: 0.195\n",
      "new_mygd_gdfa prec: 0.859, rec: 0.792, F1: 0.824, AER: 0.175\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# produce for uruba\n",
    "importlib.reload(eval_utils)\n",
    "editf_yor = 'yor-x-bible-2010'\n",
    "editf_others = ['eng-x-bible-mixed', 'deu-x-bible-newworld', 'ces-x-bible-newworld', 'fra-x-bible-louissegond', 'hin-x-bible-newworld',\n",
    "                'ita-x-bible-2009', 'prs-x-bible-goodnews', 'ron-x-bible-2006', 'spa-x-bible-newworld']\n",
    "\n",
    "#def get_pruned_verse_alignments(args):\n",
    "#    verse, current_editions = args\n",
    "    \n",
    "#    #verse_aligns_inter = autils.get_verse_alignments(verse)\n",
    "#    verse_aligns_gdfa = autils.get_verse_alignments(verse, gdfa=True)\n",
    "\n",
    "#    #autils.prune_non_necessary_alignments(verse_aligns_inter, current_editions)\n",
    "#    autils.prune_non_necessary_alignments(verse_aligns_gdfa, current_editions)\n",
    "\n",
    "#    gc.collect()\n",
    "#    return verse_aligns_gdfa\n",
    "    \n",
    "\n",
    "#verse_alignments_gdfa = {}\n",
    "#args = []\n",
    "#editfs = editf_others[:]\n",
    "#editfs.append(editf_yor)\n",
    "#for i,verse in enumerate(train_verses):\n",
    "#    args.append((verse, editfs))\n",
    "\n",
    "#print('going to get alignments')\n",
    "#with Pool(20) as p:\n",
    "#    all_res = p.map(get_pruned_verse_alignments, args)\n",
    "\n",
    "#for i,verse in enumerate(all_verses):\n",
    "#    verse_aligns_gdfa = all_res[i]\n",
    "    \n",
    "#    verse_alignments_gdfa[verse] = verse_aligns_gdfa\n",
    "\n",
    "for verse in train_dataset.nodes_map[editf_yor]:\n",
    "    if verse not in surs :\n",
    "        surs[verse] = set()\n",
    "        pros[verse] = set()\n",
    "\n",
    "#verse_alignments_gdfa = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_train_gdfa_yoruba.pickle\")\n",
    "\n",
    "for eidtf_t in editf_others:\n",
    "    res = {}\n",
    "\n",
    "    print('going to align heb ')\n",
    "    if eidtf_t in heb_test_dataset.nodes_map:\n",
    "        verses = set(heb_test_dataset.nodes_map[editf_yor].keys()).intersection(heb_test_dataset.nodes_map[eidtf_t].keys())\n",
    "        res_ = eval_utils.alignment_test(epoch, heb_test_dataset.edge_index, editf_yor, eidtf_t, list(verses), heb_test_dataset.nodes_map,\n",
    "                        dev, model, heb_test_dataset.x, pros_heb, surs_heb, heb_test_verse_alignments_inter, heb_test_verse_alignments_gdfa,\n",
    "                        writer, gnn_dataset_heb.verse_info, calc_numbers=False)\n",
    "        clean_memory()\n",
    "        res.update(res_)\n",
    "\n",
    "    print('going to align train ')\n",
    "    verses = set(train_dataset.nodes_map[editf_yor].keys()).intersection(train_dataset.nodes_map[eidtf_t].keys())\n",
    "    res_ = eval_utils.alignment_test(epoch, train_dataset.edge_index, editf_yor, eidtf_t, list(verses - set(masked_verses)), train_dataset.nodes_map,\n",
    "                    dev, model, train_dataset.x, pros, surs, verse_alignments_inter, verse_alignments_gdfa, writer, gnn_dataset_train.verse_info, calc_numbers=False)\n",
    "    clean_memory()\n",
    "    res.update(res_)\n",
    "\n",
    "    print('going to align blinker ')\n",
    "    verses = set(blinker_test_dataset.nodes_map[editf_yor].keys()).intersection(blinker_test_dataset.nodes_map[eidtf_t].keys())\n",
    "    res_ = eval_utils.alignment_test(epoch, blinker_test_dataset.edge_index, editf_yor, eidtf_t, list(verses), blinker_test_dataset.nodes_map,\n",
    "                    dev, model, blinker_test_dataset.x, pros_blinker, surs_blinker, blinker_verse_alignments_inter, blinker_verse_alignments_gdfa,\n",
    "                    writer, gnn_dataset_blinker.verse_info, calc_numbers=False)\n",
    "    clean_memory()\n",
    "    res.update(res_)\n",
    "\n",
    "    print('going to align grc ')\n",
    "    verses = set(grc_test_dataset.nodes_map[editf_yor].keys()).intersection(grc_test_dataset.nodes_map[eidtf_t].keys())\n",
    "    res_ = eval_utils.alignment_test(epoch, grc_test_dataset.edge_index, editf_yor, eidtf_t, list(verses), grc_test_dataset.nodes_map,\n",
    "                    dev, model, grc_test_dataset.x, pros_grc, surs_grc, grc_test_verse_alignments_inter, grc_test_verse_alignments_gdfa,\n",
    "                    writer, gnn_dataset_grc.verse_info, calc_numbers=False)\n",
    "    clean_memory()\n",
    "    res.update(res_)\n",
    "\n",
    "    \n",
    "    \n",
    "    print(f'going save alignments for {eidtf_t}')\n",
    "    torch.save(res, f'/mounts/work/ayyoob/results/gnn_align/yoruba/{eidtf_t}_alignments.bin')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 2/6202 [00:00<06:39, 15.54it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "going to align heb \n",
      "going to align train \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 6202/6202 [05:18<00:00, 19.46it/s]\n",
      "  3%|         | 2/73 [00:00<00:04, 17.74it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "going to align blinker \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 73/73 [00:04<00:00, 18.16it/s]\n",
      "  0%|          | 2/783 [00:00<00:42, 18.52it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "going to align grc \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 783/783 [00:39<00:00, 20.03it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "going save alignments for hin-x-bible-newworld\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 3/2168 [00:00<01:16, 28.46it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "going to align heb \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 2168/2168 [01:47<00:00, 20.10it/s]\n",
      "  0%|          | 1/23531 [00:00<44:58,  8.72it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "going to align train \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 23531/23531 [19:06<00:00, 20.52it/s]\n",
      "  0%|          | 1/243 [00:00<00:28,  8.47it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "going to align blinker \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 243/243 [00:12<00:00, 19.35it/s]\n",
      "  0%|          | 2/782 [00:00<00:40, 19.09it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "going to align grc \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 782/782 [00:37<00:00, 20.69it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "going save alignments for ita-x-bible-2009\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 3/6203 [00:00<05:32, 18.65it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "going to align heb \n",
      "going to align train \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 6203/6203 [05:06<00:00, 20.22it/s]\n",
      "  3%|         | 2/73 [00:00<00:04, 16.79it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "going to align blinker \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 73/73 [00:03<00:00, 18.78it/s]\n",
      "  0%|          | 2/783 [00:00<00:40, 19.06it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "going to align grc \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 783/783 [00:37<00:00, 20.67it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "going save alignments for prs-x-bible-goodnews\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 2/6203 [00:00<05:42, 18.10it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "going to align heb \n",
      "going to align train \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 6203/6203 [05:12<00:00, 19.85it/s]\n",
      "  3%|         | 2/73 [00:00<00:04, 16.43it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "going to align blinker \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 73/73 [00:04<00:00, 18.00it/s]\n",
      "  0%|          | 2/783 [00:00<00:40, 19.43it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "going to align grc \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 783/783 [00:38<00:00, 20.12it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "going save alignments for ron-x-bible-2006\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 3/2172 [00:00<01:12, 29.76it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "going to align heb \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 2172/2172 [01:57<00:00, 18.43it/s]\n",
      "  0%|          | 1/23560 [00:00<45:56,  8.55it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "going to align train \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 23560/23560 [20:43<00:00, 18.94it/s]\n",
      "  0%|          | 1/246 [00:00<00:32,  7.46it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "going to align blinker \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 246/246 [00:13<00:00, 17.61it/s]\n",
      "  0%|          | 2/783 [00:00<00:45, 17.01it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "going to align grc \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 783/783 [00:40<00:00, 19.29it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "going save alignments for spa-x-bible-newworld\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "global model, decoder\n",
    "#1/0\n",
    "\n",
    "decoder = None\n",
    "model = None\n",
    "\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "features = blinker_test_dataset.features[:]\n",
    "#features_edge = train_dataset.features_edge[:]\n",
    "from pprint import pprint\n",
    "#print('indim',in_dim)\n",
    "#features[-1].out_dim = 50\n",
    "for i in features:\n",
    "    #if i.type==3:\n",
    "    #    i.out_dim=4\n",
    "    print(vars(i))\n",
    "\n",
    "sum(p.out_dim for p in features)\n",
    "#train_dataset.features.pop()\n",
    "#train_dataset.features[0] = afeatures.OneHotFeature(20, 83, 'editf')\n",
    "#train_dataset.features[1] = afeatures.OneHotFeature(32, 150, 'position')\n",
    "#train_dataset.features[2] = afeatures.FloatFeature(4, 'degree_centrality')\n",
    "#train_dataset.features[3] = afeatures.FloatFeature(4, 'closeness_centrality')\n",
    "#train_dataset.features[4] = afeatures.FloatFeature(4, 'betweenness_centrality')\n",
    "#train_dataset.features[5] = afeatures.FloatFeature(4, 'load_centrality')\n",
    "#train_dataset.features[6] = afeatures.FloatFeature(4, 'harmonic_centrality')\n",
    "#train_dataset.features[7] = afeatures.OneHotFeature(32, 250, 'greedy_modularity_community')\n",
    "##train_dataset.features.append(afeatures.MappingFeature(100, 'word'))\n",
    "#torch.save(train_dataset, \"/mounts/work/ayyoob/models/gnn/dataset_helfi_train_community_word.pickle\")\n",
    "#torch.save(train_dataset.features[-3], \"./features.tmp\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# count number of deleted edges by each community detection method\n",
    "# from networkx.algorithms.community import greedy_modularity_communities, asyn_lpa_communities, label_propagation_communities, asyn_fluidc\n",
    "\n",
    "# tmp_verses = [all_verses[2]]\n",
    "# tmp_editions = small_editions[:10]\n",
    "# tmp_dataset, tmp_nodes_map = create_dataset(tmp_verses, verse_alignments_inter, tmp_editions)\n",
    "\n",
    "# tmp_g = pyg_utils.convert.to_networkx(tmp_dataset, to_undirected=True)\n",
    "def count_deleted_edges(tmp_dataset, c):\n",
    "    deleted_edges = 0\n",
    "    for i in range(0, len(tmp_dataset.edge_index[0]), 2):\n",
    "        for comp in c:\n",
    "            if tmp_dataset.edge_index[0][i].item() in comp and tmp_dataset.edge_index[1][i].item() not in comp:\n",
    "                deleted_edges += 1\n",
    "    \n",
    "    return deleted_edges\n",
    "\n",
    "# print(\"eng token count: \", tmp_nodes_map['eng-x-bible-mixed'][tmp_verses[0]])\n",
    "# print(\"original connected components\",nx.number_connected_components(tmp_g))\n",
    "\n",
    "# c = list(greedy_modularity_communities(tmp_g))\n",
    "# print(\"new connected_components\", len(c))\n",
    "# print(\"deleted edges: \", count_deleted_edges(tmp_dataset, c))\n",
    "\n",
    "# c = list(asyn_lpa_communities(tmp_g))\n",
    "# print(\"asyn_lpa_communities number of components\", len(c))\n",
    "# print(\"deleted edges: \", count_deleted_edges(tmp_dataset, c))\n",
    "\n",
    "# c = list(label_propagation_communities(tmp_g))\n",
    "# print(\"label_propagation_communities number of components\", len(c))\n",
    "# print(\"deleted edges: \", count_deleted_edges(tmp_dataset, c))\n",
    "\n",
    "# cents = nx.edge_betweenness_centrality(tmp_g)\n",
    "# vals = sorted(list(cents.values()))\n",
    "# print(vals[0], vals[10], vals[100], vals[1000], vals[2000], vals[3000], vals[10000])\n",
    "# print(vals[-1], vals[-10], vals[-100], vals[-1000], vals[-2000], vals[-3000], vals[-10000])\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# measure different community detection algorithms\n",
    "# from networkx.algorithms.community import greedy_modularity_communities, asyn_lpa_communities, label_propagation_communities, asyn_fluidc\n",
    "\n",
    "# def remove_bad_community_edges(nodes_map, verses, edition_files, alignments):\n",
    "#     edges_tmp = [[],[]]\n",
    "#     res_edges = [[],[]]\n",
    "#     for verse in verses:\n",
    "#         utils.LOG.info(f\"extracting edge features for {verse}\")\n",
    "#         for i,editf1 in enumerate(edition_files):\n",
    "#             for j,editf2 in enumerate(edition_files[i+1:]):\n",
    "#                 aligns = autils.get_aligns(editf1, editf2, alignments[verse])\n",
    "#                 if aligns != None:\n",
    "#                     for align in aligns:\n",
    "#                         n1, node_count = node_nom(verse, editf1, align[0], 0, nodes_map, None, None)\n",
    "#                         n2, node_count = node_nom(verse, editf2, align[1], 0, nodes_map, None, None)\n",
    "#                         edges_tmp[0].extend([n1, n2])\n",
    "#                         edges_tmp[1].extend([n2, n1])\n",
    "\n",
    "#         gnx = convert_to_netx(edges_tmp)\n",
    "#         print('detecting communities')\n",
    "#         coms = greedy_modularity_communities(gnx)\n",
    "\n",
    "#         print('finding good edges')\n",
    "#         for i in range(0, len(edges_tmp[0]), 2):\n",
    "#             for c in coms:\n",
    "#                 if edges_tmp[0][i] in c and edges_tmp[0][i+1] in c:\n",
    "#                     res_edges[0].extend([edges_tmp[0][i], edges_tmp[0][i+1]])\n",
    "#                     res_edges[1].extend([edges_tmp[0][i+1], edges_tmp[0][i]])\n",
    "#         edges_tmp = [[],[]]\n",
    "#     print('to keep edges:', len(res_edges[0]))\n",
    "#     return torch.tensor(res_edges, dtype=torch.long)\n",
    "\n",
    "# # old_edge_index = train_dataset.edge_index\n",
    "# # new_edge_index = remove_bad_community_edges(train_dataset.nodes_map, train_verses, small_editions, verse_alignments_inter)\n",
    "# # train_dataset.edge_index = new_edge_index\n",
    "\n",
    "# # with open(\"./dataset_greedy_modularity_communities.pickle\", 'rb') as inf:\n",
    "# #     train_dataset = pickle.load(inf)\n",
    "\n",
    "# test_dataset = train_dataset\n",
    "\n",
    "# print('orig edge count', old_edge_index.shape)\n",
    "# print('new edge count', train_dataset.edge_index.shape)\n",
    "# print(\"done\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nodes_map = train_dataset.nodes_map\n",
    "bad_edition_files = []\n",
    "for edit in nodes_map:\n",
    "    bad_count = 0\n",
    "    for verse in nodes_map[edit]:\n",
    "        if len(nodes_map[edit][verse].keys()) < 2:\n",
    "            bad_count += 1\n",
    "        if bad_count > 1:\n",
    "            bad_edition_files.append(edit)\n",
    "            break\n",
    "print(bad_edition_files)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_japanese_nodes = set()\n",
    "nodes_map = train_dataset.nodes_map\n",
    "\n",
    "for bad_editionf in bad_edition_files:\n",
    "    for verse in nodes_map[bad_editionf]:\n",
    "        for item in nodes_map[bad_editionf][verse].items():\n",
    "            all_japanese_nodes.add(item[1])\n",
    "\n",
    "print(\" all japansese nodes: \", len(all_japanese_nodes))\n",
    "edge_index = train_dataset.edge_index.to('cpu')\n",
    "remaining_edges_index = []\n",
    "for i in tqdm(range(0, edge_index.shape[1], 2)):\n",
    "    if edge_index[0, i].item() not in all_japanese_nodes and edge_index[0, i+1].item() not in all_japanese_nodes:\n",
    "        remaining_edges_index.extend([i, i+1])\n",
    "\n",
    "print('original total edges count', edge_index.shape)\n",
    "print('remaining edge count', len(remaining_edges_index))\n",
    "train_dataset.edge_index = edge_index[:, remaining_edges_index]\n",
    "train_dataset.edge_index.shape\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\n",
    "print(\"################# you have to run first three cells first ###################\")\n",
    "from networkx.algorithms.community import greedy_modularity_communities, asyn_lpa_communities, label_propagation_communities, asyn_fluidc\n",
    "from my_utils import align_utils as autils, utils\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def node_nom(verse, editf, tok_nom, node_count, nodes_map, x=None, edit_fs=None, features = None):\n",
    "    utils.setup_dict_entry(nodes_map, editf, {})\n",
    "    utils.setup_dict_entry(nodes_map[editf], verse, {})\n",
    "    if not tok_nom in nodes_map[editf][verse]:\n",
    "        nodes_map[editf][verse][tok_nom] = node_count\n",
    "        x.append([edit_fs.index(editf), tok_nom]) # TODO we should have better representation \n",
    "        node_count += 1\n",
    "\n",
    "    return nodes_map[editf][verse][tok_nom], node_count\n",
    "\n",
    "def create_dataset(verse, alignments, edition_files):\n",
    "    node_count = 0\n",
    "    edges = [[],[]]\n",
    "    x = []\n",
    "    nodes_map = {}\n",
    "    features = []\n",
    "    \n",
    "    for i,editf1 in enumerate(edition_files):\n",
    "        for j,editf2 in enumerate(edition_files[i+1:]):\n",
    "            aligns = autils.get_aligns(editf1, editf2, alignments[verse])\n",
    "            if aligns != None:\n",
    "                for align in aligns:\n",
    "                    n1, node_count = node_nom(verse, editf1, align[0], node_count, nodes_map, x, edition_files, features)\n",
    "                    n2, node_count = node_nom(verse, editf2, align[1], node_count, nodes_map, x, edition_files, features)\n",
    "                    edges[0].extend([n1, n2])\n",
    "                    edges[1].extend([n2, n1])\n",
    "\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    res = Data(x=x, edge_index=edge_index)\n",
    "    \n",
    "    res.nodes_map = nodes_map\n",
    "    res.features = features\n",
    "    return res, nodes_map"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "################# you have to run first three cells first ###################\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "blinker_verse_alignments_inter = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_inter.pickle\")\n",
    "blinker_verse_alignments_gdfa = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_gdfa.pickle\")\n",
    "\n",
    "heb_test_verse_alignments_inter = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_heb_inter.pickle\")\n",
    "heb_test_verse_alignments_gdfa = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_heb_gdfa.pickle\")\n",
    "\n",
    "grc_test_verse_alignments_inter = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_grc_inter.pickle\")\n",
    "grc_test_verse_alignments_gdfa = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_grc_gdfa.pickle\")\n",
    "\n",
    "editions_file =  \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi_lang_list.txt\"\n",
    "editions, langs = autils.load_simalign_editions(editions_file)\n",
    "current_editions = [editions[lang] for lang in langs]\n",
    "if 'jpn-x-bible-newworld' in  current_editions[:]:\n",
    "     current_editions.remove('jpn-x-bible-newworld')\n",
    "if 'grc-x-bible-unaccented' in  current_editions[:]:\n",
    "     current_editions.remove('grc-x-bible-unaccented')\n",
    "\n",
    "test_gold_eng_fra = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/eng_fra_pbc/eng-fra.gold\"\n",
    "pros_blinker, surs_blinker = autils.load_gold(test_gold_eng_fra)\n",
    "\n",
    "test_gold_helfi_heb = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-heb-fin-gold-alignments_test.txt\"\n",
    "pros_heb, surs_heb = autils.load_gold(test_gold_helfi_heb)\n",
    "\n",
    "test_gold_grc = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-grc-fin-gold-alignments_test.txt\"\n",
    "pros_grc, surs_grc = autils.load_gold(test_gold_grc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "\n",
    "def get_community_edges(c, verse):\n",
    "    res = []\n",
    "    edges = []\n",
    "    for n1 in all_nodes_map[editf1][verse].items():\n",
    "        for n2 in all_nodes_map[editf2][verse].items():\n",
    "            for com in c:\n",
    "                if n1[1] in com and n2[1] in com:\n",
    "                    res.append((n1[0], n2[0]))\n",
    "                    edges.append((n1[1], n2[1]))\n",
    "    \n",
    "    return res, edges\n",
    "\n",
    "def intersect(e1, e2):\n",
    "    res = set()\n",
    "    for item in e1:\n",
    "        if item in e2:\n",
    "            res.add(item)\n",
    "    \n",
    "    return res\n",
    "\n",
    "datasets = {'blinker' : [blinker_verse_alignments_inter, blinker_verse_alignments_gdfa, 'eng-x-bible-mixed', 'fra-x-bible-louissegond', pros_blinker, surs_blinker],\n",
    "     'heb': [heb_test_verse_alignments_inter, heb_test_verse_alignments_gdfa, 'fin-x-bible-helfi', 'heb-x-bible-helfi', pros_heb, surs_heb],\n",
    "     'grc': [grc_test_verse_alignments_inter, grc_test_verse_alignments_gdfa, 'fin-x-bible-helfi', 'grc-x-bible-helfi', pros_grc, surs_grc]}\n",
    "\n",
    "for dataset in datasets:\n",
    "    print('community for ', dataset)\n",
    "    test_verses = list(datasets[dataset][0].keys())[:]\n",
    "    verse_alignments_inter = datasets[dataset][0]\n",
    "    verse_alignments_gdfa = datasets[dataset][1]\n",
    "    editf1, editf2 = datasets[dataset][2], datasets[dataset][3]\n",
    "    pros, surs = datasets[dataset][4], datasets[dataset][5]\n",
    "\n",
    "    measures = {}\n",
    "    measures['intersection']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "    measures['gdfa']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "    measures['c1_all']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "    measures['c1_inter']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "    measures['c3_all']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "    measures['c3_inter']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "\n",
    "    no_c_sum = 0\n",
    "    c1_sum = 0\n",
    "    c3_sum = 0\n",
    "    nodes = 0\n",
    "    total_edges = 0\n",
    "    removed_edges1 = 0\n",
    "    removed_edges2 = 0\n",
    "    for verse in test_verses:\n",
    "        inter_edges = autils.get_aligns(editf1, editf2, verse_alignments_inter[verse])\n",
    "\n",
    "        all_dataset, all_nodes_map = create_dataset(verse, verse_alignments_inter, current_editions)\n",
    "        \n",
    "\n",
    "        nodes += all_dataset.x.shape[0] / len(all_nodes_map)\n",
    "        g = pyg_utils.convert.to_networkx(all_dataset, to_undirected=True)\n",
    "        \n",
    "        c1 = list(greedy_modularity_communities(g))\n",
    "        c3 = list(label_propagation_communities(g))\n",
    "\n",
    "        \n",
    "\n",
    "        c1_edges, graph_edges1 = get_community_edges(c1, verse)\n",
    "        c3_edges, graph_edges2 = get_community_edges(c3, verse)\n",
    "\n",
    "        no_c_sum += nx.number_connected_components(g)\n",
    "        c1_sum += len(c1)\n",
    "        c3_sum += len(c3)\n",
    "\n",
    "        total_edges += len(g.edges)\n",
    "        removed_edges1 += count_deleted_edges(all_dataset, c1)\n",
    "        removed_edges2 += count_deleted_edges(all_dataset, c3)\n",
    "\n",
    "        print(1, removed_edges1/ total_edges)\n",
    "        print(2, removed_edges2/ total_edges)\n",
    "        \n",
    "        #autils.calc_and_update_alignment_score(inter_edges, pros[verse], surs[verse], measures['intersection'])\n",
    "        #autils.calc_and_update_alignment_score(autils.get_aligns(editf1, editf2, verse_alignments_gdfa[verse]), pros[verse], surs[verse], measures['gdfa'])\n",
    "        #autils.calc_and_update_alignment_score(c1_edges, pros[verse], surs[verse], measures['c1_all'])\n",
    "        #autils.calc_and_update_alignment_score(c3_edges, pros[verse], surs[verse], measures['c3_all'])\n",
    "\n",
    "        #autils.calc_and_update_alignment_score(intersect(c1_edges, inter_edges), pros[verse], surs[verse], measures['c1_inter'])\n",
    "        #autils.calc_and_update_alignment_score(intersect(c3_edges, inter_edges), pros[verse], surs[verse], measures['c3_inter'])\n",
    "\n",
    "    #print('avg sentence len', nodes/len(test_verses))\n",
    "    #print('communities counts:' )\n",
    "    #print('\\t\\t\\ttotal\\t\\t average')\n",
    "    #print(f'original:\\t{no_c_sum}\\t\\t{no_c_sum/len(test_verses)}')\n",
    "    #print(f'original:\\t{c1_sum}\\t\\t{c1_sum/len(test_verses)}')\n",
    "    #print(f'original:\\t{c3_sum}\\t\\t{c3_sum/len(test_verses)}')\n",
    "    #for item in measures:\n",
    "    #    print(item, measures[item])\n",
    "\n",
    "    print('gmc', removed_edges1/total_edges)\n",
    "    print('lpc', removed_edges2/total_edges)\n",
    "    \n",
    "    \n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "community for  blinker\n",
      "1 0.03891525423728814\n",
      "2 0.0768813559322034\n",
      "1 0.031156665761607385\n",
      "2 0.07595710019006245\n",
      "1 0.03141783029001074\n",
      "2 0.08941997851772288\n",
      "1 0.036958019423517126\n",
      "2 0.08807253816921128\n",
      "1 0.0339743347087103\n",
      "2 0.07972139152818261\n",
      "1 0.031110975218235463\n",
      "2 0.07070676185962606\n",
      "1 0.029927037965465572\n",
      "2 0.06666013135967062\n",
      "1 0.030871571451994474\n",
      "2 0.06742323097463285\n",
      "1 0.03122713600520283\n",
      "2 0.06761645394683359\n",
      "1 0.03139619594315858\n",
      "2 0.06585564947204887\n",
      "1 0.03229896508286747\n",
      "2 0.06831111733177452\n",
      "1 0.031419583470467125\n",
      "2 0.06766882764004868\n",
      "1 0.030002786215707462\n",
      "2 0.06588380800935081\n",
      "1 0.02935246641363552\n",
      "2 0.06545100775094567\n",
      "1 0.030398963916969456\n",
      "2 0.06940796968497799\n",
      "1 0.03141537408759124\n",
      "2 0.07081432481751825\n",
      "1 0.03185434972235815\n",
      "2 0.0723889964470353\n",
      "1 0.031816824388166236\n",
      "2 0.07331724296617596\n",
      "1 0.03257295148613244\n",
      "2 0.0755081759869907\n",
      "1 0.03293060262266097\n",
      "2 0.07660406139765469\n",
      "1 0.03281812141952791\n",
      "2 0.07716478842047643\n",
      "1 0.03297186861543297\n",
      "2 0.07732594688040233\n",
      "1 0.03337991345113595\n",
      "2 0.07932894578675322\n",
      "1 0.032742306945253756\n",
      "2 0.0791226171647382\n",
      "1 0.03305532378859751\n",
      "2 0.07866262893537525\n",
      "1 0.03254632647660602\n",
      "2 0.07901157274434756\n",
      "1 0.032386049949937774\n",
      "2 0.07853063465179459\n",
      "1 0.03229167947472611\n",
      "2 0.07795804694512412\n",
      "1 0.03216413171660972\n",
      "2 0.07729430618971304\n",
      "1 0.0327245810317386\n",
      "2 0.07725458758543208\n",
      "1 0.032647391301902605\n",
      "2 0.0768886839248857\n",
      "1 0.03321816252686563\n",
      "2 0.07735941453328618\n",
      "1 0.033445097719269616\n",
      "2 0.07795171146892091\n",
      "1 0.03320064996116584\n",
      "2 0.07822272411396804\n",
      "1 0.033092413738812984\n",
      "2 0.07827098780313803\n",
      "1 0.03262047171667603\n",
      "2 0.07769409299205113\n",
      "1 0.032120116555147855\n",
      "2 0.07681757680408663\n",
      "1 0.0321564665444043\n",
      "2 0.07698795313031924\n",
      "1 0.03246254818929012\n",
      "2 0.07663714219182571\n",
      "1 0.032836888384029615\n",
      "2 0.07739913332491902\n",
      "1 0.0324514795956018\n",
      "2 0.07749735566870834\n",
      "1 0.032309334645575546\n",
      "2 0.07735151952886271\n",
      "1 0.03241659596754089\n",
      "2 0.07760541577849883\n",
      "1 0.032804062375656064\n",
      "2 0.07838262879663489\n",
      "1 0.032597121688253385\n",
      "2 0.07812772461999087\n",
      "1 0.03297316824854675\n",
      "2 0.07837878273688974\n",
      "1 0.032841442144438925\n",
      "2 0.07849592822414989\n",
      "1 0.03276440162837386\n",
      "2 0.07888132946687486\n",
      "1 0.032350300669094605\n",
      "2 0.0783670703819768\n",
      "1 0.032575990276415355\n",
      "2 0.0789149483599708\n",
      "1 0.032716509932851794\n",
      "2 0.07868556764423747\n",
      "1 0.03266053922358507\n",
      "2 0.07841010899200124\n",
      "1 0.032627405442844454\n",
      "2 0.07805333665707934\n",
      "1 0.03232125824486169\n",
      "2 0.07754799096896867\n",
      "1 0.03247679739308528\n",
      "2 0.07698164348309389\n",
      "1 0.03252714804092963\n",
      "2 0.076994829581153\n",
      "1 0.032669768906919666\n",
      "2 0.07769259822586079\n",
      "1 0.032450087523061576\n",
      "2 0.07711332769407148\n",
      "1 0.032766766632600065\n",
      "2 0.07650325866604119\n",
      "1 0.03296378336624415\n",
      "2 0.07712184923300994\n",
      "1 0.032833083701506664\n",
      "2 0.0772088383111802\n",
      "1 0.03287511389932283\n",
      "2 0.07759444630179552\n",
      "1 0.03317783903700833\n",
      "2 0.07802611943661536\n",
      "1 0.033035444436997574\n",
      "2 0.07805641721474936\n",
      "1 0.03286570992183223\n",
      "2 0.0775253289070761\n",
      "1 0.032804527962836345\n",
      "2 0.07730538818691422\n",
      "1 0.032917863594188315\n",
      "2 0.07744777946126528\n",
      "1 0.03302185837536867\n",
      "2 0.07737737845705445\n",
      "1 0.032960071126240754\n",
      "2 0.0774246690621059\n",
      "1 0.03284912706475524\n",
      "2 0.0772895736915378\n",
      "1 0.03264083520179372\n",
      "2 0.07677620515695067\n",
      "1 0.03260388589333146\n",
      "2 0.07676817158285974\n",
      "1 0.03264422692540518\n",
      "2 0.07674437198040045\n",
      "1 0.03263571458555275\n",
      "2 0.07676755987733527\n",
      "1 0.032666658477015185\n",
      "2 0.07693879761770483\n",
      "1 0.032605588272807635\n",
      "2 0.0766144461206233\n",
      "1 0.03245609741506366\n",
      "2 0.07623413986827865\n",
      "1 0.03243069029588436\n",
      "2 0.07597948558215392\n",
      "1 0.032336095885499114\n",
      "2 0.07557255258321639\n",
      "1 0.03250767738807176\n",
      "2 0.07588704669383171\n",
      "1 0.03260793639706116\n",
      "2 0.07582165437915801\n",
      "1 0.03252852860404466\n",
      "2 0.07552444328489417\n",
      "1 0.0325765788088085\n",
      "2 0.0757250080011563\n",
      "1 0.03234274061957145\n",
      "2 0.0755217993233889\n",
      "1 0.03245130898355903\n",
      "2 0.075680238820113\n",
      "1 0.03244642742441536\n",
      "2 0.07548335136791032\n",
      "1 0.03231768280253541\n",
      "2 0.07503518700070859\n",
      "1 0.03229087259480021\n",
      "2 0.07483365191975452\n",
      "1 0.03224952762537847\n",
      "2 0.0748969881849432\n",
      "1 0.032132145893165924\n",
      "2 0.07467387634748991\n",
      "1 0.03213995904904628\n",
      "2 0.07468621308680479\n",
      "1 0.032185688280745\n",
      "2 0.0746598619241927\n",
      "1 0.03215375724631138\n",
      "2 0.07478283640041175\n",
      "1 0.03214576678172502\n",
      "2 0.07470395700676771\n",
      "1 0.032229302983155364\n",
      "2 0.07460996611180971\n",
      "1 0.032084753752119236\n",
      "2 0.07450503451107644\n",
      "1 0.03187198089085236\n",
      "2 0.07481765870649544\n",
      "1 0.031872947616635755\n",
      "2 0.07470759302181271\n",
      "1 0.03184732992172387\n",
      "2 0.07462701475293577\n",
      "1 0.03194662447868996\n",
      "2 0.07469094131923018\n",
      "1 0.032098541798750116\n",
      "2 0.07465479905476283\n",
      "1 0.03220616394601169\n",
      "2 0.07513081146871199\n",
      "1 0.032157805681716986\n",
      "2 0.07498603453719671\n",
      "1 0.032137292522437834\n",
      "2 0.07476515922172575\n",
      "1 0.032118584813312825\n",
      "2 0.07481471122919871\n",
      "1 0.03209811093748388\n",
      "2 0.07464427801273449\n",
      "1 0.03201956071960235\n",
      "2 0.07443292205989958\n",
      "1 0.03208888275481333\n",
      "2 0.07457214822993583\n",
      "1 0.03204682609942519\n",
      "2 0.0743023777758382\n",
      "1 0.032231814486469744\n",
      "2 0.07439278386434904\n",
      "1 0.032069276559511285\n",
      "2 0.07418867739598792\n",
      "1 0.03205390435437954\n",
      "2 0.07396062365237217\n",
      "1 0.0320403820517858\n",
      "2 0.07392562048789961\n",
      "1 0.032013546089433485\n",
      "2 0.07379734368718457\n",
      "1 0.03187602583190016\n",
      "2 0.07341865490811285\n",
      "1 0.031996850303136204\n",
      "2 0.07350324886217938\n",
      "1 0.032019789381919364\n",
      "2 0.07337804191668178\n",
      "1 0.03211974700924614\n",
      "2 0.07330962153634916\n",
      "1 0.03226055150232853\n",
      "2 0.07345630479038824\n",
      "1 0.03235300502948945\n",
      "2 0.07362665991243622\n",
      "1 0.03229287240352666\n",
      "2 0.07354418971603638\n",
      "1 0.032310486500726994\n",
      "2 0.07351525178383123\n",
      "1 0.03243468322173587\n",
      "2 0.07377838747788787\n",
      "1 0.03253008715683194\n",
      "2 0.0736389768132614\n",
      "1 0.03263814100054941\n",
      "2 0.07367120371878881\n",
      "1 0.0328526163755218\n",
      "2 0.07376792166551276\n",
      "1 0.032775050033077714\n",
      "2 0.07374906893006615\n",
      "1 0.032805411201847726\n",
      "2 0.07381973658885314\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-2b46cd44d661>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mtotal_edges\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mremoved_edges1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcount_deleted_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mremoved_edges2\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcount_deleted_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremoved_edges1\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0mtotal_edges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-b7c81b1dac17>\u001b[0m in \u001b[0;36mcount_deleted_edges\u001b[0;34m(tmp_dataset, c)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcomp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mtmp_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomp\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtmp_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0mdeleted_edges\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}