{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/Users/student/ayyoob/anaconda3/envs/multalign_graph/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch, sys\n",
    "sys.path.insert(0, '../')\n",
    "from my_utils import gpu_utils\n",
    "import importlib, gc\n",
    "from my_utils.alignment_features import *\n",
    "import my_utils.alignment_features as afeatures\n",
    "importlib.reload(afeatures)\n",
    "import gnn_utils.graph_utils as gutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch-geometric\n",
    "# !pip install tensorboardX\n",
    "\n",
    "# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "# !unzip ngrok-stable-linux-amd64.zip\n",
    "\n",
    "#  print(torch.version.cuda)\n",
    "#  print(torch.__version__)    \n",
    "\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24159\n",
      "reading inter verse alignments\n",
      "done reading inter verse alignments\n"
     ]
    }
   ],
   "source": [
    "from my_utils import align_utils as autils, utils\n",
    "import argparse\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "\n",
    "# set random seed\n",
    "config_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc-ui-demo/config_pbc.ini\"\n",
    "utils.setup(config_file)\n",
    "\n",
    "params = argparse.Namespace()\n",
    "\n",
    "\n",
    "params.gold_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-fin-grc-gold-alignments_train.txt\"\n",
    "pros, surs = autils.load_gold(params.gold_file)\n",
    "all_verses = list(pros.keys())\n",
    "params.gold_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-fin-heb-gold-alignments_train.txt\"\n",
    "pros, surs = autils.load_gold(params.gold_file)\n",
    "all_verses.extend(list(pros.keys()))\n",
    "all_verses = list(set(all_verses))\n",
    "print(len(all_verses))\n",
    "\n",
    "params.editions_file =  \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi_lang_list.txt\"\n",
    "editions, langs = autils.load_simalign_editions(params.editions_file)\n",
    "current_editions = [editions[lang] for lang in langs]\n",
    "\n",
    "def get_pruned_verse_alignments(args):\n",
    "    verse, current_editions = args\n",
    "    \n",
    "    verse_aligns_inter = autils.get_verse_alignments(verse)\n",
    "    verse_aligns_gdfa = autils.get_verse_alignments(verse, gdfa=True)\n",
    "\n",
    "    autils.prune_non_necessary_alignments(verse_aligns_inter, current_editions)\n",
    "    autils.prune_non_necessary_alignments(verse_aligns_gdfa, current_editions)\n",
    "\n",
    "    gc.collect()\n",
    "    return verse_aligns_inter, verse_aligns_gdfa\n",
    "    \n",
    "\n",
    "verse_alignments_inter = {}\n",
    "verse_alignments_gdfa = {}\n",
    "args = []\n",
    "for i,verse in enumerate(all_verses):\n",
    "    args.append((verse, current_editions[:]))\n",
    "\n",
    "#print('going to get alignments')\n",
    "#with Pool(20) as p:\n",
    "#    all_res = p.map(get_pruned_verse_alignments, args)\n",
    "\n",
    "#for i,verse in enumerate(all_verses):\n",
    "#    verse_aligns_inter, verse_aligns_gdfa = all_res[i]\n",
    "    \n",
    "    #verse_alignments_inter[verse] = verse_aligns_inter\n",
    "    #verse_alignments_gdfa[verse] = verse_aligns_gdfa\n",
    "\n",
    "#utils.LOG.info(\"done reading alignments\")\n",
    "#torch.save(verse_alignments_inter, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_train_inter.pickle\")\n",
    "#torch.save(verse_alignments_gdfa, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_train_gdfa.pickle\")\n",
    "#utils.LOG.info('done saving pruned alignments')\n",
    "\n",
    "print('reading inter verse alignments')\n",
    "verse_alignments_inter = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_train_inter.pickle\")\n",
    "gc.collect()\n",
    "print('done reading inter verse alignments')\n",
    "\n",
    "for verse in all_verses[:]:\n",
    "    if len(verse_alignments_inter[verse].keys()) < 10:\n",
    "        all_verses.remove(verse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importlib.reload(afeatures)\n",
    "class Encoder2(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Encoder2, self).__init__()\n",
    "        self.conv1 = pyg_nn.GATConv(in_channels, 2*out_channels)\n",
    "        self.conv2 = pyg_nn.GATConv(2 * out_channels , out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.elu(self.conv1(x, edge_index, ))\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features, n_head = 2, edge_feature_dim = 0,):\n",
    "        super(Encoder, self).__init__()\n",
    "        #self.lin = nn.Linear(in_channels, out_channels)\n",
    "        self.conv1 = pyg_nn.GATConv(in_channels, 2*out_channels, heads= n_head)\n",
    "        self.conv2 = pyg_nn.GATConv(2 * n_head *  out_channels , out_channels, heads= 1)\n",
    "        #self.conv3 = pyg_nn.GATConv(2 * n_head *  out_channels , out_channels, heads= n_head)\n",
    "        #self.f_embedding = nn.Linear(in_channels, in_channels)\n",
    "        self.fin_lin = nn.Linear(out_channels, out_channels)\n",
    "        \n",
    "\n",
    "        self.feature_encoder = afeatures.FeatureEncoding(features, word_vectors)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.feature_encoder(x, dev)\n",
    "        x = F.elu(self.conv1(x, edge_index, ))\n",
    "        #x = self.conv_gin(x, edge_index)\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        return F.relu(self.fin_lin(x))#, self.conv3(x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSDecoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_class, drop_out=0):\n",
    "        super(POSDecoder, self).__init__()\n",
    "\n",
    "        self.transfer = nn.Sequential(nn.Linear(input_size, hidden_size), nn.ReLU(), nn.Dropout(drop_out),\n",
    "                        nn.Linear(hidden_size, n_class))\n",
    "\n",
    "    def forward(self, z, index):\n",
    "        h = z[index, :]\n",
    "\n",
    "        res = self.transfer(h)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_memory():\n",
    "    gc.collect()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def train(epoch):\n",
    "    global optimizer\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for i,batch_ in enumerate(tqdm(data_loader)):\n",
    "        for verse in batch_:\n",
    "            if verse in masked_verses:\n",
    "                continue\n",
    "            batch = batch_[verse]\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            x = batch['x'].to(dev)\n",
    "            edge_index = batch['edge_index'].to(dev)\n",
    "            if torch.max(edge_index) >= x.shape[0]:\n",
    "                print(torch.max(edge_index), x.shape)\n",
    "                print(batch)\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                z = model.encode(x, edge_index)\n",
    "            except Exception as e:\n",
    "                global sag, khar, gav\n",
    "                sag, khar, gav =  (i, batch_, verse)\n",
    "                print(e)\n",
    "                1/0\n",
    "\n",
    "            nodes = batch['nodes'].to(dev)\n",
    "            \n",
    "            preds = model.decoder(z, nodes)\n",
    "            \n",
    "            labels = batch['labels'].to(dev)\n",
    "            labels = torch.max(labels, dim=1).indices\n",
    "\n",
    "            loss = criterion(labels, preds)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() \n",
    "            \n",
    "    \n",
    "    writer.add_scalar(\"loss\", total_loss, epoch)\n",
    "    print(f\"train loss: {total_loss}\")\n",
    "    print(f\"cluster loss: {cluster_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data_loader):\n",
    "    model.eval()\n",
    "    tot_correct = tot = 0\n",
    "    with torch.no_grad():\n",
    "        for i,batch_ in enumerate(tqdm(data_loader)):\n",
    "            for verse in batch_:\n",
    "                batch = batch_[verse]\n",
    "                \n",
    "                x = batch['x'].to(dev)\n",
    "                edge_index = batch['edge_index'].to(dev)\n",
    "                if torch.max(edge_index) >= x.shape[0]:\n",
    "                    print(torch.max(edge_index), x.shape)\n",
    "                    print(batch)\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    z = model.encode(x, edge_index)\n",
    "                except Exception as e:\n",
    "                    global sag, khar, gav\n",
    "                    sag, khar, gav =  (i, batch_, verse)\n",
    "                    print(e)\n",
    "                    1/0\n",
    "\n",
    "                nodes = batch['nodes'].to(dev)\n",
    "                \n",
    "                preds = model.decoder(z, nodes)\n",
    "                preds = torch.max(preds, dim=1).indices\n",
    "                \n",
    "                labels = batch['labels'].to(dev)\n",
    "                labels = torch.max(labels, dim=1).indices\n",
    "                \n",
    "                corrects = preds == labels\n",
    "                tot += preds.shape[0]\n",
    "                tot_correct += corrects.sum().float()\n",
    "\n",
    "\n",
    "    return tot_correct/tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30723771, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "train_verses = all_verses[:]\n",
    "test_verses = all_verses[:] \n",
    "editf1 = 'fin-x-bible-helfi'\n",
    "editf2 = \"heb-x-bible-helfi\"\n",
    "\n",
    "\n",
    "if 'jpn-x-bible-newworld' in  current_editions[:]:\n",
    "     current_editions.remove('jpn-x-bible-newworld')\n",
    "if 'grc-x-bible-unaccented' in  current_editions[:]:\n",
    "     current_editions.remove('grc-x-bible-unaccented')\n",
    "\n",
    "train_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_helfi_train_community_word.pickle\", map_location=torch.device('cpu'))\n",
    "#train_dataset, train_nodes_map = create_dataset(train_verses, verse_alignments_inter, small_editions)\n",
    "features = train_dataset.features\n",
    "train_nodes_map = train_dataset.nodes_map\n",
    "#edge_index_intra_sent = train_dataset.edge_index_intra_sent\n",
    "#test_edge_index_intra_sent = edge_index_intra_sent\n",
    "\n",
    "# test_dataset, test_nodes_map = create_dataset(test_verses, verse_alignments_inter, small_editions)\n",
    "test_dataset, test_nodes_map = train_dataset, train_nodes_map\n",
    "test_verses = train_verses\n",
    "print(train_dataset.x.shape)\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import collections\n",
    "\n",
    "postag_map = {\"ADJ\": 0, \"ADP\": 1, \"ADV\": 2, \"AUX\": 3, \"CCONJ\": 4, \"DET\": 5, \"INTJ\": 6, \"NOUN\": 7, \"NUM\": 8, \"PART\": 9, \"PRON\": 10, \"PROPN\": 11, \"PUNCT\": 12, \"SCONJ\": 13, \"SYM\": 14, \"VERB\": 15, \"X\": 16}\n",
    "\n",
    "pos_lang_list = [\"eng-x-bible-mixed\", \"deu-x-bible-newworld\", \"ces-x-bible-newworld\", \n",
    "\t\t\"fra-x-bible-louissegond\",\"hin-x-bible-newworld\", \"ita-x-bible-2009\", \n",
    "\t\t\"prs-x-bible-goodnews\", \"ron-x-bible-2006\", \"spa-x-bible-newworld\"]\n",
    "\n",
    "def get_pos_tags(dataset):\n",
    "\tall_tags = {}\n",
    "\tfor lang in pos_lang_list:\n",
    "\t\tif lang not in dataset.nodes_map:\n",
    "\t\t\tcontinue\n",
    "\t\tall_tags[lang] = {}\n",
    "\t\twith codecs.open(F\"/mounts/work/mjalili/projects/gnn-align/data/pbc_pos_tags/{lang}.conllu\", \"r\", \"utf-8\") as lang_pos:\n",
    "\t\t\ttag_sent = []\n",
    "\t\t\tsent_id = \"\"\n",
    "\t\t\tfor sline in lang_pos:\n",
    "\t\t\t\tsline = sline.strip()\n",
    "\t\t\t\tif sline == \"\":\n",
    "\t\t\t\t\tif sent_id not in dataset.nodes_map[lang]:\n",
    "\t\t\t\t\t\ttag_sent = []\n",
    "\t\t\t\t\t\tsent_id = \"\"\n",
    "\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t\tall_tags[lang][sent_id] = [p[3] for p in tag_sent]\n",
    "\t\t\t\t\ttag_sent = []\n",
    "\t\t\t\t\tsent_id = \"\"\n",
    "\t\t\t\telif \"# verse_id\" in sline:\n",
    "\t\t\t\t\tsent_id = sline.split()[-1]\n",
    "\t\t\t\telif sline[0] == \"#\":\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttag_sent.append(sline.split(\"\\t\"))\n",
    "\n",
    "\tpos_labels = torch.zeros((dataset.x.shape[0], len(postag_map)))\n",
    "\tpos_node_cover = collections.defaultdict(list)\n",
    "\n",
    "\tfor lang in all_tags:\n",
    "\t\tfor sent_id in all_tags[lang]:\n",
    "\t\t\tsent_tags = all_tags[lang][sent_id]\n",
    "\t\t\tfor w_i in range(len(sent_tags)):\n",
    "\t\t\t\tif w_i not in dataset.nodes_map[lang][sent_id]:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tpos_labels[dataset.nodes_map[lang][sent_id][w_i], postag_map[sent_tags[w_i]]] = 1\n",
    "\t\t\t\tpos_node_cover[sent_id].append(dataset.nodes_map[lang][sent_id][w_i])\n",
    "\n",
    "\treturn pos_labels, pos_node_cover\n",
    "\t#pos_pickle = {\"pos_labels\": pos_labels, \"node_ids_train\": pos_ids_train, \"node_ids_dev\": pos_ids_dev}\n",
    "\t#torch.save(pos_pickle, '/mounts/work/ayyoob/models/gnn/postag')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.models import Word2Vec\n",
    "#w2v_model = Word2Vec.load(\"/mounts/work/ayyoob/models/w2v/word2vec_helfi_langs_15e.model\")\n",
    "\n",
    "#print(w2v_model.wv.vectors.shape)\n",
    "\n",
    "#word_vectors = torch.from_numpy(w2v_model.wv.vectors).float()\n",
    "\n",
    "#print(word_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-18 17:59:44,617 - analytics - INFO - done reading alignments\n",
      "2021-10-18 17:59:44,619 - analytics - INFO - done saving pruned alignments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading inter verse alignments\n",
      "done reading inter verse alignments\n"
     ]
    }
   ],
   "source": [
    "blinker_test_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_blinker_full_community_word.pickle\", map_location=torch.device('cpu'))\n",
    "editf12 = \"eng-x-bible-mixed\"\n",
    "editf22 = 'fra-x-bible-louissegond'\n",
    "\n",
    "test_gold_eng_fra = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/eng_fra_pbc/eng-fra.gold\"\n",
    "\n",
    "pros_blinker, surs_blinker = autils.load_gold(test_gold_eng_fra)\n",
    "blinker_verses = list(pros_blinker.keys())\n",
    "\n",
    "#blinker_verse_alignments_inter = {}\n",
    "#blinker_verse_alignments_gdfa = {}\n",
    "#args = []\n",
    "#for i,verse in enumerate(blinker_verses):\n",
    "#    args.append((verse, current_editions))\n",
    "\n",
    "#with Pool(20) as p:\n",
    "#    all_res = p.map(get_pruned_verse_alignments, args)\n",
    "\n",
    "#for i,verse in enumerate(blinker_verses):\n",
    "#    verse_aligns_inter, verse_aligns_gdfa = all_res[i]\n",
    "    \n",
    "#    blinker_verse_alignments_inter[verse] = verse_aligns_inter\n",
    "#    blinker_verse_alignments_gdfa[verse] = verse_aligns_gdfa\n",
    "\n",
    "utils.LOG.info(\"done reading alignments\")\n",
    "#torch.save(blinker_verse_alignments_inter, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_inter.pickle\")\n",
    "#torch.save(blinker_verse_alignments_gdfa, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_gdfa.pickle\")\n",
    "utils.LOG.info('done saving pruned alignments')\n",
    "\n",
    "print('reading inter verse alignments')\n",
    "blinker_verse_alignments_inter = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_inter.pickle\")\n",
    "#blinker_verse_alignments_gdfa = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_gdfa.pickle\")\n",
    "gc.collect()\n",
    "print('done reading inter verse alignments')\n",
    "\n",
    "verses_map = {}\n",
    "\n",
    "for edit in blinker_test_dataset.nodes_map:\n",
    "    for verse in blinker_test_dataset.nodes_map[edit]:\n",
    "        if verse not in verses_map:\n",
    "            for tok in blinker_test_dataset.nodes_map[edit][verse]:\n",
    "                verses_map[verse] = blinker_test_dataset.nodes_map[edit][verse][tok]\n",
    "                break\n",
    "\n",
    "sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "blinker_verses = [item[0] for item in sorted_verses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##importlib.reload(afeatures)\n",
    "#grc_test_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_helfi_grc_test_community_word.pickle\", map_location=torch.device('cpu'))\n",
    "#editf_fin = \"fin-x-bible-helfi\"\n",
    "#editf_grc = 'grc-x-bible-helfi'\n",
    "\n",
    "#test_gold_grc = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-fin-grc-gold-alignments_test.txt\"\n",
    "\n",
    "#pros_grc, surs_grc = autils.load_gold(test_gold_grc)\n",
    "#grc_verses = list(pros_grc.keys())\n",
    "\n",
    "\n",
    "#grc_test_verse_alignments_inter = {}\n",
    "#grc_test_verse_alignments_gdfa = {}\n",
    "#gc.collect()\n",
    "##args = []\n",
    "##for i,verse in enumerate(grc_verses):\n",
    "##    args.append((verse, current_editions))\n",
    "\n",
    "##with Pool(20) as p:\n",
    "##    all_res = p.map(get_pruned_verse_alignments, args)\n",
    "\n",
    "##for i,verse in enumerate(grc_verses):\n",
    "##    verse_aligns_inter, verse_aligns_gdfa = all_res[i]\n",
    "    \n",
    "##    grc_test_verse_alignments_inter[verse] = verse_aligns_inter\n",
    "##    grc_test_verse_alignments_gdfa[verse] = verse_aligns_gdfa\n",
    "\n",
    "#utils.LOG.info(\"done reading alignments\")\n",
    "##torch.save(grc_test_verse_alignments_inter, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_grc_inter.pickle\")\n",
    "##torch.save(grc_test_verse_alignments_gdfa, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_grc_gdfa.pickle\")\n",
    "#utils.LOG.info('done saving pruned alignments')\n",
    "\n",
    "#print('reading inter verse alignments')\n",
    "#grc_test_verse_alignments_inter = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_grc_inter.pickle\")\n",
    "#grc_test_verse_alignments_gdfa = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_grc_gdfa.pickle\")\n",
    "#gc.collect()\n",
    "#print('done reading inter verse alignments')\n",
    "\n",
    "#verses_map = {}\n",
    "\n",
    "#for edit in grc_test_dataset.nodes_map:\n",
    "#    for verse in grc_test_dataset.nodes_map[edit]:\n",
    "#        if verse not in verses_map:\n",
    "#            for tok in grc_test_dataset.nodes_map[edit][verse]:\n",
    "#                verses_map[verse] = grc_test_dataset.nodes_map[edit][verse][tok]\n",
    "#                break\n",
    "\n",
    "#sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "#grc_test_verses = [item[0] for item in sorted_verses]\n",
    "\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heb_test_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_helfi_heb_test_community_word.pickle\", map_location=torch.device('cpu'))\n",
    "#editf_fin = \"fin-x-bible-helfi\"\n",
    "#editf_heb = 'heb-x-bible-helfi'\n",
    "\n",
    "#test_gold_heb = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-fin-heb-gold-alignments_test.txt\"\n",
    "\n",
    "#pros_heb, surs_heb = autils.load_gold(test_gold_heb)\n",
    "#heb_verses = list(pros_heb.keys())\n",
    "\n",
    "\n",
    "#heb_test_verse_alignments_inter = {}\n",
    "#heb_test_verse_alignments_gdfa = {}\n",
    "##args = []\n",
    "##for i,verse in enumerate(heb_verses):\n",
    "##    args.append((verse, current_editions))\n",
    "\n",
    "##with Pool(20) as p:\n",
    "##    all_res = p.map(get_pruned_verse_alignments, args)\n",
    "\n",
    "##for i,verse in enumerate(heb_verses):\n",
    "##    verse_aligns_inter, verse_aligns_gdfa = all_res[i]\n",
    "    \n",
    "##    heb_test_verse_alignments_inter[verse] = verse_aligns_inter\n",
    "##    heb_test_verse_alignments_gdfa[verse] = verse_aligns_gdfa\n",
    "\n",
    "##utils.LOG.info(\"done reading alignments\")\n",
    "##torch.save(heb_test_verse_alignments_inter, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_heb_inter.pickle\")\n",
    "##torch.save(heb_test_verse_alignments_gdfa, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_heb_gdfa.pickle\")\n",
    "##utils.LOG.info('done saving pruned alignments')\n",
    "\n",
    "#print('reading inter verse alignments')\n",
    "#heb_test_verse_alignments_inter = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_heb_inter.pickle\")\n",
    "#heb_test_verse_alignments_gdfa = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_heb_gdfa.pickle\")\n",
    "#gc.collect()\n",
    "#print('done reading inter verse alignments')\n",
    "\n",
    "#verses_map = {}\n",
    "\n",
    "#for edit in heb_test_dataset.nodes_map:\n",
    "#    for verse in heb_test_dataset.nodes_map[edit]:\n",
    "#        if verse not in verses_map:\n",
    "#            for tok in heb_test_dataset.nodes_map[edit][verse]:\n",
    "#                verses_map[verse] = heb_test_dataset.nodes_map[edit][verse][tok]\n",
    "#                break\n",
    "\n",
    "#sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "#heb_test_verses = [item[0] for item in sorted_verses]\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "verses_map = {}\n",
    "\n",
    "for edit in train_dataset.nodes_map:\n",
    "    for verse in train_dataset.nodes_map[edit]:\n",
    "        if verse not in verses_map:\n",
    "            for tok in train_dataset.nodes_map[edit][verse]:\n",
    "                verses_map[verse] = train_dataset.nodes_map[edit][verse][tok]\n",
    "                break\n",
    "\n",
    "sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "all_verses = [item[0] for item in sorted_verses]\n",
    "\n",
    "long_verses = set()\n",
    "\n",
    "for edit in train_dataset.nodes_map.keys():\n",
    "    for verse in train_dataset.nodes_map[edit]:\n",
    "        to_print = False\n",
    "        for tok in train_dataset.nodes_map[edit][verse]:\n",
    "            if tok > 150:\n",
    "                to_print = True\n",
    "        if to_print == True:\n",
    "            long_verses.add(verse)\n",
    "\n",
    "\n",
    "train_verses = all_verses[:]\n",
    "\n",
    "masked_verses = list(long_verses)\n",
    "masked_verses.extend(blinker_verses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:19<00:00, 12.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "479366"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "data_dir_train = \"/mounts/data/proj/ayyoob/align_induction/dataset/dataset_helfi_train_community_word\"\n",
    "data_dir_blinker = \"/mounts/data/proj/ayyoob/align_induction/dataset/pruned_alignments_blinker_inter/\"\n",
    "\n",
    "class GNNDatasetPOSTAG(Dataset):\n",
    "\n",
    "    def __init__(self, dataset, verses, edit_files, alignments, node_cover, pos_labels, data_dir, create_data=False, group_size = 20):\n",
    "        self.verses = list(verses)\n",
    "        self.edit_files = list(edit_files)\n",
    "        self.pos_labels = pos_labels\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        self.items = self.calculate_size( self.verses, group_size, node_cover)\n",
    "        self.alignments = alignments\n",
    "        if create_data:\n",
    "            self.calculate_verse_stats(verses, edit_files, alignments, dataset, data_dir)\n",
    "    \n",
    "    def calculate_size(self, verses, group_size, covered_verses):\n",
    "        res = []\n",
    "        for verse in verses:\n",
    "            covered_nodes = covered_verses[verse]\n",
    "            random.shuffle(covered_nodes)\n",
    "            items = [covered_nodes[i:i + group_size] for i in range(0, len(covered_nodes), group_size)]\n",
    "            res.extend([(verse, i) for i in items])\n",
    "\n",
    "        return res\n",
    "\n",
    "    def calculate_verse_stats(self,verses, edition_files, alignments, dataset, data_dir):\n",
    "\n",
    "        min_edge = 0\n",
    "        for verse in tqdm(verses):\n",
    "            min_nodes = 99999999999999\n",
    "            max_nodes = 0\n",
    "            edges_tmp = [[],[]]\n",
    "            x_tmp = []\n",
    "            features = []\n",
    "            for i,editf1 in enumerate(edition_files):\n",
    "                for j,editf2 in enumerate(edition_files[i+1:]):\n",
    "                    aligns = autils.get_aligns(editf1, editf2, alignments[verse])\n",
    "                    if aligns != None:\n",
    "                        for align in aligns:\n",
    "                            try:\n",
    "                                n1,_ = gutils.node_nom(verse, editf1, align[0], None, dataset.nodes_map, x_tmp, edition_files, features)\n",
    "                                n2,_ = gutils.node_nom(verse, editf2, align[1], None, dataset.nodes_map, x_tmp, edition_files, features)\n",
    "                                edges_tmp[0].extend([n1, n2])\n",
    "\n",
    "                                max_nodes = max(n1, n2, max_nodes)\n",
    "                                min_nodes = min(n1, n2, min_nodes)\n",
    "                            except Exception as e:\n",
    "                                print(editf1, editf2, verse)\n",
    "                                raise(e)\n",
    "\n",
    "            self.verse_info = {}\n",
    "\n",
    "            self.verse_info['padding'] = min_nodes\n",
    "            \n",
    "            self.verse_info['x'] = torch.clone(dataset.x[min_nodes:max_nodes+1,:])\n",
    "            \n",
    "            self.verse_info['edge_index'] = torch.clone(dataset.edge_index[:, min_edge : min_edge + len(edges_tmp[0])] - min_nodes)\n",
    "\n",
    "            if torch.min(self.verse_info['edge_index']) != 0:\n",
    "                print(verse, min_nodes, max_nodes, min_edge, len(edges_tmp[0]))\n",
    "                print(torch.min(self.verse_info['edge_index']))\n",
    "            \n",
    "            if self.verse_info['x'].shape[0] != torch.max(self.verse_info['edge_index']) + 1 :\n",
    "                print(verse, min_nodes, max_nodes, min_edge, len(edges_tmp[0]))\n",
    "                print(torch.min(self.verse_info['edge_index']))\n",
    "            \n",
    "            min_edge = min_edge + len(edges_tmp[0])\n",
    "\n",
    "            torch.save(self.verse_info, f\"{data_dir}/verses/{verse}_info.torch.bin\")\n",
    "        \n",
    "        dataset.x = None\n",
    "        dataset.edge_index = None\n",
    "        torch.save(dataset, f\"{data_dir}/train_dataset_nox_noedge.torch.bin\")\n",
    "\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        verse, nodes = self.items[idx]\n",
    "\n",
    "        self.verse_info = {verse: torch.load(f'{self.data_dir}/verses/{verse}_info.torch.bin')}\n",
    "         \n",
    "        return {'nodes':torch.LongTensor(nodes) , 'verse':verse, 'self': self}\n",
    "\n",
    "def collate_fun(input):\n",
    "    res = {}\n",
    "    for item in input:\n",
    "        verse = item['verse'] \n",
    "        if verse not in res:\n",
    "            res[verse] = {'x':item['self'].verse_info[verse]['x'], 'edge_index':item['self'].verse_info[verse]['edge_index']\n",
    "                 ,'nodes':torch.LongTensor([]), 'padding':item['self'].verse_info[verse]['padding']}\n",
    "\n",
    "        res[verse]['nodes'] = torch.cat((item['nodes'], res[verse]['nodes']), dim=0)\n",
    "    \n",
    "    for verse in res:\n",
    "        res[verse]['labels'] = input[0]['self'].pos_labels[ res[verse]['nodes'] ,:]\n",
    "        res[verse]['nodes'] -=  res[verse]['padding']\n",
    "\n",
    "     \n",
    "    return res\n",
    "\n",
    "#train_pos_labels, train_pos_node_cover = get_pos_tags(train_dataset)\n",
    "#gnn_dataset_train_pos = GNNDatasetPOSTAG(train_dataset, train_verses, current_editions, verse_alignments_inter,\n",
    "#                        train_pos_node_cover, train_pos_labels, data_dir_train)\n",
    "torch.save({'pos_labels':train_pos_labels, 'pos_node_cover':train_pos_node_cover}, f'{data_dir_train}/pos_data.torch.bin')\n",
    "\n",
    "blinker_pos_labels, blinker_pos_node_cover = get_pos_tags(blinker_test_dataset)\n",
    "gnn_dataset_blinker_pos = GNNDatasetPOSTAG(blinker_test_dataset, blinker_verses, current_editions, blinker_verse_alignments_inter,\n",
    "                             blinker_pos_node_cover, blinker_pos_labels, data_dir_blinker)\n",
    "torch.save({'pos_labels':blinker_pos_labels, 'pos_node_cover': blinker_pos_node_cover}, f'{data_dir_blinker}/pos_data.torch.bin')\n",
    "\n",
    "len(gnn_dataset_train_pos)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "gnn_dataset = gnn_dataset_blinker_pos\n",
    "data_loader = DataLoader(gnn_dataset_blinker_pos, batch_size=1, collate_fn=collate_fun, shuffle=True)\n",
    "\n",
    "clean_memory()\n",
    "n_head = 1\n",
    "\n",
    "def turn_off_embedding_updates(encoder):\n",
    "    for i,ft in enumerate(encoder.feature_types):\n",
    "        if ft.type == MAPPING:\n",
    "            encoder.layers[i] = MappingEncoding(encoder.layers[i].emb.weight, freeze=True)\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "channels = 512\n",
    "\n",
    "in_dim = sum(t.out_dim for t in features)\n",
    "\n",
    "classifier = Classifier(n_head * channels, len(postag_map))\n",
    "model.decoder = classifier\n",
    "\n",
    "print(\"sending input to gpu\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S-\") + f\"samett-{channels}chs-feat{train_dataset.num_node_features}-\")\n",
    "\n",
    "torch.set_printoptions(edgeitems=5)\n",
    "print(\"model params - decoder params - conv1\", sum(p.numel() for p in model.parameters()), sum(p.numel() for p in decoder.parameters()))\n",
    "\n",
    "for epoch in range(1, 2):\n",
    "    print(f\"\\n----------------epoch {epoch} ---------------\")\n",
    "    \n",
    "    #if epoch % 1 == 0:\n",
    "    #    train_neg_edge_index = gutils.get_negative_edges(train_verses, small_editions, train_dataset.nodes_map,  verse_alignments_inter).to(dev)\n",
    "        #edge_index_seq_sent_neg = get_negative_edges_seq(train_dataset.nodes_map).to(dev)\n",
    "\n",
    "    train(epoch)\n",
    "    save_model(model)\n",
    "    clean_memory()\n",
    "    if epoch % 1 == 0:\n",
    "        #alignment_test(epoch, test_dataset.edge_index, editf1, editf2, test_verses[:30], test_nodes_map,\n",
    "        #    dev, model, x_test, pros, surs, verse_alignments_inter, verse_alignments_gdfa, writer, gnn_dataset.verse_info)\n",
    "        #eval_utils.alignment_test(epoch, test_dataset.edge_index, editf1, editf2, test_verses[:], test_nodes_map,\n",
    "        #    dev, model, x_test, pros, surs, verse_alignments_inter, verse_alignments_gdfa, writer, gnn_dataset.verse_info)\n",
    "\n",
    "        eval_utils.alignment_test(epoch, grc_test_dataset.edge_index, editf_fin, editf_grc, grc_test_verses[:], grc_test_dataset.nodes_map,\n",
    "                        dev, model, grc_test_dataset.x, pros_grc, surs_grc, grc_test_verse_alignments_inter, grc_test_verse_alignments_gdfa, writer, gnn_dataset_grc.verse_info)\n",
    "\n",
    "        eval_utils.alignment_test(epoch, heb_test_dataset.edge_index, editf_fin, editf_heb, heb_test_verses[:], heb_test_dataset.nodes_map,\n",
    "                                dev, model, heb_test_dataset.x, pros_heb, surs_heb, heb_test_verse_alignments_inter, heb_test_verse_alignments_gdfa, writer, gnn_dataset_heb.verse_info)\n",
    "\n",
    "        eval_utils.alignment_test(epoch, blinker_test_dataset.edge_index, editf12, editf22, blinker_verses, blinker_test_dataset.nodes_map,\n",
    "                    dev, model, blinker_test_dataset.x, pros_blinker, surs_blinker, blinker_verse_alignments_inter, blinker_verse_alignments_gdfa, writer, gnn_dataset_blinker.verse_info)\n",
    "        # auc, ap = test(edge_index_seq_sent, edge_index_seq_sent_neg, epoch)\n",
    "        # print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))\n",
    "    \n",
    "    clean_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n"
     ]
    }
   ],
   "source": [
    "from gnn_utils import eval_utils\n",
    "train_dataset.train_mask = train_dataset.val_mask = train_dataset.test_mask = train_dataset.y = None\n",
    "test_dataset.train_mask = test_dataset.val_mask = test_dataset.test_mask = test_dataset.y = None\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "features = train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    model.encoder.feature_encoder.feature_types[0] = afeatures.OneHotFeature(20, 83, 'editf')\n",
    "    model.encoder.feature_encoder.feature_types[1] = afeatures.OneHotFeature(32, 150, 'position')\n",
    "    model.encoder.feature_encoder.feature_types[2] = afeatures.FloatFeature(4, 'degree_centrality')\n",
    "    model.encoder.feature_encoder.feature_types[3] = afeatures.FloatFeature(4, 'closeness_centrality')\n",
    "    model.encoder.feature_encoder.feature_types[4] = afeatures.FloatFeature(4, 'betweenness_centrality')\n",
    "    model.encoder.feature_encoder.feature_types[5] = afeatures.FloatFeature(4, 'load_centrality')\n",
    "    model.encoder.feature_encoder.feature_types[6] = afeatures.FloatFeature(4, 'harmonic_centrality')\n",
    "    model.encoder.feature_encoder.feature_types[7] = afeatures.OneHotFeature(32, 250, 'greedy_modularity_community')\n",
    "    model.encoder.feature_encoder.feature_types[8] = afeatures.OneHotFeature(32, 250, 'community_2')\n",
    "    model.encoder.feature_encoder.feature_types[9] = afeatures.MappingFeature(100, 'word')\n",
    "    torch.save(model, '/mounts/work/ayyoob/models/gnn/checkpoint/gnn_512_flggll_word_halfTrain_nofeatlinear_encoderlineear_decoderonelayer' + datetime.now().strftime(\"%Y%m%d-%H%M%S-\") + '.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge features size:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/109374 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending input to gpu\n",
      "model params - decoder params - conv1 237583457 1050625\n",
      "\n",
      "----------------epoch 1 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:34<00:00, 22.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.632, rec: 0.78, F1: 0.698, AER: 0.302\n",
      "argmax prec: 0.845, rec: 0.698, F1: 0.764, AER: 0.235\n",
      "resnorm prec: 0.549, rec: 0.785, F1: 0.646, AER: 0.354\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.815, rec: 0.72, F1: 0.765, AER: 0.235\n",
      "my_gd_gdfa prec: 0.719, rec: 0.785, F1: 0.751, AER: 0.25\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [01:27<00:00, 25.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.512, rec: 0.741, F1: 0.606, AER: 0.394\n",
      "argmax prec: 0.774, rec: 0.62, F1: 0.688, AER: 0.311\n",
      "resnorm prec: 0.487, rec: 0.751, F1: 0.591, AER: 0.409\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.756, rec: 0.655, F1: 0.702, AER: 0.298\n",
      "my_gd_gdfa prec: 0.602, rec: 0.697, F1: 0.646, AER: 0.354\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:14<00:00, 17.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.772, rec: 0.743, F1: 0.757, AER: 0.242\n",
      "argmax prec: 0.932, rec: 0.668, F1: 0.778, AER: 0.219\n",
      "resnorm prec: 0.556, rec: 0.782, F1: 0.65, AER: 0.358\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.898, rec: 0.704, F1: 0.789, AER: 0.208\n",
      "my_gd_gdfa prec: 0.847, rec: 0.79, F1: 0.818, AER: 0.181\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:34<00:00, 22.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.642, rec: 0.793, F1: 0.71, AER: 0.291\n",
      "argmax prec: 0.849, rec: 0.697, F1: 0.766, AER: 0.235\n",
      "resnorm prec: 0.59, rec: 0.787, F1: 0.674, AER: 0.325\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.828, rec: 0.722, F1: 0.771, AER: 0.228\n",
      "my_gd_gdfa prec: 0.726, rec: 0.783, F1: 0.753, AER: 0.246\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [01:28<00:00, 25.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.519, rec: 0.753, F1: 0.614, AER: 0.386\n",
      "argmax prec: 0.777, rec: 0.619, F1: 0.689, AER: 0.311\n",
      "resnorm prec: 0.508, rec: 0.751, F1: 0.606, AER: 0.394\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.764, rec: 0.655, F1: 0.705, AER: 0.295\n",
      "my_gd_gdfa prec: 0.607, rec: 0.697, F1: 0.649, AER: 0.351\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:14<00:00, 17.46it/s]\n",
      " 18%|█▊        | 20001/109374 [43:17<583:27:03, 23.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.785, rec: 0.752, F1: 0.768, AER: 0.231\n",
      "argmax prec: 0.939, rec: 0.674, F1: 0.785, AER: 0.212\n",
      "resnorm prec: 0.623, rec: 0.785, F1: 0.695, AER: 0.311\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.91, rec: 0.709, F1: 0.797, AER: 0.2\n",
      "my_gd_gdfa prec: 0.855, rec: 0.791, F1: 0.822, AER: 0.177\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:33<00:00, 23.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.648, rec: 0.8, F1: 0.716, AER: 0.284\n",
      "argmax prec: 0.849, rec: 0.698, F1: 0.766, AER: 0.234\n",
      "resnorm prec: 0.571, rec: 0.799, F1: 0.666, AER: 0.334\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.824, rec: 0.73, F1: 0.774, AER: 0.226\n",
      "my_gd_gdfa prec: 0.725, rec: 0.788, F1: 0.755, AER: 0.245\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [01:28<00:00, 25.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.525, rec: 0.762, F1: 0.622, AER: 0.378\n",
      "argmax prec: 0.781, rec: 0.621, F1: 0.692, AER: 0.308\n",
      "resnorm prec: 0.489, rec: 0.77, F1: 0.598, AER: 0.402\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.762, rec: 0.667, F1: 0.711, AER: 0.289\n",
      "my_gd_gdfa prec: 0.608, rec: 0.705, F1: 0.653, AER: 0.347\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:14<00:00, 17.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.789, rec: 0.755, F1: 0.772, AER: 0.227\n",
      "argmax prec: 0.941, rec: 0.676, F1: 0.787, AER: 0.211\n",
      "resnorm prec: 0.618, rec: 0.793, F1: 0.695, AER: 0.312\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.913, rec: 0.714, F1: 0.801, AER: 0.195\n",
      "my_gd_gdfa prec: 0.858, rec: 0.79, F1: 0.823, AER: 0.176\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:34<00:00, 22.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.651, rec: 0.802, F1: 0.719, AER: 0.281\n",
      "argmax prec: 0.85, rec: 0.696, F1: 0.765, AER: 0.235\n",
      "resnorm prec: 0.559, rec: 0.803, F1: 0.659, AER: 0.341\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.822, rec: 0.734, F1: 0.776, AER: 0.225\n",
      "my_gd_gdfa prec: 0.724, rec: 0.79, F1: 0.756, AER: 0.245\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [01:29<00:00, 24.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.525, rec: 0.76, F1: 0.621, AER: 0.379\n",
      "argmax prec: 0.782, rec: 0.622, F1: 0.693, AER: 0.307\n",
      "resnorm prec: 0.481, rec: 0.774, F1: 0.593, AER: 0.407\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.754, rec: 0.674, F1: 0.712, AER: 0.288\n",
      "my_gd_gdfa prec: 0.605, rec: 0.71, F1: 0.653, AER: 0.347\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 18.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.793, rec: 0.758, F1: 0.775, AER: 0.224\n",
      "argmax prec: 0.94, rec: 0.677, F1: 0.787, AER: 0.21\n",
      "resnorm prec: 0.615, rec: 0.796, F1: 0.694, AER: 0.313\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.912, rec: 0.719, F1: 0.804, AER: 0.193\n",
      "my_gd_gdfa prec: 0.857, rec: 0.794, F1: 0.824, AER: 0.174\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:33<00:00, 23.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.645, rec: 0.796, F1: 0.713, AER: 0.287\n",
      "argmax prec: 0.851, rec: 0.696, F1: 0.766, AER: 0.234\n",
      "resnorm prec: 0.615, rec: 0.786, F1: 0.69, AER: 0.31\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.831, rec: 0.725, F1: 0.774, AER: 0.226\n",
      "my_gd_gdfa prec: 0.729, rec: 0.784, F1: 0.756, AER: 0.245\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [01:28<00:00, 25.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.522, rec: 0.756, F1: 0.618, AER: 0.382\n",
      "argmax prec: 0.777, rec: 0.619, F1: 0.689, AER: 0.311\n",
      "resnorm prec: 0.523, rec: 0.752, F1: 0.617, AER: 0.383\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.762, rec: 0.662, F1: 0.708, AER: 0.292\n",
      "my_gd_gdfa prec: 0.608, rec: 0.7, F1: 0.651, AER: 0.349\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 18.40it/s]\n",
      " 46%|████▌     | 50000/109374 [1:47:55<664:10:25, 40.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.785, rec: 0.754, F1: 0.769, AER: 0.23\n",
      "argmax prec: 0.936, rec: 0.674, F1: 0.784, AER: 0.213\n",
      "resnorm prec: 0.649, rec: 0.777, F1: 0.707, AER: 0.297\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.914, rec: 0.711, F1: 0.8, AER: 0.197\n",
      "my_gd_gdfa prec: 0.858, rec: 0.791, F1: 0.823, AER: 0.176\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:33<00:00, 23.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.647, rec: 0.8, F1: 0.715, AER: 0.285\n",
      "argmax prec: 0.853, rec: 0.697, F1: 0.767, AER: 0.233\n",
      "resnorm prec: 0.624, rec: 0.784, F1: 0.695, AER: 0.305\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.835, rec: 0.729, F1: 0.778, AER: 0.221\n",
      "my_gd_gdfa prec: 0.733, rec: 0.788, F1: 0.76, AER: 0.241\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [01:27<00:00, 25.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.525, rec: 0.762, F1: 0.622, AER: 0.379\n",
      "argmax prec: 0.785, rec: 0.62, F1: 0.693, AER: 0.307\n",
      "resnorm prec: 0.534, rec: 0.753, F1: 0.625, AER: 0.375\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.769, rec: 0.667, F1: 0.714, AER: 0.286\n",
      "my_gd_gdfa prec: 0.613, rec: 0.703, F1: 0.655, AER: 0.345\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 17.88it/s]\n",
      " 55%|█████▍    | 60001/109374 [2:09:40<376:11:01, 27.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.787, rec: 0.757, F1: 0.772, AER: 0.228\n",
      "argmax prec: 0.938, rec: 0.674, F1: 0.784, AER: 0.213\n",
      "resnorm prec: 0.675, rec: 0.773, F1: 0.721, AER: 0.282\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.919, rec: 0.707, F1: 0.799, AER: 0.198\n",
      "my_gd_gdfa prec: 0.861, rec: 0.788, F1: 0.823, AER: 0.175\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:33<00:00, 23.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.639, rec: 0.788, F1: 0.706, AER: 0.294\n",
      "argmax prec: 0.847, rec: 0.694, F1: 0.763, AER: 0.237\n",
      "resnorm prec: 0.6, rec: 0.784, F1: 0.68, AER: 0.32\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.822, rec: 0.724, F1: 0.77, AER: 0.23\n",
      "my_gd_gdfa prec: 0.725, rec: 0.783, F1: 0.753, AER: 0.247\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [01:28<00:00, 25.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.517, rec: 0.75, F1: 0.612, AER: 0.388\n",
      "argmax prec: 0.778, rec: 0.615, F1: 0.687, AER: 0.313\n",
      "resnorm prec: 0.528, rec: 0.747, F1: 0.619, AER: 0.381\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.762, rec: 0.66, F1: 0.707, AER: 0.293\n",
      "my_gd_gdfa prec: 0.609, rec: 0.698, F1: 0.65, AER: 0.35\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 18.00it/s]\n",
      " 64%|██████▍   | 70001/109374 [2:31:31<299:36:29, 27.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.782, rec: 0.751, F1: 0.766, AER: 0.233\n",
      "argmax prec: 0.938, rec: 0.673, F1: 0.784, AER: 0.213\n",
      "resnorm prec: 0.651, rec: 0.772, F1: 0.706, AER: 0.297\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.915, rec: 0.709, F1: 0.799, AER: 0.198\n",
      "my_gd_gdfa prec: 0.859, rec: 0.79, F1: 0.823, AER: 0.175\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:34<00:00, 22.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.639, rec: 0.789, F1: 0.706, AER: 0.294\n",
      "argmax prec: 0.854, rec: 0.694, F1: 0.766, AER: 0.234\n",
      "resnorm prec: 0.553, rec: 0.796, F1: 0.653, AER: 0.347\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.819, rec: 0.737, F1: 0.776, AER: 0.224\n",
      "my_gd_gdfa prec: 0.723, rec: 0.79, F1: 0.755, AER: 0.245\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [01:28<00:00, 25.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.516, rec: 0.751, F1: 0.612, AER: 0.388\n",
      "argmax prec: 0.782, rec: 0.616, F1: 0.689, AER: 0.31\n",
      "resnorm prec: 0.479, rec: 0.764, F1: 0.589, AER: 0.411\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.753, rec: 0.67, F1: 0.709, AER: 0.291\n",
      "my_gd_gdfa prec: 0.605, rec: 0.704, F1: 0.651, AER: 0.349\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 17.99it/s]\n",
      " 73%|███████▎  | 80001/109374 [2:53:34<251:58:09, 30.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.785, rec: 0.754, F1: 0.769, AER: 0.23\n",
      "argmax prec: 0.938, rec: 0.675, F1: 0.785, AER: 0.212\n",
      "resnorm prec: 0.617, rec: 0.785, F1: 0.691, AER: 0.315\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.913, rec: 0.715, F1: 0.802, AER: 0.195\n",
      "my_gd_gdfa prec: 0.857, rec: 0.791, F1: 0.823, AER: 0.176\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:34<00:00, 22.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.638, rec: 0.787, F1: 0.705, AER: 0.295\n",
      "argmax prec: 0.852, rec: 0.694, F1: 0.765, AER: 0.235\n",
      "resnorm prec: 0.592, rec: 0.788, F1: 0.676, AER: 0.324\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.823, rec: 0.733, F1: 0.775, AER: 0.225\n",
      "my_gd_gdfa prec: 0.726, rec: 0.787, F1: 0.755, AER: 0.245\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [01:29<00:00, 24.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.518, rec: 0.751, F1: 0.613, AER: 0.387\n",
      "argmax prec: 0.779, rec: 0.615, F1: 0.687, AER: 0.312\n",
      "resnorm prec: 0.519, rec: 0.754, F1: 0.615, AER: 0.386\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.758, rec: 0.665, F1: 0.708, AER: 0.291\n",
      "my_gd_gdfa prec: 0.607, rec: 0.702, F1: 0.651, AER: 0.349\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 18.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.783, rec: 0.753, F1: 0.768, AER: 0.231\n",
      "argmax prec: 0.941, rec: 0.674, F1: 0.785, AER: 0.212\n",
      "resnorm prec: 0.656, rec: 0.777, F1: 0.711, AER: 0.293\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.915, rec: 0.714, F1: 0.802, AER: 0.195\n",
      "my_gd_gdfa prec: 0.858, rec: 0.79, F1: 0.823, AER: 0.176\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:34<00:00, 22.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.641, rec: 0.791, F1: 0.708, AER: 0.292\n",
      "argmax prec: 0.853, rec: 0.697, F1: 0.767, AER: 0.233\n",
      "resnorm prec: 0.585, rec: 0.791, F1: 0.673, AER: 0.328\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.822, rec: 0.735, F1: 0.776, AER: 0.224\n",
      "my_gd_gdfa prec: 0.726, rec: 0.789, F1: 0.756, AER: 0.244\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [01:28<00:00, 25.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.516, rec: 0.75, F1: 0.611, AER: 0.389\n",
      "argmax prec: 0.783, rec: 0.616, F1: 0.69, AER: 0.31\n",
      "resnorm prec: 0.533, rec: 0.749, F1: 0.623, AER: 0.377\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.763, rec: 0.664, F1: 0.71, AER: 0.29\n",
      "my_gd_gdfa prec: 0.611, rec: 0.7, F1: 0.652, AER: 0.348\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 18.46it/s]\n",
      " 91%|█████████▏| 100001/109374 [3:36:40<61:19:50, 23.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.782, rec: 0.754, F1: 0.768, AER: 0.231\n",
      "argmax prec: 0.942, rec: 0.675, F1: 0.786, AER: 0.211\n",
      "resnorm prec: 0.679, rec: 0.771, F1: 0.722, AER: 0.281\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.921, rec: 0.714, F1: 0.804, AER: 0.193\n",
      "my_gd_gdfa prec: 0.863, rec: 0.79, F1: 0.825, AER: 0.174\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109374/109374 [3:54:23<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 104311179.82562804\n",
      "cluster loss: 13982.5107421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:34<00:00, 22.74it/s]\n",
      "  0%|          | 1/2225 [00:00<04:17,  8.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.634, rec: 0.783, F1: 0.701, AER: 0.299\n",
      "argmax prec: 0.847, rec: 0.691, F1: 0.761, AER: 0.239\n",
      "resnorm prec: 0.632, rec: 0.776, F1: 0.697, AER: 0.303\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.822, rec: 0.727, F1: 0.772, AER: 0.228\n",
      "my_gd_gdfa prec: 0.725, rec: 0.783, F1: 0.753, AER: 0.247\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [01:28<00:00, 25.14it/s]\n",
      "  1%|          | 3/250 [00:00<00:11, 21.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.512, rec: 0.744, F1: 0.607, AER: 0.394\n",
      "argmax prec: 0.776, rec: 0.611, F1: 0.684, AER: 0.316\n",
      "resnorm prec: 0.574, rec: 0.729, F1: 0.642, AER: 0.358\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.765, rec: 0.654, F1: 0.705, AER: 0.295\n",
      "my_gd_gdfa prec: 0.609, rec: 0.692, F1: 0.648, AER: 0.352\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 17.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.783, rec: 0.755, F1: 0.769, AER: 0.231\n",
      "argmax prec: 0.942, rec: 0.673, F1: 0.785, AER: 0.212\n",
      "resnorm prec: 0.712, rec: 0.76, F1: 0.735, AER: 0.266\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.927, rec: 0.708, F1: 0.803, AER: 0.194\n",
      "my_gd_gdfa prec: 0.867, rec: 0.785, F1: 0.824, AER: 0.174\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    }
   ],
   "source": [
    "#from tqdm import tqdm\n",
    "\n",
    "#torch.cuda.set_device(4)\n",
    "#features = train_dataset.features\n",
    "\n",
    "#data_loader = DataLoader(pos_gnn_dataset_train, batch_size=1, shuffle=True)\n",
    "#test_data_loader = DataLoader(pos_gnn_dataset_dev, batch_size=1, shuffle=True)\n",
    "\n",
    "#clean_memory()\n",
    "#drop_out = 0\n",
    "#n_head = 1\n",
    "\n",
    "#channels = 512\n",
    "\n",
    "#decoder_in_dim = n_head * channels \n",
    "#decoder = POSDecoder(decoder_in_dim, decoder_in_dim*2, len(postag_map))\n",
    "#model = torch.load('/mounts/work/ayyoob/models/gnn/checkpoint/gnn_512_flggll_word_halfTrain_nofeatlinear_encoderlineear_decoderonelayer20210910-235352-.pickle')\n",
    "#model.decoder = decoder\n",
    "#model.to(dev)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "#torch.set_printoptions(edgeitems=5)\n",
    "#print(\"model params - decoder params - conv1\", sum(p.numel() for p in model.parameters()), sum(p.numel() for p in decoder.parameters()))\n",
    "\n",
    "#for epoch in range(1, 10):\n",
    "#    print(f\"\\n----------------epoch {epoch} ---------------\")\n",
    "    \n",
    "#    #if epoch % 1 == 0:\n",
    "#    #    train_neg_edge_index = gutils.get_negative_edges(train_verses, small_editions, train_dataset.nodes_map,  verse_alignments_inter).to(dev)\n",
    "#        #edge_index_seq_sent_neg = get_negative_edges_seq(train_dataset.nodes_map).to(dev)\n",
    "\n",
    "#    train(epoch)\n",
    "#    save_model(model)\n",
    "#    test(epoch, test_data_loader) \n",
    "#    clean_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = sag\n",
    "batch = khar\n",
    "verse = gav\n",
    "print(i, verse)\n",
    "\n",
    "keys = list(gnn_dataset.verse_info.keys())\n",
    "\n",
    "gnn_dataset.verse_info[verse]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1180/1180 [00:32<00:00, 36.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0\n",
      "cluster loss: 0\n",
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.783, rec: 0.754, F1: 0.768, AER: 0.231\n",
      "argmax prec: 0.94, rec: 0.673, F1: 0.784, AER: 0.213\n",
      "resnorm prec: 0.707, rec: 0.759, F1: 0.732, AER: 0.269\n",
      "itermax2-.9 prec: 0.852, rec: 0.724, F1: 0.783, AER: 0.214\n",
      "itermax2-.95 prec: 0.85, rec: 0.724, F1: 0.782, AER: 0.215\n",
      "itermax2-.8 prec: 0.854, rec: 0.725, F1: 0.784, AER: 0.214\n",
      "my_gd prec: 0.924, rec: 0.704, F1: 0.799, AER: 0.198\n",
      "my_gd_gdfa prec: 0.865, rec: 0.784, F1: 0.823, AER: 0.175\n",
      "new1 prec: 0.945, rec: 0.661, F1: 0.778, AER: 0.219\n",
      "new_mygd prec: 0.92, rec: 0.706, F1: 0.799, AER: 0.198\n",
      "new_mygd_gdfa prec: 0.863, rec: 0.786, F1: 0.823, AER: 0.176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 2999/7000 [07:08<09:53,  6.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.647, rec: 0.797, F1: 0.714, AER: 0.286\n",
      "argmax prec: 0.857, rec: 0.697, F1: 0.769, AER: 0.231\n",
      "resnorm prec: 0.679, rec: 0.779, F1: 0.726, AER: 0.275\n",
      "itermax2-.9 prec: 0.736, rec: 0.766, F1: 0.751, AER: 0.25\n",
      "itermax2-.95 prec: 0.734, rec: 0.764, F1: 0.749, AER: 0.251\n",
      "itermax2-.8 prec: 0.738, rec: 0.766, F1: 0.752, AER: 0.248\n",
      "my_gd prec: 0.838, rec: 0.731, F1: 0.781, AER: 0.22\n",
      "my_gd_gdfa prec: 0.736, rec: 0.786, F1: 0.76, AER: 0.24\n",
      "new1 prec: 0.874, rec: 0.69, F1: 0.771, AER: 0.229\n",
      "new_mygd prec: 0.835, rec: 0.732, F1: 0.78, AER: 0.22\n",
      "new_mygd_gdfa prec: 0.734, rec: 0.786, F1: 0.759, AER: 0.241\n",
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.515, rec: 0.75, F1: 0.611, AER: 0.389\n",
      "argmax prec: 0.774, rec: 0.607, F1: 0.68, AER: 0.32\n",
      "resnorm prec: 0.548, rec: 0.727, F1: 0.625, AER: 0.375\n",
      "itermax2-.9 prec: 0.643, rec: 0.693, F1: 0.667, AER: 0.333\n",
      "itermax2-.95 prec: 0.64, rec: 0.692, F1: 0.665, AER: 0.335\n",
      "itermax2-.8 prec: 0.645, rec: 0.694, F1: 0.669, AER: 0.331\n",
      "my_gd prec: 0.75, rec: 0.65, F1: 0.696, AER: 0.304\n",
      "my_gd_gdfa prec: 0.602, rec: 0.689, F1: 0.643, AER: 0.357\n",
      "new1 prec: 0.799, rec: 0.606, F1: 0.689, AER: 0.311\n",
      "new_mygd prec: 0.745, rec: 0.653, F1: 0.696, AER: 0.304\n",
      "new_mygd_gdfa prec: 0.599, rec: 0.692, F1: 0.642, AER: 0.358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3001/7000 [12:08<69:33:39, 62.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.788, rec: 0.757, F1: 0.772, AER: 0.227\n",
      "argmax prec: 0.939, rec: 0.672, F1: 0.783, AER: 0.213\n",
      "resnorm prec: 0.711, rec: 0.767, F1: 0.738, AER: 0.264\n",
      "itermax2-.9 prec: 0.85, rec: 0.728, F1: 0.784, AER: 0.213\n",
      "itermax2-.95 prec: 0.849, rec: 0.728, F1: 0.784, AER: 0.214\n",
      "itermax2-.8 prec: 0.851, rec: 0.727, F1: 0.784, AER: 0.213\n",
      "my_gd prec: 0.919, rec: 0.708, F1: 0.8, AER: 0.197\n",
      "my_gd_gdfa prec: 0.863, rec: 0.785, F1: 0.822, AER: 0.176\n",
      "new1 prec: 0.945, rec: 0.663, F1: 0.779, AER: 0.218\n",
      "new_mygd prec: 0.914, rec: 0.709, F1: 0.799, AER: 0.198\n",
      "new_mygd_gdfa prec: 0.859, rec: 0.787, F1: 0.821, AER: 0.177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 5999/7000 [18:42<02:03,  8.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.648, rec: 0.797, F1: 0.715, AER: 0.285\n",
      "argmax prec: 0.856, rec: 0.698, F1: 0.769, AER: 0.231\n",
      "resnorm prec: 0.669, rec: 0.781, F1: 0.721, AER: 0.279\n",
      "itermax2-.9 prec: 0.742, rec: 0.765, F1: 0.753, AER: 0.247\n",
      "itermax2-.95 prec: 0.741, rec: 0.765, F1: 0.753, AER: 0.247\n",
      "itermax2-.8 prec: 0.743, rec: 0.765, F1: 0.754, AER: 0.246\n",
      "my_gd prec: 0.838, rec: 0.731, F1: 0.781, AER: 0.219\n",
      "my_gd_gdfa prec: 0.736, rec: 0.785, F1: 0.76, AER: 0.24\n",
      "new1 prec: 0.874, rec: 0.692, F1: 0.772, AER: 0.228\n",
      "new_mygd prec: 0.836, rec: 0.732, F1: 0.781, AER: 0.22\n",
      "new_mygd_gdfa prec: 0.735, rec: 0.786, F1: 0.76, AER: 0.24\n",
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.51, rec: 0.748, F1: 0.606, AER: 0.393\n",
      "argmax prec: 0.777, rec: 0.602, F1: 0.678, AER: 0.322\n",
      "resnorm prec: 0.545, rec: 0.726, F1: 0.623, AER: 0.377\n",
      "itermax2-.9 prec: 0.649, rec: 0.69, F1: 0.669, AER: 0.332\n",
      "itermax2-.95 prec: 0.646, rec: 0.689, F1: 0.667, AER: 0.333\n",
      "itermax2-.8 prec: 0.65, rec: 0.69, F1: 0.669, AER: 0.33\n",
      "my_gd prec: 0.759, rec: 0.646, F1: 0.698, AER: 0.302\n",
      "my_gd_gdfa prec: 0.606, rec: 0.685, F1: 0.643, AER: 0.357\n",
      "new1 prec: 0.802, rec: 0.602, F1: 0.688, AER: 0.312\n",
      "new_mygd prec: 0.752, rec: 0.65, F1: 0.697, AER: 0.303\n",
      "new_mygd_gdfa prec: 0.603, rec: 0.689, F1: 0.643, AER: 0.357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 6001/7000 [23:40<17:24:46, 62.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.785, rec: 0.757, F1: 0.771, AER: 0.229\n",
      "argmax prec: 0.941, rec: 0.671, F1: 0.783, AER: 0.213\n",
      "resnorm prec: 0.7, rec: 0.767, F1: 0.732, AER: 0.27\n",
      "itermax2-.9 prec: 0.849, rec: 0.726, F1: 0.783, AER: 0.215\n",
      "itermax2-.95 prec: 0.847, rec: 0.724, F1: 0.781, AER: 0.216\n",
      "itermax2-.8 prec: 0.85, rec: 0.726, F1: 0.783, AER: 0.214\n",
      "my_gd prec: 0.917, rec: 0.706, F1: 0.798, AER: 0.2\n",
      "my_gd_gdfa prec: 0.861, rec: 0.784, F1: 0.821, AER: 0.178\n",
      "new1 prec: 0.945, rec: 0.663, F1: 0.779, AER: 0.218\n",
      "new_mygd prec: 0.912, rec: 0.708, F1: 0.797, AER: 0.2\n",
      "new_mygd_gdfa prec: 0.857, rec: 0.787, F1: 0.821, AER: 0.178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7000/7000 [25:46<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 5611854.063873291\n",
      "cluster loss: 748.3435668945312\n",
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.64, rec: 0.79, F1: 0.707, AER: 0.293\n",
      "argmax prec: 0.855, rec: 0.696, F1: 0.767, AER: 0.233\n",
      "resnorm prec: 0.666, rec: 0.773, F1: 0.716, AER: 0.285\n",
      "itermax2-.9 prec: 0.736, rec: 0.757, F1: 0.746, AER: 0.254\n",
      "itermax2-.95 prec: 0.734, rec: 0.756, F1: 0.745, AER: 0.255\n",
      "itermax2-.8 prec: 0.737, rec: 0.758, F1: 0.747, AER: 0.253\n",
      "my_gd prec: 0.836, rec: 0.729, F1: 0.779, AER: 0.221\n",
      "my_gd_gdfa prec: 0.736, rec: 0.784, F1: 0.759, AER: 0.241\n",
      "new1 prec: 0.872, rec: 0.69, F1: 0.77, AER: 0.23\n",
      "new_mygd prec: 0.833, rec: 0.73, F1: 0.778, AER: 0.222\n",
      "new_mygd_gdfa prec: 0.733, rec: 0.785, F1: 0.758, AER: 0.242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 2998/6635 [05:44<05:29, 11.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.643, rec: 0.79, F1: 0.709, AER: 0.291\n",
      "argmax prec: 0.852, rec: 0.7, F1: 0.769, AER: 0.231\n",
      "resnorm prec: 0.66, rec: 0.778, F1: 0.714, AER: 0.286\n",
      "itermax2-.9 prec: 0.738, rec: 0.76, F1: 0.749, AER: 0.251\n",
      "itermax2-.95 prec: 0.737, rec: 0.76, F1: 0.748, AER: 0.252\n",
      "itermax2-.8 prec: 0.739, rec: 0.76, F1: 0.749, AER: 0.251\n",
      "my_gd prec: 0.834, rec: 0.728, F1: 0.777, AER: 0.222\n",
      "my_gd_gdfa prec: 0.734, rec: 0.784, F1: 0.758, AER: 0.242\n",
      "new1 prec: 0.871, rec: 0.692, F1: 0.771, AER: 0.229\n",
      "new_mygd prec: 0.832, rec: 0.729, F1: 0.777, AER: 0.223\n",
      "new_mygd_gdfa prec: 0.732, rec: 0.785, F1: 0.758, AER: 0.242\n",
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.516, rec: 0.751, F1: 0.612, AER: 0.388\n",
      "argmax prec: 0.78, rec: 0.612, F1: 0.686, AER: 0.314\n",
      "resnorm prec: 0.568, rec: 0.728, F1: 0.638, AER: 0.362\n",
      "itermax2-.9 prec: 0.652, rec: 0.698, F1: 0.674, AER: 0.326\n",
      "itermax2-.95 prec: 0.649, rec: 0.696, F1: 0.672, AER: 0.328\n",
      "itermax2-.8 prec: 0.653, rec: 0.698, F1: 0.675, AER: 0.325\n",
      "my_gd prec: 0.766, rec: 0.655, F1: 0.706, AER: 0.294\n",
      "my_gd_gdfa prec: 0.611, rec: 0.693, F1: 0.649, AER: 0.351\n",
      "new1 prec: 0.807, rec: 0.611, F1: 0.695, AER: 0.304\n",
      "new_mygd prec: 0.761, rec: 0.658, F1: 0.706, AER: 0.294\n",
      "new_mygd_gdfa prec: 0.608, rec: 0.695, F1: 0.649, AER: 0.352\n",
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.784, rec: 0.752, F1: 0.768, AER: 0.231\n",
      "argmax prec: 0.939, rec: 0.674, F1: 0.785, AER: 0.212\n",
      "resnorm prec: 0.679, rec: 0.77, F1: 0.722, AER: 0.281\n",
      "itermax2-.9 prec: 0.851, rec: 0.726, F1: 0.784, AER: 0.214\n",
      "itermax2-.95 prec: 0.85, rec: 0.726, F1: 0.783, AER: 0.214\n",
      "itermax2-.8 prec: 0.853, rec: 0.726, F1: 0.784, AER: 0.213\n",
      "my_gd prec: 0.912, rec: 0.709, F1: 0.798, AER: 0.199\n",
      "my_gd_gdfa prec: 0.857, rec: 0.788, F1: 0.821, AER: 0.177\n",
      "new1 prec: 0.942, rec: 0.668, F1: 0.782, AER: 0.215\n",
      "new_mygd prec: 0.907, rec: 0.712, F1: 0.798, AER: 0.199\n",
      "new_mygd_gdfa prec: 0.853, rec: 0.79, F1: 0.82, AER: 0.178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 5998/6635 [15:47<01:30,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.646, rec: 0.796, F1: 0.713, AER: 0.287\n",
      "argmax prec: 0.858, rec: 0.699, F1: 0.77, AER: 0.229\n",
      "resnorm prec: 0.609, rec: 0.791, F1: 0.688, AER: 0.312\n",
      "itermax2-.9 prec: 0.745, rec: 0.764, F1: 0.754, AER: 0.246\n",
      "itermax2-.95 prec: 0.743, rec: 0.763, F1: 0.753, AER: 0.247\n",
      "itermax2-.8 prec: 0.746, rec: 0.764, F1: 0.755, AER: 0.245\n",
      "my_gd prec: 0.831, rec: 0.738, F1: 0.782, AER: 0.218\n",
      "my_gd_gdfa prec: 0.733, rec: 0.789, F1: 0.76, AER: 0.24\n",
      "new1 prec: 0.874, rec: 0.696, F1: 0.775, AER: 0.225\n",
      "new_mygd prec: 0.829, rec: 0.741, F1: 0.783, AER: 0.218\n",
      "new_mygd_gdfa prec: 0.731, rec: 0.791, F1: 0.76, AER: 0.24\n",
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.512, rec: 0.749, F1: 0.608, AER: 0.392\n",
      "argmax prec: 0.783, rec: 0.608, F1: 0.684, AER: 0.316\n",
      "resnorm prec: 0.529, rec: 0.74, F1: 0.617, AER: 0.383\n",
      "itermax2-.9 prec: 0.649, rec: 0.695, F1: 0.671, AER: 0.329\n",
      "itermax2-.95 prec: 0.646, rec: 0.693, F1: 0.669, AER: 0.331\n",
      "itermax2-.8 prec: 0.651, rec: 0.696, F1: 0.673, AER: 0.327\n",
      "my_gd prec: 0.754, rec: 0.66, F1: 0.704, AER: 0.296\n",
      "my_gd_gdfa prec: 0.606, rec: 0.695, F1: 0.647, AER: 0.352\n",
      "new1 prec: 0.808, rec: 0.612, F1: 0.696, AER: 0.304\n",
      "new_mygd prec: 0.748, rec: 0.663, F1: 0.703, AER: 0.297\n",
      "new_mygd_gdfa prec: 0.603, rec: 0.698, F1: 0.647, AER: 0.353\n",
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.788, rec: 0.759, F1: 0.773, AER: 0.226\n",
      "argmax prec: 0.944, rec: 0.674, F1: 0.786, AER: 0.211\n",
      "resnorm prec: 0.652, rec: 0.781, F1: 0.711, AER: 0.293\n",
      "itermax2-.9 prec: 0.853, rec: 0.728, F1: 0.786, AER: 0.212\n",
      "itermax2-.95 prec: 0.851, rec: 0.729, F1: 0.785, AER: 0.212\n",
      "itermax2-.8 prec: 0.853, rec: 0.727, F1: 0.785, AER: 0.212\n",
      "my_gd prec: 0.915, rec: 0.718, F1: 0.805, AER: 0.192\n",
      "my_gd_gdfa prec: 0.859, rec: 0.792, F1: 0.824, AER: 0.174\n",
      "new1 prec: 0.944, rec: 0.672, F1: 0.785, AER: 0.212\n",
      "new_mygd prec: 0.908, rec: 0.721, F1: 0.804, AER: 0.193\n",
      "new_mygd_gdfa prec: 0.854, rec: 0.794, F1: 0.823, AER: 0.176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6635/6635 [21:55<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3504449.5449852943\n",
      "cluster loss: 509.94970703125\n",
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.516, rec: 0.752, F1: 0.612, AER: 0.388\n",
      "argmax prec: 0.784, rec: 0.612, F1: 0.687, AER: 0.313\n",
      "resnorm prec: 0.533, rec: 0.744, F1: 0.621, AER: 0.379\n",
      "itermax2-.9 prec: 0.655, rec: 0.7, F1: 0.677, AER: 0.324\n",
      "itermax2-.95 prec: 0.652, rec: 0.698, F1: 0.674, AER: 0.325\n",
      "itermax2-.8 prec: 0.657, rec: 0.7, F1: 0.678, AER: 0.322\n",
      "my_gd prec: 0.757, rec: 0.665, F1: 0.708, AER: 0.292\n",
      "my_gd_gdfa prec: 0.607, rec: 0.699, F1: 0.65, AER: 0.35\n",
      "new1 prec: 0.809, rec: 0.617, F1: 0.7, AER: 0.3\n",
      "new_mygd prec: 0.751, rec: 0.669, F1: 0.708, AER: 0.293\n",
      "new_mygd_gdfa prec: 0.604, rec: 0.703, F1: 0.65, AER: 0.351\n"
     ]
    }
   ],
   "source": [
    "data_loader_blinker = DataLoader(gnn_dataset_blinker, batch_size=1, collate_fn=collate_fun, shuffle=True)\n",
    "data_loader_heb = DataLoader(gnn_dataset_heb, batch_size=1, collate_fn=collate_fun, shuffle=True)\n",
    "data_loader_grc = DataLoader(gnn_dataset_grc, batch_size=1, collate_fn=collate_fun, shuffle=True)\n",
    "\n",
    "clean_memory()\n",
    "data_loader = data_loader_blinker\n",
    "gnn_dataset = gnn_dataset_blinker\n",
    "train(1)\n",
    "clean_memory()\n",
    "eval_utils.alignment_test(epoch, blinker_test_dataset.edge_index, editf12, editf22, blinker_verses[:], blinker_test_dataset.nodes_map,\n",
    "                    dev, model, blinker_test_dataset.x, pros_blinker, surs_blinker, blinker_verse_alignments_inter, blinker_verse_alignments_gdfa, writer, gnn_dataset_blinker.verse_info)\n",
    "clean_memory()\n",
    "\n",
    "data_loader = data_loader_grc\n",
    "gnn_dataset = gnn_dataset_grc\n",
    "train(1)\n",
    "clean_memory()\n",
    "eval_utils.alignment_test(epoch, grc_test_dataset.edge_index, editf_fin, editf_grc, grc_test_verses[:], grc_test_dataset.nodes_map,\n",
    "                        dev, model, grc_test_dataset.x, pros_grc, surs_grc, grc_test_verse_alignments_inter, grc_test_verse_alignments_gdfa, writer, gnn_dataset_grc.verse_info)\n",
    "clean_memory()\n",
    "\n",
    "data_loader = data_loader_heb\n",
    "gnn_dataset = gnn_dataset_heb\n",
    "train(1)\n",
    "clean_memory()\n",
    "eval_utils.alignment_test(epoch, heb_test_dataset.edge_index, editf_fin, editf_heb, heb_test_verses[:], heb_test_dataset.nodes_map,\n",
    "                        dev, model, heb_test_dataset.x, pros_heb, surs_heb, heb_test_verse_alignments_inter, heb_test_verse_alignments_gdfa, writer, gnn_dataset_heb.verse_info)\n",
    "clean_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.521, rec: 0.757, F1: 0.617, AER: 0.383\n",
      "argmax prec: 0.781, rec: 0.618, F1: 0.69, AER: 0.31\n",
      "resnorm prec: 0.513, rec: 0.758, F1: 0.612, AER: 0.388\n",
      "itermax2-.9 prec: 0.657, rec: 0.702, F1: 0.679, AER: 0.321\n",
      "itermax2-.95 prec: 0.655, rec: 0.701, F1: 0.677, AER: 0.323\n",
      "itermax2-.8 prec: 0.66, rec: 0.702, F1: 0.68, AER: 0.32\n",
      "my_gd prec: 0.759, rec: 0.663, F1: 0.708, AER: 0.292\n",
      "my_gd_gdfa prec: 0.607, rec: 0.701, F1: 0.651, AER: 0.349\n",
      "new1 prec: 0.808, rec: 0.616, F1: 0.699, AER: 0.3\n",
      "new_mygd prec: 0.753, rec: 0.667, F1: 0.707, AER: 0.293\n",
      "new_mygd_gdfa prec: 0.604, rec: 0.704, F1: 0.65, AER: 0.35\n",
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.643, rec: 0.792, F1: 0.71, AER: 0.29\n",
      "argmax prec: 0.85, rec: 0.7, F1: 0.768, AER: 0.233\n",
      "resnorm prec: 0.582, rec: 0.791, F1: 0.671, AER: 0.329\n",
      "itermax2-.9 prec: 0.736, rec: 0.756, F1: 0.746, AER: 0.254\n",
      "itermax2-.95 prec: 0.733, rec: 0.755, F1: 0.744, AER: 0.256\n",
      "itermax2-.8 prec: 0.738, rec: 0.756, F1: 0.747, AER: 0.253\n",
      "my_gd prec: 0.824, rec: 0.728, F1: 0.773, AER: 0.227\n",
      "my_gd_gdfa prec: 0.727, rec: 0.785, F1: 0.755, AER: 0.245\n",
      "new1 prec: 0.866, rec: 0.69, F1: 0.768, AER: 0.232\n",
      "new_mygd prec: 0.821, rec: 0.73, F1: 0.773, AER: 0.227\n",
      "new_mygd_gdfa prec: 0.724, rec: 0.786, F1: 0.754, AER: 0.246\n",
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.789, rec: 0.756, F1: 0.772, AER: 0.227\n",
      "argmax prec: 0.94, rec: 0.675, F1: 0.786, AER: 0.211\n",
      "resnorm prec: 0.656, rec: 0.784, F1: 0.714, AER: 0.29\n",
      "itermax2-.9 prec: 0.852, rec: 0.727, F1: 0.785, AER: 0.213\n",
      "itermax2-.95 prec: 0.851, rec: 0.728, F1: 0.785, AER: 0.213\n",
      "itermax2-.8 prec: 0.854, rec: 0.727, F1: 0.785, AER: 0.212\n",
      "my_gd prec: 0.918, rec: 0.713, F1: 0.803, AER: 0.194\n",
      "my_gd_gdfa prec: 0.863, rec: 0.791, F1: 0.825, AER: 0.173\n",
      "new1 prec: 0.942, rec: 0.668, F1: 0.782, AER: 0.215\n",
      "new_mygd prec: 0.913, rec: 0.714, F1: 0.801, AER: 0.195\n",
      "new_mygd_gdfa prec: 0.859, rec: 0.792, F1: 0.824, AER: 0.175\n"
     ]
    }
   ],
   "source": [
    "from gnn_utils import eval_utils\n",
    "importlib.reload(eval_utils)\n",
    "\n",
    "clean_memory()\n",
    "eval_utils.alignment_test(epoch, heb_test_dataset.edge_index, editf_fin, editf_heb, heb_test_verses, heb_test_dataset.nodes_map,\n",
    "                        dev, model, heb_test_dataset.x, pros_heb, surs_heb, heb_test_verse_alignments_inter, heb_test_verse_alignments_gdfa, writer, gnn_dataset_heb.verse_info)\n",
    "\n",
    "eval_utils.alignment_test(epoch, grc_test_dataset.edge_index, editf_fin, editf_grc, grc_test_verses, grc_test_dataset.nodes_map,\n",
    "                        dev, model, grc_test_dataset.x, pros_grc, surs_grc, grc_test_verse_alignments_inter, grc_test_verse_alignments_gdfa, writer, gnn_dataset_grc.verse_info)\n",
    "\n",
    "eval_utils.alignment_test(epoch, blinker_test_dataset.edge_index, editf12, editf22, blinker_verses, blinker_test_dataset.nodes_map,\n",
    "                    dev, model, blinker_test_dataset.x, pros_blinker, surs_blinker, blinker_verse_alignments_inter, blinker_verse_alignments_gdfa, writer, gnn_dataset_blinker.verse_info)\n",
    "\n",
    "clean_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/6202 [00:00<06:39, 15.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align heb \n",
      "going to align train \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6202/6202 [05:18<00:00, 19.46it/s]\n",
      "  3%|▎         | 2/73 [00:00<00:04, 17.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align blinker \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:04<00:00, 18.16it/s]\n",
      "  0%|          | 2/783 [00:00<00:42, 18.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align grc \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:39<00:00, 20.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going save alignments for hin-x-bible-newworld\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/2168 [00:00<01:16, 28.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align heb \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2168/2168 [01:47<00:00, 20.10it/s]\n",
      "  0%|          | 1/23531 [00:00<44:58,  8.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align train \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23531/23531 [19:06<00:00, 20.52it/s]\n",
      "  0%|          | 1/243 [00:00<00:28,  8.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align blinker \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 243/243 [00:12<00:00, 19.35it/s]\n",
      "  0%|          | 2/782 [00:00<00:40, 19.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align grc \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:37<00:00, 20.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going save alignments for ita-x-bible-2009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/6203 [00:00<05:32, 18.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align heb \n",
      "going to align train \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6203/6203 [05:06<00:00, 20.22it/s]\n",
      "  3%|▎         | 2/73 [00:00<00:04, 16.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align blinker \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:03<00:00, 18.78it/s]\n",
      "  0%|          | 2/783 [00:00<00:40, 19.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align grc \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:37<00:00, 20.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going save alignments for prs-x-bible-goodnews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/6203 [00:00<05:42, 18.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align heb \n",
      "going to align train \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6203/6203 [05:12<00:00, 19.85it/s]\n",
      "  3%|▎         | 2/73 [00:00<00:04, 16.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align blinker \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:04<00:00, 18.00it/s]\n",
      "  0%|          | 2/783 [00:00<00:40, 19.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align grc \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:38<00:00, 20.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going save alignments for ron-x-bible-2006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/2172 [00:00<01:12, 29.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align heb \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2172/2172 [01:57<00:00, 18.43it/s]\n",
      "  0%|          | 1/23560 [00:00<45:56,  8.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align train \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23560/23560 [20:43<00:00, 18.94it/s]\n",
      "  0%|          | 1/246 [00:00<00:32,  7.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align blinker \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 246/246 [00:13<00:00, 17.61it/s]\n",
      "  0%|          | 2/783 [00:00<00:45, 17.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align grc \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:40<00:00, 19.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going save alignments for spa-x-bible-newworld\n"
     ]
    }
   ],
   "source": [
    "# produce for uruba\n",
    "importlib.reload(eval_utils)\n",
    "editf_yor = 'yor-x-bible-2010'\n",
    "editf_others = ['eng-x-bible-mixed', 'deu-x-bible-newworld', 'ces-x-bible-newworld', 'fra-x-bible-louissegond', 'hin-x-bible-newworld',\n",
    "                'ita-x-bible-2009', 'prs-x-bible-goodnews', 'ron-x-bible-2006', 'spa-x-bible-newworld']\n",
    "\n",
    "#def get_pruned_verse_alignments(args):\n",
    "#    verse, current_editions = args\n",
    "    \n",
    "#    #verse_aligns_inter = autils.get_verse_alignments(verse)\n",
    "#    verse_aligns_gdfa = autils.get_verse_alignments(verse, gdfa=True)\n",
    "\n",
    "#    #autils.prune_non_necessary_alignments(verse_aligns_inter, current_editions)\n",
    "#    autils.prune_non_necessary_alignments(verse_aligns_gdfa, current_editions)\n",
    "\n",
    "#    gc.collect()\n",
    "#    return verse_aligns_gdfa\n",
    "    \n",
    "\n",
    "#verse_alignments_gdfa = {}\n",
    "#args = []\n",
    "#editfs = editf_others[:]\n",
    "#editfs.append(editf_yor)\n",
    "#for i,verse in enumerate(train_verses):\n",
    "#    args.append((verse, editfs))\n",
    "\n",
    "#print('going to get alignments')\n",
    "#with Pool(20) as p:\n",
    "#    all_res = p.map(get_pruned_verse_alignments, args)\n",
    "\n",
    "#for i,verse in enumerate(all_verses):\n",
    "#    verse_aligns_gdfa = all_res[i]\n",
    "    \n",
    "#    verse_alignments_gdfa[verse] = verse_aligns_gdfa\n",
    "\n",
    "for verse in train_dataset.nodes_map[editf_yor]:\n",
    "    if verse not in surs :\n",
    "        surs[verse] = set()\n",
    "        pros[verse] = set()\n",
    "\n",
    "#verse_alignments_gdfa = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_train_gdfa_yoruba.pickle\")\n",
    "\n",
    "for eidtf_t in editf_others:\n",
    "    res = {}\n",
    "\n",
    "    print('going to align heb ')\n",
    "    if eidtf_t in heb_test_dataset.nodes_map:\n",
    "        verses = set(heb_test_dataset.nodes_map[editf_yor].keys()).intersection(heb_test_dataset.nodes_map[eidtf_t].keys())\n",
    "        res_ = eval_utils.alignment_test(epoch, heb_test_dataset.edge_index, editf_yor, eidtf_t, list(verses), heb_test_dataset.nodes_map,\n",
    "                        dev, model, heb_test_dataset.x, pros_heb, surs_heb, heb_test_verse_alignments_inter, heb_test_verse_alignments_gdfa,\n",
    "                        writer, gnn_dataset_heb.verse_info, calc_numbers=False)\n",
    "        clean_memory()\n",
    "        res.update(res_)\n",
    "\n",
    "    print('going to align train ')\n",
    "    verses = set(train_dataset.nodes_map[editf_yor].keys()).intersection(train_dataset.nodes_map[eidtf_t].keys())\n",
    "    res_ = eval_utils.alignment_test(epoch, train_dataset.edge_index, editf_yor, eidtf_t, list(verses - set(masked_verses)), train_dataset.nodes_map,\n",
    "                    dev, model, train_dataset.x, pros, surs, verse_alignments_inter, verse_alignments_gdfa, writer, gnn_dataset_train.verse_info, calc_numbers=False)\n",
    "    clean_memory()\n",
    "    res.update(res_)\n",
    "\n",
    "    print('going to align blinker ')\n",
    "    verses = set(blinker_test_dataset.nodes_map[editf_yor].keys()).intersection(blinker_test_dataset.nodes_map[eidtf_t].keys())\n",
    "    res_ = eval_utils.alignment_test(epoch, blinker_test_dataset.edge_index, editf_yor, eidtf_t, list(verses), blinker_test_dataset.nodes_map,\n",
    "                    dev, model, blinker_test_dataset.x, pros_blinker, surs_blinker, blinker_verse_alignments_inter, blinker_verse_alignments_gdfa,\n",
    "                    writer, gnn_dataset_blinker.verse_info, calc_numbers=False)\n",
    "    clean_memory()\n",
    "    res.update(res_)\n",
    "\n",
    "    print('going to align grc ')\n",
    "    verses = set(grc_test_dataset.nodes_map[editf_yor].keys()).intersection(grc_test_dataset.nodes_map[eidtf_t].keys())\n",
    "    res_ = eval_utils.alignment_test(epoch, grc_test_dataset.edge_index, editf_yor, eidtf_t, list(verses), grc_test_dataset.nodes_map,\n",
    "                    dev, model, grc_test_dataset.x, pros_grc, surs_grc, grc_test_verse_alignments_inter, grc_test_verse_alignments_gdfa,\n",
    "                    writer, gnn_dataset_grc.verse_info, calc_numbers=False)\n",
    "    clean_memory()\n",
    "    res.update(res_)\n",
    "\n",
    "    \n",
    "    \n",
    "    print(f'going save alignments for {eidtf_t}')\n",
    "    torch.save(res, f'/mounts/work/ayyoob/results/gnn_align/yoruba/{eidtf_t}_alignments.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global model, decoder\n",
    "#1/0\n",
    "\n",
    "decoder = None\n",
    "model = None\n",
    "\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = blinker_test_dataset.features[:]\n",
    "#features_edge = train_dataset.features_edge[:]\n",
    "from pprint import pprint\n",
    "#print('indim',in_dim)\n",
    "#features[-1].out_dim = 50\n",
    "for i in features:\n",
    "    #if i.type==3:\n",
    "    #    i.out_dim=4\n",
    "    print(vars(i))\n",
    "\n",
    "sum(p.out_dim for p in features)\n",
    "#train_dataset.features.pop()\n",
    "#train_dataset.features[0] = afeatures.OneHotFeature(20, 83, 'editf')\n",
    "#train_dataset.features[1] = afeatures.OneHotFeature(32, 150, 'position')\n",
    "#train_dataset.features[2] = afeatures.FloatFeature(4, 'degree_centrality')\n",
    "#train_dataset.features[3] = afeatures.FloatFeature(4, 'closeness_centrality')\n",
    "#train_dataset.features[4] = afeatures.FloatFeature(4, 'betweenness_centrality')\n",
    "#train_dataset.features[5] = afeatures.FloatFeature(4, 'load_centrality')\n",
    "#train_dataset.features[6] = afeatures.FloatFeature(4, 'harmonic_centrality')\n",
    "#train_dataset.features[7] = afeatures.OneHotFeature(32, 250, 'greedy_modularity_community')\n",
    "##train_dataset.features.append(afeatures.MappingFeature(100, 'word'))\n",
    "#torch.save(train_dataset, \"/mounts/work/ayyoob/models/gnn/dataset_helfi_train_community_word.pickle\")\n",
    "#torch.save(train_dataset.features[-3], \"./features.tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of deleted edges by each community detection method\n",
    "# from networkx.algorithms.community import greedy_modularity_communities, asyn_lpa_communities, label_propagation_communities, asyn_fluidc\n",
    "\n",
    "# tmp_verses = [all_verses[2]]\n",
    "# tmp_editions = small_editions[:10]\n",
    "# tmp_dataset, tmp_nodes_map = create_dataset(tmp_verses, verse_alignments_inter, tmp_editions)\n",
    "\n",
    "# tmp_g = pyg_utils.convert.to_networkx(tmp_dataset, to_undirected=True)\n",
    "# def count_deleted_edges(tmp_dataset, c):\n",
    "#     deleted_edges = 0\n",
    "#     for i in range(0, len(tmp_dataset.edge_index[0]), 2):\n",
    "#         for comp in c:\n",
    "#             if tmp_dataset.edge_index[0][i].item() in comp and tmp_dataset.edge_index[1][i].item() not in comp:\n",
    "#                 deleted_edges += 1\n",
    "    \n",
    "#     return deleted_edges\n",
    "\n",
    "# print(\"eng token count: \", tmp_nodes_map['eng-x-bible-mixed'][tmp_verses[0]])\n",
    "# print(\"original connected components\",nx.number_connected_components(tmp_g))\n",
    "\n",
    "# c = list(greedy_modularity_communities(tmp_g))\n",
    "# print(\"new connected_components\", len(c))\n",
    "# print(\"deleted edges: \", count_deleted_edges(tmp_dataset, c))\n",
    "\n",
    "# c = list(asyn_lpa_communities(tmp_g))\n",
    "# print(\"asyn_lpa_communities number of components\", len(c))\n",
    "# print(\"deleted edges: \", count_deleted_edges(tmp_dataset, c))\n",
    "\n",
    "# c = list(label_propagation_communities(tmp_g))\n",
    "# print(\"label_propagation_communities number of components\", len(c))\n",
    "# print(\"deleted edges: \", count_deleted_edges(tmp_dataset, c))\n",
    "\n",
    "# cents = nx.edge_betweenness_centrality(tmp_g)\n",
    "# vals = sorted(list(cents.values()))\n",
    "# print(vals[0], vals[10], vals[100], vals[1000], vals[2000], vals[3000], vals[10000])\n",
    "# print(vals[-1], vals[-10], vals[-100], vals[-1000], vals[-2000], vals[-3000], vals[-10000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure different community detection algorithms\n",
    "# from networkx.algorithms.community import greedy_modularity_communities, asyn_lpa_communities, label_propagation_communities, asyn_fluidc\n",
    "\n",
    "# def remove_bad_community_edges(nodes_map, verses, edition_files, alignments):\n",
    "#     edges_tmp = [[],[]]\n",
    "#     res_edges = [[],[]]\n",
    "#     for verse in verses:\n",
    "#         utils.LOG.info(f\"extracting edge features for {verse}\")\n",
    "#         for i,editf1 in enumerate(edition_files):\n",
    "#             for j,editf2 in enumerate(edition_files[i+1:]):\n",
    "#                 aligns = autils.get_aligns(editf1, editf2, alignments[verse])\n",
    "#                 if aligns != None:\n",
    "#                     for align in aligns:\n",
    "#                         n1, node_count = node_nom(verse, editf1, align[0], 0, nodes_map, None, None)\n",
    "#                         n2, node_count = node_nom(verse, editf2, align[1], 0, nodes_map, None, None)\n",
    "#                         edges_tmp[0].extend([n1, n2])\n",
    "#                         edges_tmp[1].extend([n2, n1])\n",
    "\n",
    "#         gnx = convert_to_netx(edges_tmp)\n",
    "#         print('detecting communities')\n",
    "#         coms = greedy_modularity_communities(gnx)\n",
    "\n",
    "#         print('finding good edges')\n",
    "#         for i in range(0, len(edges_tmp[0]), 2):\n",
    "#             for c in coms:\n",
    "#                 if edges_tmp[0][i] in c and edges_tmp[0][i+1] in c:\n",
    "#                     res_edges[0].extend([edges_tmp[0][i], edges_tmp[0][i+1]])\n",
    "#                     res_edges[1].extend([edges_tmp[0][i+1], edges_tmp[0][i]])\n",
    "#         edges_tmp = [[],[]]\n",
    "#     print('to keep edges:', len(res_edges[0]))\n",
    "#     return torch.tensor(res_edges, dtype=torch.long)\n",
    "\n",
    "# # old_edge_index = train_dataset.edge_index\n",
    "# # new_edge_index = remove_bad_community_edges(train_dataset.nodes_map, train_verses, small_editions, verse_alignments_inter)\n",
    "# # train_dataset.edge_index = new_edge_index\n",
    "\n",
    "# # with open(\"./dataset_greedy_modularity_communities.pickle\", 'rb') as inf:\n",
    "# #     train_dataset = pickle.load(inf)\n",
    "\n",
    "# test_dataset = train_dataset\n",
    "\n",
    "# print('orig edge count', old_edge_index.shape)\n",
    "# print('new edge count', train_dataset.edge_index.shape)\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_map = train_dataset.nodes_map\n",
    "bad_edition_files = []\n",
    "for edit in nodes_map:\n",
    "    bad_count = 0\n",
    "    for verse in nodes_map[edit]:\n",
    "        if len(nodes_map[edit][verse].keys()) < 2:\n",
    "            bad_count += 1\n",
    "        if bad_count > 1:\n",
    "            bad_edition_files.append(edit)\n",
    "            break\n",
    "print(bad_edition_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_japanese_nodes = set()\n",
    "nodes_map = train_dataset.nodes_map\n",
    "\n",
    "for bad_editionf in bad_edition_files:\n",
    "    for verse in nodes_map[bad_editionf]:\n",
    "        for item in nodes_map[bad_editionf][verse].items():\n",
    "            all_japanese_nodes.add(item[1])\n",
    "\n",
    "print(\" all japansese nodes: \", len(all_japanese_nodes))\n",
    "edge_index = train_dataset.edge_index.to('cpu')\n",
    "remaining_edges_index = []\n",
    "for i in tqdm(range(0, edge_index.shape[1], 2)):\n",
    "    if edge_index[0, i].item() not in all_japanese_nodes and edge_index[0, i+1].item() not in all_japanese_nodes:\n",
    "        remaining_edges_index.extend([i, i+1])\n",
    "\n",
    "print('original total edges count', edge_index.shape)\n",
    "print('remaining edge count', len(remaining_edges_index))\n",
    "train_dataset.edge_index = edge_index[:, remaining_edges_index]\n",
    "train_dataset.edge_index.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import greedy_modularity_communities, asyn_lpa_communities, label_propagation_communities, asyn_fluidc\n",
    "\n",
    "def get_community_edges(c, verse):\n",
    "    res = []\n",
    "    for n1 in all_nodes_map[editf1][verse].items():\n",
    "        for n2 in all_nodes_map[editf2][verse].items():\n",
    "            for com in c:\n",
    "                if n1[1] in com and n2[1] in com:\n",
    "                    res.append((n1[0], n2[0]))\n",
    "    \n",
    "    return res\n",
    "\n",
    "def intersect(e1, e2):\n",
    "    res = set()\n",
    "    for item in e1:\n",
    "        if item in e2:\n",
    "            res.add(item)\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "measures = {}\n",
    "measures['intersection']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "measures['gdfa']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "measures['c1_all']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "measures['c1_inter']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "measures['c2_all']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "measures['c2_inter']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "measures['c3_all']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "measures['c3_inter']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "numbers = [100, 300, 600, 1000, 1300, 1600, 2000, 3000, 4000, 5000]\n",
    "for i in numbers:\n",
    "    measures[i] = {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "    measures[i+1] = {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "coms = {}\n",
    "for verse in test_verses:\n",
    "    inter_edges = autils.get_aligns(editf1, editf2, verse_alignments_inter[verse])\n",
    "\n",
    "    all_dataset, all_nodes_map = create_dataset([verse], verse_alignments_inter, small_editions)\n",
    "    print(\"converting\")\n",
    "    g = pyg_utils.convert.to_networkx(all_dataset, to_undirected=True)\n",
    "    print(\"detecting community 1\")\n",
    "    c1 = list(greedy_modularity_communities(g))\n",
    "    print(\"detecting community 2\")\n",
    "    c2 = list(asyn_lpa_communities(g))\n",
    "    print(\"detecting community 3\")\n",
    "    c3 = list(label_propagation_communities(g))\n",
    "\n",
    "    print(\"detecting community 4\")\n",
    "    edge_betweenness = [y[0] for y in sorted(nx.edge_betweenness_centrality(g).items(), key=lambda x: x[1], reverse=True)]\n",
    "    \n",
    "    print('orig communities', nx.number_connected_components(g))\n",
    "    prev_i = 0\n",
    "    for i in numbers:\n",
    "        for j in range(prev_i, i):\n",
    "            g.remove_edge(*edge_betweenness[j])\n",
    "        prev_i = i\n",
    "        com = list(nx.connected_components(g))\n",
    "        edges = get_community_edges(com, verse)\n",
    "        autils.calc_and_update_alignment_score(edges, pros[verse], surs[verse], measures[i])\n",
    "        autils.calc_and_update_alignment_score(intersect(edges, inter_edges), pros[verse], surs[verse], measures[i+1]) \n",
    "        print(f'communities {i}', nx.number_connected_components(g))\n",
    "\n",
    "    c1_edges = get_community_edges(c1, verse)\n",
    "    c2_edges = get_community_edges(c2, verse)\n",
    "    c3_edges = get_community_edges(c3, verse)\n",
    "    print('c1 communities', len(c1))\n",
    "    print('c2 communities', len(c2))\n",
    "    print('c3 communities', len(c3))\n",
    "\n",
    "\n",
    "\n",
    "    autils.calc_and_update_alignment_score(inter_edges, pros[verse], surs[verse], measures['intersection'])\n",
    "    autils.calc_and_update_alignment_score(autils.get_aligns(editf1, editf2, verse_alignments_gdfa[verse]), pros[verse], surs[verse], measures['gdfa'])\n",
    "\n",
    "    autils.calc_and_update_alignment_score(c1_edges, pros[verse], surs[verse], measures['c1_all'])\n",
    "    autils.calc_and_update_alignment_score(c2_edges, pros[verse], surs[verse], measures['c2_all'])\n",
    "    autils.calc_and_update_alignment_score(c3_edges, pros[verse], surs[verse], measures['c3_all'])\n",
    "\n",
    "    autils.calc_and_update_alignment_score(intersect(c1_edges, inter_edges), pros[verse], surs[verse], measures['c1_inter'])\n",
    "    autils.calc_and_update_alignment_score(intersect(c2_edges, inter_edges), pros[verse], surs[verse], measures['c2_inter'])\n",
    "    autils.calc_and_update_alignment_score(intersect(c3_edges, inter_edges), pros[verse], surs[verse], measures['c3_inter'])\n",
    "\n",
    "    for item in measures:\n",
    "        print(item, measures[item])\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aab640873abb67ad450730b814c8d7de015aa287a6fc3bae4b7154b533e57676"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('multalign_graph': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
