{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/Users/student/ayyoob/anaconda3/envs/multalign_graph/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch, sys\n",
    "sys.path.insert(0, '../')\n",
    "from my_utils import gpu_utils\n",
    "import importlib, gc\n",
    "from my_utils.alignment_features import *\n",
    "import my_utils.alignment_features as afeatures\n",
    "importlib.reload(afeatures)\n",
    "import gnn_utils.graph_utils as gutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch-geometric\n",
    "# !pip install tensorboardX\n",
    "\n",
    "# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "# !unzip ngrok-stable-linux-amd64.zip\n",
    "\n",
    "#  print(torch.version.cuda)\n",
    "#  print(torch.__version__)    \n",
    "\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24159\n",
      "reading inter verse alignments\n",
      "done reading inter verse alignments\n"
     ]
    }
   ],
   "source": [
    "from my_utils import align_utils as autils, utils\n",
    "import argparse\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "\n",
    "# set random seed\n",
    "config_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc-ui-demo/config_pbc.ini\"\n",
    "utils.setup(config_file)\n",
    "\n",
    "params = argparse.Namespace()\n",
    "\n",
    "\n",
    "params.gold_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-fin-grc-gold-alignments_train.txt\"\n",
    "pros, surs = autils.load_gold(params.gold_file)\n",
    "all_verses = list(pros.keys())\n",
    "params.gold_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-fin-heb-gold-alignments_train.txt\"\n",
    "pros, surs = autils.load_gold(params.gold_file)\n",
    "all_verses.extend(list(pros.keys()))\n",
    "all_verses = list(set(all_verses))\n",
    "print(len(all_verses))\n",
    "\n",
    "params.editions_file =  \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi_lang_list.txt\"\n",
    "editions, langs = autils.load_simalign_editions(params.editions_file)\n",
    "current_editions = [editions[lang] for lang in langs]\n",
    "\n",
    "def get_pruned_verse_alignments(args):\n",
    "    verse, current_editions = args\n",
    "    \n",
    "    verse_aligns_inter = autils.get_verse_alignments(verse)\n",
    "    verse_aligns_gdfa = autils.get_verse_alignments(verse, gdfa=True)\n",
    "\n",
    "    autils.prune_non_necessary_alignments(verse_aligns_inter, current_editions)\n",
    "    autils.prune_non_necessary_alignments(verse_aligns_gdfa, current_editions)\n",
    "\n",
    "    gc.collect()\n",
    "    return verse_aligns_inter, verse_aligns_gdfa\n",
    "    \n",
    "\n",
    "verse_alignments_inter = {}\n",
    "verse_alignments_gdfa = {}\n",
    "args = []\n",
    "for i,verse in enumerate(all_verses):\n",
    "    args.append((verse, current_editions[:]))\n",
    "\n",
    "#print('going to get alignments')\n",
    "#with Pool(20) as p:\n",
    "#    all_res = p.map(get_pruned_verse_alignments, args)\n",
    "\n",
    "#for i,verse in enumerate(all_verses):\n",
    "#    verse_aligns_inter, verse_aligns_gdfa = all_res[i]\n",
    "    \n",
    "    #verse_alignments_inter[verse] = verse_aligns_inter\n",
    "    #verse_alignments_gdfa[verse] = verse_aligns_gdfa\n",
    "\n",
    "#utils.LOG.info(\"done reading alignments\")\n",
    "#torch.save(verse_alignments_inter, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_train_inter.pickle\")\n",
    "#torch.save(verse_alignments_gdfa, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_train_gdfa.pickle\")\n",
    "#utils.LOG.info('done saving pruned alignments')\n",
    "\n",
    "print('reading inter verse alignments')\n",
    "verse_alignments_inter = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_train_inter.pickle\")\n",
    "gc.collect()\n",
    "print('done reading inter verse alignments')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_channels, hidden_channels)\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.lin3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importlib.reload(afeatures)\n",
    "class Encoder2(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Encoder2, self).__init__()\n",
    "        self.conv1 = pyg_nn.GATConv(in_channels, 2*out_channels)\n",
    "        self.conv2 = pyg_nn.GATConv(2 * out_channels , out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.elu(self.conv1(x, edge_index, ))\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features, n_head = 2, edge_feature_dim = 0,):\n",
    "        super(Encoder, self).__init__()\n",
    "        #self.lin = nn.Linear(in_channels, out_channels)\n",
    "        self.conv1 = pyg_nn.GATConv(in_channels, 2*out_channels, heads= n_head)\n",
    "        self.conv2 = pyg_nn.GATConv(2 * n_head *  out_channels , out_channels, heads= 1)\n",
    "        #self.conv3 = pyg_nn.GATConv(2 * n_head *  out_channels , out_channels, heads= n_head)\n",
    "        #self.f_embedding = nn.Linear(in_channels, in_channels)\n",
    "        self.fin_lin = nn.Linear(out_channels, out_channels)\n",
    "        \n",
    "\n",
    "        self.feature_encoder = afeatures.FeatureEncoding(features, word_vectors)\n",
    "        #self.already_inited = False\n",
    "        #self.prev_edge_index = None\n",
    "        #self.prev_edge_attr = None\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.feature_encoder(x, dev)\n",
    "        #x = F.relu(self.f_embedding(x))\n",
    "        #if not self.already_inited or self.prev_edge_index.data_ptr() != edge_index.data_ptr():\n",
    "        #    edge_index_np = edge_index.cpu().numpy()\n",
    "        #    val_indices = x_edge_np[edge_index_np[0, :], edge_index_np[1, :]]\n",
    "        #    vals = x_edge_vals[val_indices, :]\n",
    "        #    vals = vals.reshape((vals.shape[1], vals.shape[2]))\n",
    "        #    self.prev_edge_attr = vals.to(dev)\n",
    "        #    self.prev_edge_index = edge_index\n",
    "        #    self.already_inited = True\n",
    "        #x = self.lin(x)\n",
    "        x = F.elu(self.conv1(x, edge_index, ))\n",
    "        #x = self.conv_gin(x, edge_index)\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        return F.relu(self.fin_lin(x))#, self.conv3(x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_memory():\n",
    "    gc.collect()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def train(epoch):\n",
    "    global optimizer\n",
    "    total_loss = 0\n",
    "    cluster_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    #for i in tqdm(range(int(train_pos_edge_index_permed.shape[1]/batch_size)+1)):\n",
    "    for i,batch_ in enumerate(tqdm(data_loader)):\n",
    "        for verse in batch_:\n",
    "            batch = batch_[verse]\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            x = batch['x'].to(dev)\n",
    "            edge_index = batch['edge_index'].to(dev)\n",
    "            if torch.max(edge_index) >= x.shape[0]:\n",
    "                print(torch.max(edge_index), x.shape)\n",
    "                print(batch)\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                z = model.encode(x, edge_index)\n",
    "            except Exception as e:\n",
    "                global sag, khar, gav\n",
    "                sag, khar, gav =  (i, batch_, verse)\n",
    "                print(e)\n",
    "                1/0\n",
    "            #z1 = encoder2(z, torch.tensor(batch['intra_sent_edges'], dtype=torch.long).to(dev))\n",
    "            #z = torch.cat((z,z1), dim=1)\n",
    "            #for j in range(5):\n",
    "            #    discriminator_optimizer.zero_grad()\n",
    "            #    discriminator_loss = model.discriminator_loss(z) / (int(train_pos_edge_index_permed.shape[1]/batch_size)+1)\n",
    "            #    discriminator_loss.backward()\n",
    "            #    discriminator_optimizer.step()\n",
    "            \n",
    "            pos = torch.tensor(batch['pos'], dtype=torch.long).to(dev)\n",
    "            neg = torch.tensor(batch['neg'], dtype=torch.long).to(dev)\n",
    "            #nodes = torch.tensor(list(batch['nodes']), dtype=torch.long).to(dev)\n",
    "\n",
    "            loss1 = model.recon_loss( z, pos, neg) #TODO try providing better neg edges\n",
    "            #ortho_loss, mincut_loss, entropy_loss = model.decoder.clustering_loss(z, nodes, batch['adjacency'])\n",
    "            \n",
    "            loss =   loss1 * pos.shape[1] #+ ortho_loss + mincut_loss #+ 0.05 * entropy_loss #* pos.shape[1]/train_neg_edge_index.shape[1] #+ model.reg_loss(z)/(int(train_pos_edge_index_permed.shape[1]/batch_size)+1)# + (1 / x.shape[0]) * model.kl_loss()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() \n",
    "            cluster_loss += loss1\n",
    "\n",
    "            \n",
    "            if i  % 10000 == 9999:\n",
    "                #alignment_test(epoch, test_dataset.edge_index, editf1, editf2, test_verses, test_nodes_map,\n",
    "                #        dev, model, x_test, pros, surs, verse_alignments_inter, verse_alignments_gdfa, writer, gnn_dataset.verse_info)\n",
    "\n",
    "                clean_memory()\n",
    "                \n",
    "                eval_utils.alignment_test(epoch, grc_test_dataset.edge_index, editf_fin, editf_grc, grc_test_verses[:], grc_test_dataset.nodes_map,\n",
    "                        dev, model, grc_test_dataset.x, pros_grc, surs_grc, grc_test_verse_alignments_inter, grc_test_verse_alignments_gdfa, writer, gnn_dataset_grc.verse_info)\n",
    "\n",
    "                eval_utils.alignment_test(epoch, heb_test_dataset.edge_index, editf_fin, editf_heb, heb_test_verses[:], heb_test_dataset.nodes_map,\n",
    "                                        dev, model, heb_test_dataset.x, pros_heb, surs_heb, heb_test_verse_alignments_inter, heb_test_verse_alignments_gdfa, writer, gnn_dataset_heb.verse_info)\n",
    "\n",
    "                eval_utils.alignment_test(epoch, blinker_test_dataset.edge_index, editf12, editf22, blinker_verses, blinker_test_dataset.nodes_map,\n",
    "                            dev, model, blinker_test_dataset.x, pros_blinker, surs_blinker, blinker_verse_alignments_inter, blinker_verse_alignments_gdfa, writer, gnn_dataset_blinker.verse_info)\n",
    "                \n",
    "                clean_memory()   \n",
    "                # decoder.set_objective('sequence_prediction')\n",
    "                # auc, ap = test(edge_index_seq_sent, edge_index_seq_sent_neg, epoch)\n",
    "                # print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))\n",
    "                # if epoch > 4:\n",
    "                #     decoder.set_objective('link_prediction')\n",
    "                model.train()\n",
    "            \n",
    "            #if (i+1)*batch_size > train_pos_edge_index.shape[1]:\n",
    "            #    break\n",
    "\n",
    "            #if i % 51 == 0:\n",
    "            #    clean_memory\n",
    "    \n",
    "    writer.add_scalar(\"loss\", total_loss, epoch)\n",
    "    print(f\"train loss: {total_loss}\")\n",
    "    print(f\"cluster loss: {cluster_loss}\")\n",
    "\n",
    "def test(pos_edge_index, neg_edge_index, epoch):\n",
    "    model.eval()\n",
    "    tot_auc = tot_ap = 0\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(x_test, torch.cat((train_pos_edge_index, neg_edge_index), dim=1).to(dev))\n",
    "        \n",
    "        neg_pos_coeff = neg_edge_index.shape[1]/ pos_edge_index.shape[1]\n",
    "        for i in (range(int(pos_edge_index.shape[1]/batch_size)+1)):\n",
    "            auc,ap = model.test(z, pos_edge_index[:, i*batch_size:(i+1)*batch_size].to(dev),\n",
    "                                neg_edge_index[:, int(i*batch_size*neg_pos_coeff):int((i+1)*batch_size*neg_pos_coeff)].to(dev))\n",
    "\n",
    "            tot_auc += auc * pos_edge_index[:, i*batch_size:(i+1)*batch_size].shape[1]\n",
    "            tot_ap += ap *  pos_edge_index[:, i*batch_size:(i+1)*batch_size].shape[1]\n",
    "\n",
    "\n",
    "    return tot_auc/pos_edge_index.shape[1], tot_ap/pos_edge_index.shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-15\n",
    "\n",
    "def _diag(x):\n",
    "    eye = torch.eye(x.size(0)).type_as(x)\n",
    "    out = eye * x.unsqueeze(1).expand(x.size(0), x.size(0))\n",
    "    return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, edge_features, n_cluster=32):\n",
    "        super(Decoder, self).__init__()\n",
    "        #self.feature_encoder = afeatures.FeatureEncoding(edge_features)\n",
    "        self.features_size = sum([x.out_dim for x in edge_features])\n",
    "        self.representataion_size = (input_size - self.features_size)\n",
    "\n",
    "        self.transfer = nn.Sequential(nn.Linear(input_size, hidden_size*2), nn.ReLU(), nn.Dropout(drop_out),\n",
    "                            #nn.Linear(hidden_size*2, hidden_size), nn.ReLU(), nn.Dropout(drop_out),\n",
    "                            nn.Linear(hidden_size*2, 1))\n",
    "\n",
    "        #self.transfer = nn.Sequential(nn.ELU(), nn.Linear(n_cluster*2, 1), nn.ELU())\n",
    "\n",
    "        #self.n_cluster = n_cluster                \n",
    "        #self.cluster = nn.Sequential(nn.Linear(int((input_size - len(edge_features))/2), hidden_size*2), nn.ELU(), nn.Linear(hidden_size*2, 2*n_cluster))\n",
    "        #self.actual_cluster = nn.Linear(2*n_cluster, n_cluster)\n",
    "        #self.cos = nn.CosineSimilarity(dim=1)        \n",
    "        #self.dist = nn.PairwiseDistance()\n",
    "        #self.gnn_transform = nn.Sequential(nn.Linear(self.representataion_size, hidden_size), nn.ReLU(), nn.Dropout(drop_out))\n",
    "        self.counter = 0\n",
    "\n",
    "        self.objective = 'link_prediction'\n",
    "    def forward(self, z, edge_index, sigmoid = True):\n",
    "        if self.features_size > 0:\n",
    "            if self.objective == 'link_prediction':\n",
    "                edge_index_np = edge_index.cpu().numpy()\n",
    "                val_indices = x_edge_np[edge_index_np[0, :], edge_index_np[1, :]]\n",
    "                val_indices = np.squeeze(np.asarray(val_indices))\n",
    "                vals = x_edge_vals2[val_indices, :]\n",
    "            elif self.objective == 'sequence_prediction':\n",
    "                vals = torch.zeros((edge_index.shape[1], self.features_size)).to(dev)\n",
    "\n",
    "\n",
    "            features = self.feature_encoder(vals.to(dev), dev)\n",
    "            #features = vals.to(dev)\n",
    "            h1 = z[edge_index[0, :]]\n",
    "            h2 = z[edge_index[1, :]]\n",
    "\n",
    "            self.counter += 1\n",
    "\n",
    "            #rep = self.gnn_transform(torch.cat((h1, h2), dim=1))\n",
    "            res = self.transfer(torch.cat((self.cluster(h1), self.cluster(h2), features), dim=1))\n",
    "            #res = self.transfer(features)\n",
    "        else:\n",
    "            h1 = z[edge_index[0, :]]\n",
    "            h2 = z[edge_index[1, :]]\n",
    "\n",
    "            \n",
    "            res = self.transfer(torch.cat((h1, h2), dim=-1))\n",
    "\n",
    "            #res = self.transfer(torch.cat((self.cluster(h1), self.cluster(h2)), dim=1))\n",
    "            #res = torch.sum(torch.pow(F.softmax(self.cluster(h1)/1, dim=1) - F.softmax(self.cluster(h2)/1, dim=1), 2), dim=1)\n",
    "            #res = self.cos(self.cluster(h1), self.cluster(h2))\n",
    "            #res = - self.dist(self.cluster(h1), self.cluster(h2))\n",
    "            #print(res)\n",
    "        res = torch.sigmoid(res) if sigmoid else res\n",
    "        return res\n",
    "\n",
    "    def set_objective(self, objective):\n",
    "        self.objective = objective\n",
    "        \n",
    "    def clustering_loss(self, z, nodes, adjacency):\n",
    "        s = self.actual_cluster(torch.relu(self.cluster(z[nodes])))\n",
    "        s = torch.softmax(s, dim=-1)\n",
    "        entropy_loss = (-s * torch.log(s + EPS)).sum(dim=-1).mean()\n",
    "\n",
    "        ss = torch.matmul(s.transpose(0, 1), s)\n",
    "        i_s = torch.eye(self.n_cluster).type_as(ss)\n",
    "        ortho_loss = torch.norm(\n",
    "            ss / torch.norm(ss, dim=(-1, -2), keepdim=True) -\n",
    "            i_s / torch.norm(i_s), dim=(-1, -2))\n",
    "        ortho_loss = torch.mean(ortho_loss)\n",
    "\n",
    "        adjacency = adjacency.to(dev).float()\n",
    "        out_adj = torch.matmul(s.transpose(0, 1),torch.sparse.mm(adjacency, s))\n",
    "        # MinCUT regularization.\n",
    "        mincut_num = torch.trace(out_adj)\n",
    "        #d_flat = torch.einsum('ij->i', adjacency) # FIXME since I don't consider the whole adjacency matrix this could be a source of problem\n",
    "        d_flat = torch.sparse.sum(adjacency, dim=1).to_dense()\n",
    "        d = _diag(d_flat)\n",
    "        mincut_den = torch.trace(\n",
    "            torch.matmul(torch.matmul(s.transpose(0, 1), d), s))\n",
    "        mincut_loss = -(mincut_num / mincut_den)\n",
    "        mincut_loss = torch.mean(mincut_loss)\n",
    "\n",
    "        return ortho_loss, mincut_loss, entropy_loss\n",
    "    \n",
    "    def get_alignments(self, z, edge_index):\n",
    "        h1 = z[edge_index[0, :]]\n",
    "        h2 = z[edge_index[1, :]]\n",
    "        \n",
    "        h1 = torch.softmax(self.cluster(h1), dim=1)\n",
    "        h2 = torch.softmax(self.cluster(h2), dim=1)\n",
    "\n",
    "        h1_max = torch.argmax(h1, dim=1)\n",
    "        h2_max = torch.argmax(h2, dim=1)\n",
    "\n",
    "        h1_cluster = torch.zeros(*h1.shape)\n",
    "        h2_cluster = torch.zeros(*h2.shape)\n",
    "\n",
    "        h1_cluster[range(h1.size(0)), h1_max] = 1\n",
    "        h2_cluster[range(h2.size(0)), h2_max] = 1\n",
    "        res = torch.max(h1_cluster * h2_cluster, dim=1).values\n",
    "\n",
    "        #res = h1 * h2\n",
    "        #res = torch.sum(res, dim = 1)\n",
    "        return torch.unsqueeze(res, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30723771, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "train_verses = all_verses[:]\n",
    "test_verses = all_verses[:] \n",
    "editf1 = 'fin-x-bible-helfi'\n",
    "editf2 = \"heb-x-bible-helfi\"\n",
    "\n",
    "\n",
    "if 'jpn-x-bible-newworld' in  current_editions[:]:\n",
    "     current_editions.remove('jpn-x-bible-newworld')\n",
    "if 'grc-x-bible-unaccented' in  current_editions[:]:\n",
    "     current_editions.remove('grc-x-bible-unaccented')\n",
    "\n",
    "train_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_helfi_train_community_word.pickle\", map_location=torch.device('cpu'))\n",
    "#train_dataset, train_nodes_map = create_dataset(train_verses, verse_alignments_inter, small_editions)\n",
    "features = train_dataset.features\n",
    "train_nodes_map = train_dataset.nodes_map\n",
    "#edge_index_intra_sent = train_dataset.edge_index_intra_sent\n",
    "#test_edge_index_intra_sent = edge_index_intra_sent\n",
    "\n",
    "# test_dataset, test_nodes_map = create_dataset(test_verses, verse_alignments_inter, small_editions)\n",
    "test_dataset, test_nodes_map = train_dataset, train_nodes_map\n",
    "test_verses = train_verses\n",
    "print(train_dataset.x.shape)\n",
    "\n",
    "# gutils.augment_features(test_dataset)\n",
    "# x_edge, features_edge = gutils.create_edge_attribs(train_nodes_map, train_verses, small_editions, verse_alignments_inter, train_dataset.x.shape[0])\n",
    "# with open(\"./dataset.pickle\", 'wb') as of:\n",
    "#     pickle.dump(train_dataset, of)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2354770, 100)\n",
      "torch.Size([2354770, 100])\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec.load(\"/mounts/work/ayyoob/models/w2v/word2vec_helfi_langs_15e.model\")\n",
    "\n",
    "print(w2v_model.wv.vectors.shape)\n",
    "\n",
    "word_vectors = torch.from_numpy(w2v_model.wv.vectors).float()\n",
    "\n",
    "print(word_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-14 23:19:55,134 - analytics - INFO - done reading alignments\n",
      "2021-11-14 23:19:55,136 - analytics - INFO - done saving pruned alignments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading inter verse alignments\n",
      "done reading inter verse alignments\n"
     ]
    }
   ],
   "source": [
    "blinker_test_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_blinker_full_community_word.pickle\", map_location=torch.device('cpu'))\n",
    "editf12 = \"eng-x-bible-mixed\"\n",
    "editf22 = 'fra-x-bible-louissegond'\n",
    "\n",
    "test_gold_eng_fra = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/eng_fra_pbc/eng-fra.gold\"\n",
    "\n",
    "pros_blinker, surs_blinker = autils.load_gold(test_gold_eng_fra)\n",
    "blinker_verses = list(pros_blinker.keys())\n",
    "\n",
    "#blinker_verse_alignments_inter = {}\n",
    "#blinker_verse_alignments_gdfa = {}\n",
    "#args = []\n",
    "#for i,verse in enumerate(blinker_verses):\n",
    "#    args.append((verse, current_editions))\n",
    "\n",
    "#with Pool(20) as p:\n",
    "#    all_res = p.map(get_pruned_verse_alignments, args)\n",
    "\n",
    "#for i,verse in enumerate(blinker_verses):\n",
    "#    verse_aligns_inter, verse_aligns_gdfa = all_res[i]\n",
    "    \n",
    "#    blinker_verse_alignments_inter[verse] = verse_aligns_inter\n",
    "#    blinker_verse_alignments_gdfa[verse] = verse_aligns_gdfa\n",
    "\n",
    "utils.LOG.info(\"done reading alignments\")\n",
    "#torch.save(blinker_verse_alignments_inter, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_inter.pickle\")\n",
    "#torch.save(blinker_verse_alignments_gdfa, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_gdfa.pickle\")\n",
    "utils.LOG.info('done saving pruned alignments')\n",
    "\n",
    "print('reading inter verse alignments')\n",
    "blinker_verse_alignments_inter = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_inter.pickle\")\n",
    "blinker_verse_alignments_gdfa = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_gdfa.pickle\")\n",
    "gc.collect()\n",
    "print('done reading inter verse alignments')\n",
    "\n",
    "verses_map = {}\n",
    "\n",
    "for edit in blinker_test_dataset.nodes_map:\n",
    "    for verse in blinker_test_dataset.nodes_map[edit]:\n",
    "        if verse not in verses_map:\n",
    "            for tok in blinker_test_dataset.nodes_map[edit][verse]:\n",
    "                verses_map[verse] = blinker_test_dataset.nodes_map[edit][verse][tok]\n",
    "                break\n",
    "\n",
    "sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "blinker_verses = [item[0] for item in sorted_verses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##importlib.reload(afeatures)\n",
    "#grc_test_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_helfi_grc_test_community_word.pickle\", map_location=torch.device('cpu'))\n",
    "#editf_fin = \"fin-x-bible-helfi\"\n",
    "#editf_grc = 'grc-x-bible-helfi'\n",
    "\n",
    "#test_gold_grc = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-grc-fin-gold-alignments_test.txt\"\n",
    "\n",
    "#pros_grc, surs_grc = autils.load_gold(test_gold_grc)\n",
    "#grc_verses = list(pros_grc.keys())\n",
    "\n",
    "\n",
    "#grc_test_verse_alignments_inter = {}\n",
    "#grc_test_verse_alignments_gdfa = {}\n",
    "#gc.collect()\n",
    "##args = []\n",
    "##for i,verse in enumerate(grc_verses):\n",
    "##    args.append((verse, current_editions))\n",
    "\n",
    "##with Pool(20) as p:\n",
    "##    all_res = p.map(get_pruned_verse_alignments, args)\n",
    "\n",
    "##for i,verse in enumerate(grc_verses):\n",
    "##    verse_aligns_inter, verse_aligns_gdfa = all_res[i]\n",
    "    \n",
    "##    grc_test_verse_alignments_inter[verse] = verse_aligns_inter\n",
    "##    grc_test_verse_alignments_gdfa[verse] = verse_aligns_gdfa\n",
    "\n",
    "#utils.LOG.info(\"done reading alignments\")\n",
    "##torch.save(grc_test_verse_alignments_inter, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_grc_inter.pickle\")\n",
    "##torch.save(grc_test_verse_alignments_gdfa, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_grc_gdfa.pickle\")\n",
    "#utils.LOG.info('done saving pruned alignments')\n",
    "\n",
    "#print('reading inter verse alignments')\n",
    "#grc_test_verse_alignments_inter = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_grc_inter.pickle\")\n",
    "#grc_test_verse_alignments_gdfa = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_grc_gdfa.pickle\")\n",
    "#gc.collect()\n",
    "#print('done reading inter verse alignments')\n",
    "\n",
    "#verses_map = {}\n",
    "\n",
    "#for edit in grc_test_dataset.nodes_map:\n",
    "#    for verse in grc_test_dataset.nodes_map[edit]:\n",
    "#        if verse not in verses_map:\n",
    "#            for tok in grc_test_dataset.nodes_map[edit][verse]:\n",
    "#                verses_map[verse] = grc_test_dataset.nodes_map[edit][verse][tok]\n",
    "#                break\n",
    "\n",
    "#sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "#grc_test_verses = [item[0] for item in sorted_verses]\n",
    "\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading inter verse alignments\n",
      "done reading inter verse alignments\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heb_test_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_helfi_heb_test_community_word.pickle\", map_location=torch.device('cpu'))\n",
    "editf_fin = \"fin-x-bible-helfi\"\n",
    "editf_heb = 'heb-x-bible-helfi'\n",
    "\n",
    "test_gold_heb = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-fin-heb-gold-alignments_test.txt\"\n",
    "\n",
    "pros_heb, surs_heb = autils.load_gold(test_gold_heb)\n",
    "heb_verses = list(pros_heb.keys())\n",
    "\n",
    "\n",
    "heb_test_verse_alignments_inter = {}\n",
    "heb_test_verse_alignments_gdfa = {}\n",
    "#args = []\n",
    "#for i,verse in enumerate(heb_verses):\n",
    "#    args.append((verse, current_editions))\n",
    "\n",
    "#with Pool(20) as p:\n",
    "#    all_res = p.map(get_pruned_verse_alignments, args)\n",
    "\n",
    "#for i,verse in enumerate(heb_verses):\n",
    "#    verse_aligns_inter, verse_aligns_gdfa = all_res[i]\n",
    "    \n",
    "#    heb_test_verse_alignments_inter[verse] = verse_aligns_inter\n",
    "#    heb_test_verse_alignments_gdfa[verse] = verse_aligns_gdfa\n",
    "\n",
    "#utils.LOG.info(\"done reading alignments\")\n",
    "#torch.save(heb_test_verse_alignments_inter, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_heb_inter.pickle\")\n",
    "#torch.save(heb_test_verse_alignments_gdfa, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_heb_gdfa.pickle\")\n",
    "#utils.LOG.info('done saving pruned alignments')\n",
    "\n",
    "print('reading inter verse alignments')\n",
    "heb_test_verse_alignments_inter = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_heb_inter.pickle\")\n",
    "heb_test_verse_alignments_gdfa = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_heb_gdfa.pickle\")\n",
    "gc.collect()\n",
    "print('done reading inter verse alignments')\n",
    "\n",
    "verses_map = {}\n",
    "\n",
    "for edit in heb_test_dataset.nodes_map:\n",
    "    for verse in heb_test_dataset.nodes_map[edit]:\n",
    "        if verse not in verses_map:\n",
    "            for tok in heb_test_dataset.nodes_map[edit][verse]:\n",
    "                verses_map[verse] = heb_test_dataset.nodes_map[edit][verse][tok]\n",
    "                break\n",
    "\n",
    "sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "heb_test_verses = [item[0] for item in sorted_verses]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "verses_map = {}\n",
    "\n",
    "for edit in train_dataset.nodes_map:\n",
    "    for verse in train_dataset.nodes_map[edit]:\n",
    "        if verse not in verses_map:\n",
    "            for tok in train_dataset.nodes_map[edit][verse]:\n",
    "                verses_map[verse] = train_dataset.nodes_map[edit][verse][tok]\n",
    "                break\n",
    "\n",
    "sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "all_verses = [item[0] for item in sorted_verses]\n",
    "\n",
    "long_verses = set()\n",
    "\n",
    "for edit in train_dataset.nodes_map.keys():\n",
    "    for verse in train_dataset.nodes_map[edit]:\n",
    "        to_print = False\n",
    "        for tok in train_dataset.nodes_map[edit][verse]:\n",
    "            if tok > 150:\n",
    "                to_print = True\n",
    "        if to_print == True:\n",
    "            long_verses.add(verse)\n",
    "\n",
    "\n",
    "train_verses = all_verses[:]\n",
    "\n",
    "masked_verses = list(long_verses)\n",
    "#masked_verses.extend(blinker_verses)\n",
    "\n",
    "#grc_train_gold_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-grc-fin-gold-alignments_train.txt\"\n",
    "#grc_train_pros, grc_train_surs = autils.load_gold(grc_train_gold_file)\n",
    "#grc_train_verses = list(pros.keys())\n",
    "\n",
    "#masked_verses.extend(grc_train_verses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "not presented: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:15<00:00, 16.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "class GNNDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset, verses, edit_files, alignments, group_size = 360):\n",
    "        self.verses = list(verses)\n",
    "        self.edit_files = list(edit_files)\n",
    "        self.nodes_map = dataset.nodes_map\n",
    "\n",
    "        self.items = self.calculate_size(self.nodes_map, self.verses, self.edit_files, group_size)\n",
    "        self.alignments = alignments\n",
    "        self.verse_info = {}\n",
    "        self.calculate_verse_stats(verses, edit_files, alignments, dataset)\n",
    "    \n",
    "    def calculate_size(self, nodes_map, verses, edit_files, group_size):\n",
    "        res = []\n",
    "        item = []\n",
    "        self.not_presented = []\n",
    "        print('1')\n",
    "        verses = list(set(verses) - set(masked_verses))\n",
    "        print('2')\n",
    "        for verse in verses:\n",
    "            \n",
    "            if len(item) > 0:\n",
    "                res.append(item)\n",
    "                item = []\n",
    "            for i,editf1 in enumerate(edit_files):\n",
    "                if editf1 not in nodes_map:\n",
    "                    self.not_presented.append(editf1)\n",
    "                    continue\n",
    "                if verse in nodes_map[editf1]:\n",
    "                    for editf2 in edit_files[i+1:]:\n",
    "                        if editf2 not in nodes_map:\n",
    "                            self.not_presented.append(editf2)\n",
    "                            continue\n",
    "                        if verse in nodes_map[editf2]:\n",
    "                            item.append((verse, editf1, editf2))\n",
    "                            if len(item) >= group_size:\n",
    "                                res.append(item)\n",
    "                                item = []\n",
    "        \n",
    "        if len(item)>0:\n",
    "            res.append(item)\n",
    "        \n",
    "        print(f\"not presented: {set(self.not_presented)}\")\n",
    "\n",
    "        return res\n",
    "\n",
    "    def calculate_verse_stats(self,verses, edition_files, alignments, dataset):\n",
    "        min_edge = 0\n",
    "        for verse in tqdm(verses):\n",
    "            min_nodes = 99999999999999\n",
    "            max_nodes = 0\n",
    "            #utils.LOG.info(f\"adding {verse}\")\n",
    "            edges_tmp = [[],[]]\n",
    "            x_tmp = []\n",
    "            features = []\n",
    "            for i,editf1 in enumerate(edition_files):\n",
    "                for j,editf2 in enumerate(edition_files[i+1:]):\n",
    "                    aligns = autils.get_aligns(editf1, editf2, alignments[verse])\n",
    "                    if aligns != None:\n",
    "                        for align in aligns:\n",
    "                            try:\n",
    "                                n1,_ = gutils.node_nom(verse, editf1, align[0], None, dataset.nodes_map, x_tmp, edition_files, features)\n",
    "                                n2,_ = gutils.node_nom(verse, editf2, align[1], None, dataset.nodes_map, x_tmp, edition_files, features)\n",
    "                                edges_tmp[0].extend([n1, n2])\n",
    "\n",
    "                                max_nodes = max(n1, n2, max_nodes)\n",
    "                                min_nodes = min(n1, n2, min_nodes)\n",
    "                            except Exception as e:\n",
    "                                print(editf1, editf2, verse)\n",
    "                                raise(e)\n",
    "\n",
    "            self.verse_info[verse] = {}\n",
    "\n",
    "            self.verse_info[verse]['padding'] = min_nodes\n",
    "\n",
    "            self.verse_info[verse]['x'] = dataset.x[min_nodes:max_nodes+1,:]\n",
    "            \n",
    "            self.verse_info[verse]['edge_index'] = dataset.edge_index[:, min_edge : min_edge + len(edges_tmp[0])] - min_nodes\n",
    "\n",
    "            if torch.min(self.verse_info[verse]['edge_index']) != 0:\n",
    "                print(verse, min_nodes, max_nodes, min_edge, len(edges_tmp[0]))\n",
    "                print(torch.min(self.verse_info[verse]['edge_index']))\n",
    "            \n",
    "            if self.verse_info[verse]['x'].shape[0] != torch.max(self.verse_info[verse]['edge_index']) + 1 :\n",
    "                print(verse, min_nodes, max_nodes, min_edge, len(edges_tmp[0]))\n",
    "                print(torch.min(self.verse_info[verse]['edge_index']))\n",
    "            \n",
    "            min_edge = min_edge + len(edges_tmp[0])\n",
    "\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        #return self.length\n",
    "        return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        item = self.items[idx]\n",
    "\n",
    "        res_pos = [[],[]]\n",
    "        res_neg = [[],[]]\n",
    "        nodes = set()\n",
    "        for instance in item:\n",
    "            verse, editf1, editf2 = instance\n",
    "            aligns = autils.get_aligns(editf1, editf2, self.alignments[verse]) \n",
    "            if aligns != None:\n",
    "                for align in aligns:\n",
    "                    p1, p2 = align\n",
    "                    n1 = self.nodes_map[editf1][verse][p1] - self.verse_info[verse]['padding']\n",
    "                    n2 = self.nodes_map[editf2][verse][p2] - self.verse_info[verse]['padding']\n",
    "                    res_pos[0].extend([n1,n2])\n",
    "                    res_pos[1].extend([n2,n1])\n",
    "\n",
    "                    n2_ = random.choice( list(self.nodes_map[editf2][verse].values()) ) - self.verse_info[verse]['padding']\n",
    "                    n1_ = random.choice( list(self.nodes_map[editf1][verse].values()) ) - self.verse_info[verse]['padding']\n",
    "                    \n",
    "                    if n2_ != n2:\n",
    "                        res_neg[0].extend([n1, n2_])\n",
    "                        res_neg[1].extend([n2_, n1])\n",
    "                    \n",
    "                    if n1_ != n1:\n",
    "                        res_neg[0].extend([n1_, n2])\n",
    "                        res_neg[1].extend([n2, n1_])\n",
    "                    \n",
    "                    #nodes.update([n1, n2, n1_, n2_])\n",
    "                \n",
    "        \n",
    "        return {'pos':res_pos, 'neg':res_neg, 'nodes':nodes, 'verse':verse, 'editf1':editf1, 'editf2':editf2}\n",
    "\n",
    "def collate_fun(input):\n",
    "    res = {}\n",
    "    #all_edits = {}\n",
    "    for item in input:\n",
    "        verse = item['verse'] \n",
    "        if verse not in res :\n",
    "            res[verse] = {'pos': [[],[]], 'neg' : [[],[]],\n",
    "                 'x':gnn_dataset.verse_info[verse]['x'], 'edge_index':gnn_dataset.verse_info[verse]['edge_index']\n",
    "                 ,'intra_sent_edges':[[],[]]}\n",
    "        \n",
    "        res[verse]['pos'][0].extend(item['pos'][0])\n",
    "        res[verse]['pos'][1].extend(item['pos'][1])\n",
    "\n",
    "        res[verse]['neg'][0].extend(item['neg'][0])\n",
    "        res[verse]['neg'][1].extend(item['neg'][1])\n",
    "\n",
    "        #if verse not in all_edits:\n",
    "        #    all_edits[verse] = []\n",
    "\n",
    "        #if item['editf1'] not in all_edits[verse]:\n",
    "        #    e = eval_utils.get_all_edges(verse, item['editf1'], train_dataset.nodes_map, gnn_dataset.verse_info)\n",
    "        #    res[verse]['intra_sent_edges'][0].extend(e[0])\n",
    "        #    res[verse]['intra_sent_edges'][1].extend(e[1])\n",
    "\n",
    "        #if item['editf2'] not in all_edits[verse]:\n",
    "        #    e = eval_utils.get_all_edges(verse, item['editf2'], train_dataset.nodes_map, gnn_dataset.verse_info)\n",
    "        #    res[verse]['intra_sent_edges'][0].extend(e[0])\n",
    "        #    res[verse]['intra_sent_edges'][1].extend(e[1])\n",
    "\n",
    "    #nodes = list(nodes)\n",
    "    #mapping = {node:pos for pos, node in enumerate(nodes)}\n",
    "    ##indices = [[i for i in range(len(res_pos[0]))],[i for i in range(len(res_pos[1]))]]\n",
    "    #indices = [[],[]]\n",
    "    ##adjacency = torch.zeros((len(nodes), len(nodes)), dtype=torch.float)\n",
    "    #for i in range(len(res_pos[0])):\n",
    "    ##    adjacency[mapping[res_pos[0][i]], mapping[res_pos[1][i]]] = 1\n",
    "    #    indices[0].append(mapping[res_pos[0][i]])\n",
    "    #    indices[1].append(mapping[res_pos[1][i]])\n",
    "\n",
    "    #adjacency = torch.sparse_coo_tensor(indices, [1 for i in range(len(res_pos[0]))], (len(nodes), len(nodes)))\n",
    "    return res\n",
    "\n",
    "\n",
    "#gnn_dataset_train = GNNDataset(train_dataset, train_verses, current_editions, verse_alignments_inter)\n",
    "gnn_dataset_blinker = GNNDataset(blinker_test_dataset, blinker_verses, current_editions, blinker_verse_alignments_inter)\n",
    "#gnn_dataset_heb = GNNDataset(heb_test_dataset, heb_test_verses, current_editions, heb_test_verse_alignments_inter)\n",
    "#gnn_dataset_grc = GNNDataset(grc_test_dataset, grc_test_verses, current_editions, grc_test_verse_alignments_inter)\n",
    "\n",
    "#len(gnn_dataset_train)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnn_utils import eval_utils\n",
    "train_dataset.train_mask = train_dataset.val_mask = train_dataset.test_mask = train_dataset.y = None\n",
    "test_dataset.train_mask = test_dataset.val_mask = test_dataset.test_mask = test_dataset.y = None\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "features = train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name):\n",
    "    model.encoder.feature_encoder.feature_types[0] = afeatures.OneHotFeature(20, 83, 'editf')\n",
    "    model.encoder.feature_encoder.feature_types[1] = afeatures.OneHotFeature(32, 150, 'position')\n",
    "    model.encoder.feature_encoder.feature_types[2] = afeatures.FloatFeature(4, 'degree_centrality')\n",
    "    model.encoder.feature_encoder.feature_types[3] = afeatures.FloatFeature(4, 'closeness_centrality')\n",
    "    model.encoder.feature_encoder.feature_types[4] = afeatures.FloatFeature(4, 'betweenness_centrality')\n",
    "    model.encoder.feature_encoder.feature_types[5] = afeatures.FloatFeature(4, 'load_centrality')\n",
    "    model.encoder.feature_encoder.feature_types[6] = afeatures.FloatFeature(4, 'harmonic_centrality')\n",
    "    model.encoder.feature_encoder.feature_types[7] = afeatures.OneHotFeature(32, 250, 'greedy_modularity_community')\n",
    "    model.encoder.feature_encoder.feature_types[8] = afeatures.OneHotFeature(32, 250, 'community_2')\n",
    "    model.encoder.feature_encoder.feature_types[9] = afeatures.MappingFeature(100, 'word')\n",
    "    torch.save(model, f'/mounts/work/ayyoob/models/gnn/checkpoint/gnn_256_flggll_word_halfTrain_nofeatlinear_encoderlineear_decoderonelayer_onlyheb_{name}' + datetime.now().strftime(\"%Y%m%d-%H%M%S-\") + '.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importlib.reload(eval_utils)\n",
    "#model = torch.load('/mounts/work/ayyoob/models/gnn/checkpoint/gnn_512_flggll_word_halfTrain_nofeatlinear_encoderlineear_decoderonelayer_onlyheb_epoch220210912-043444-.pickle').to(dev)\n",
    "res_file = open('/mounts/work/ayyoob/results/gnn_align/new_aignments/fin_heb/TGDFA_aligns.txt', 'w')\n",
    "orig_file  = open('/mounts/work/ayyoob/results/gnn_align/new_aignments/fin_heb/orig_gdfa_aligns.txt', 'w')\n",
    "\n",
    "\n",
    "#writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S-\") + f\"samett-1chs-feat{train_dataset.num_node_features}-\")\n",
    "#res = eval_utils.alignment_test(1, heb_test_dataset.edge_index, editf_fin, editf_heb, heb_test_verses[:], heb_test_dataset.nodes_map,\n",
    "#                        dev, model, heb_test_dataset.x, pros_heb, surs_heb, heb_test_verse_alignments_inter, heb_test_verse_alignments_gdfa, writer, gnn_dataset_heb.verse_info)\n",
    "\n",
    "for verse in res:\n",
    "    out_string = ' '.join([f'{x[0]}-{x[1]}' for x in res[verse]['GNN_my_gd']])\n",
    "    res_file.write(verse + '\\t' + out_string+'\\n')\n",
    "\n",
    "    orig_aligns = autils.get_aligns(editf1, editf2, heb_test_verse_alignments_gdfa[verse])\n",
    "    out_string = ' '.join([f'{x[0]}-{x[1]}' for x in orig_aligns])\n",
    "    orig_file.write(verse + '\\t' + out_string+'\\n')\n",
    "\n",
    "res_file.close()\n",
    "orig_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01003018\n",
      "01010010\n",
      "01027033\n",
      "01029027\n",
      "01036011\n",
      "01036016\n",
      "01046014\n",
      "01048002\n",
      "01048010\n",
      "02019018\n",
      "02022002\n",
      "02028017\n",
      "02032026\n",
      "02039010\n",
      "03013057\n",
      "03014043\n",
      "03015031\n",
      "03017013\n",
      "03026016\n",
      "04003008\n",
      "04003027\n",
      "04006009\n",
      "04011008\n",
      "04011020\n",
      "04014019\n",
      "04026026\n",
      "04033001\n",
      "04033022\n",
      "04033023\n",
      "05006007\n",
      "05008011\n",
      "05017009\n",
      "06009021\n",
      "06009023\n",
      "06009027\n",
      "06011017\n",
      "06012007\n",
      "06019033\n",
      "07001005\n",
      "07015005\n",
      "09003011\n",
      "09009009\n",
      "09012023\n",
      "09017018\n",
      "10001022\n",
      "10002025\n",
      "10005023\n",
      "10010016\n",
      "10010017\n",
      "10013015\n",
      "10020026\n",
      "10023012\n",
      "11002027\n",
      "11004003\n",
      "11004012\n",
      "11006006\n",
      "11007031\n",
      "11007045\n",
      "11022010\n",
      "12008011\n",
      "12011004\n",
      "12011019\n",
      "12015016\n",
      "12021012\n",
      "13001036\n",
      "13003021\n",
      "13004041\n",
      "13006068\n",
      "13007033\n",
      "13008032\n",
      "13009037\n",
      "13009038\n",
      "13011014\n",
      "13011044\n",
      "13014014\n",
      "13018008\n",
      "13023019\n",
      "13024022\n",
      "13024023\n",
      "13025008\n",
      "13026023\n",
      "13026029\n",
      "13026031\n",
      "13026032\n",
      "13027004\n",
      "14011023\n",
      "14018009\n",
      "14022004\n",
      "14034013\n",
      "14036014\n",
      "15005008\n",
      "15008023\n",
      "16003008\n",
      "16003031\n",
      "16003032\n",
      "17003012\n",
      "17008009\n",
      "17009030\n",
      "19002002\n",
      "19033010\n",
      "19049006\n",
      "19056001\n",
      "19057004\n",
      "19068006\n",
      "19077019\n",
      "19078053\n",
      "19083017\n",
      "19088018\n",
      "19092006\n",
      "19092012\n",
      "19096010\n",
      "19098009\n",
      "19099004\n",
      "19099005\n",
      "19099009\n",
      "19104022\n",
      "19109008\n",
      "19119016\n",
      "19124004\n",
      "19144006\n",
      "20010003\n",
      "20015023\n",
      "20017007\n",
      "20018007\n",
      "20024017\n",
      "20024027\n",
      "22004008\n",
      "23003009\n",
      "23005027\n",
      "23008002\n",
      "23008008\n",
      "23010015\n",
      "23019020\n",
      "23029021\n",
      "23040022\n",
      "23041003\n",
      "23052013\n",
      "24014006\n",
      "24015018\n",
      "24018013\n",
      "24019003\n",
      "24021002\n",
      "24022016\n",
      "24023005\n",
      "24023019\n",
      "24030023\n",
      "24031015\n",
      "24040008\n",
      "24043012\n",
      "24050033\n",
      "26001007\n",
      "26001028\n",
      "26005017\n",
      "26006009\n",
      "26010004\n",
      "26012006\n",
      "26020043\n",
      "26021019\n",
      "26023034\n",
      "26028013\n",
      "26032010\n",
      "26036031\n",
      "26041006\n",
      "26046023\n",
      "26048030\n",
      "27003004\n",
      "27009011\n",
      "27009025\n",
      "27009026\n",
      "27010006\n",
      "29003021\n",
      "30005024\n",
      "33005002\n",
      "33007017\n",
      "34002012\n",
      "35002002\n",
      "35003005\n",
      "40002018\n",
      "40007016\n",
      "40010024\n",
      "40010025\n",
      "40027024\n",
      "41001026\n",
      "41003034\n",
      "41009026\n",
      "41012034\n",
      "42005007\n",
      "42005010\n",
      "42006040\n",
      "42023014\n",
      "42023030\n",
      "43011010\n",
      "43013022\n",
      "44001020\n",
      "44002010\n",
      "44004026\n",
      "44005036\n",
      "44007042\n",
      "44008027\n",
      "44010004\n",
      "44012019\n",
      "44013001\n",
      "44014006\n",
      "44014012\n",
      "44014013\n",
      "44014020\n",
      "44016001\n",
      "44017011\n",
      "44020004\n",
      "44024003\n",
      "44024014\n",
      "44025024\n",
      "44026012\n",
      "44027006\n",
      "44028011\n",
      "44028018\n",
      "45011024\n",
      "45012008\n",
      "46001014\n",
      "46004017\n",
      "46007036\n",
      "46014009\n",
      "46014019\n",
      "47003013\n",
      "48002017\n",
      "48004008\n",
      "49005007\n",
      "49005032\n",
      "50001025\n",
      "50004005\n",
      "51003015\n",
      "51004002\n",
      "52002017\n",
      "54002007\n",
      "54003013\n",
      "54004015\n",
      "55001011\n",
      "56001010\n",
      "57001011\n",
      "58001003\n",
      "58006008\n",
      "58010022\n",
      "58012028\n",
      "59002010\n",
      "63001007\n",
      "66002018\n",
      "66016009\n",
      "66021006\n",
      "66021020\n",
      "66022013\n"
     ]
    }
   ],
   "source": [
    "res_file = open('/mounts/work/ayyoob/results/gnn_align/new_aignments/eng_fra/TGDFA_aligns.txt', 'w')\n",
    "orig_file  = open('/mounts/work/ayyoob/results/gnn_align/new_aignments/eng_fra/orig_gdfa_aligns.txt', 'w')\n",
    "\n",
    "#res = eval_utils.alignment_test(1, blinker_test_dataset.edge_index, editf12, editf22, blinker_verses[:], blinker_test_dataset.nodes_map,\n",
    "#                    dev, model, blinker_test_dataset.x, pros_blinker, surs_blinker, blinker_verse_alignments_inter, blinker_verse_alignments_gdfa, writer, gnn_dataset_blinker.verse_info)\n",
    "\n",
    "for verse in res:\n",
    "    out_string = ' '.join([f'{x[0]}-{x[1]}' for x in res[verse]['GNN_my_gd']])\n",
    "    res_file.write(verse + '\\t' + out_string+'\\n')\n",
    "\n",
    "    orig_aligns = autils.get_aligns(editf12, editf22, blinker_verse_alignments_gdfa[verse])\n",
    "    out_string = ' '.join([f'{x[0]}-{x[1]}' for x in orig_aligns])\n",
    "    orig_file.write(verse + '\\t' + out_string+'\\n')\n",
    "\n",
    "res_file.close()\n",
    "orig_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (1, 5), (3, 3), (6, 10), (6, 11), (7, 7), (7, 9), (8, 8)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(res.keys())\n",
    "autils.get_aligns(editf1, editf2, heb_test_verse_alignments_gdfa[verse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge features size:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/55527 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending input to gpu\n",
      "model params - decoder params - conv1 237583457 1050625\n",
      "\n",
      "----------------epoch 1 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:20<00:00, 38.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.632, rec: 0.782, F1: 0.699, AER: 0.301\n",
      "argmax prec: 0.851, rec: 0.694, F1: 0.765, AER: 0.235\n",
      "resnorm prec: 0.488, rec: 0.803, F1: 0.607, AER: 0.393\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.815, rec: 0.726, F1: 0.768, AER: 0.232\n",
      "my_gd_gdfa prec: 0.719, rec: 0.787, F1: 0.751, AER: 0.249\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [00:47<00:00, 46.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.507, rec: 0.74, F1: 0.602, AER: 0.398\n",
      "argmax prec: 0.771, rec: 0.61, F1: 0.681, AER: 0.319\n",
      "resnorm prec: 0.432, rec: 0.761, F1: 0.551, AER: 0.449\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.749, rec: 0.647, F1: 0.694, AER: 0.306\n",
      "my_gd_gdfa prec: 0.598, rec: 0.692, F1: 0.642, AER: 0.359\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:08<00:00, 28.54it/s]\n",
      " 18%|█▊        | 10001/55527 [22:13<210:43:53, 16.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.766, rec: 0.742, F1: 0.754, AER: 0.246\n",
      "argmax prec: 0.933, rec: 0.669, F1: 0.779, AER: 0.218\n",
      "resnorm prec: 0.506, rec: 0.796, F1: 0.619, AER: 0.393\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.903, rec: 0.7, F1: 0.789, AER: 0.208\n",
      "my_gd_gdfa prec: 0.85, rec: 0.786, F1: 0.817, AER: 0.182\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:20<00:00, 39.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.646, rec: 0.796, F1: 0.713, AER: 0.287\n",
      "argmax prec: 0.847, rec: 0.697, F1: 0.765, AER: 0.235\n",
      "resnorm prec: 0.652, rec: 0.774, F1: 0.708, AER: 0.292\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.84, rec: 0.715, F1: 0.772, AER: 0.228\n",
      "my_gd_gdfa prec: 0.734, rec: 0.78, F1: 0.756, AER: 0.244\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [00:47<00:00, 47.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.516, rec: 0.755, F1: 0.613, AER: 0.387\n",
      "argmax prec: 0.782, rec: 0.612, F1: 0.687, AER: 0.313\n",
      "resnorm prec: 0.559, rec: 0.72, F1: 0.629, AER: 0.37\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.787, rec: 0.638, F1: 0.705, AER: 0.295\n",
      "my_gd_gdfa prec: 0.617, rec: 0.683, F1: 0.648, AER: 0.351\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:09<00:00, 27.38it/s]\n",
      " 36%|███▌      | 20000/55527 [44:11<250:18:13, 25.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.783, rec: 0.752, F1: 0.767, AER: 0.232\n",
      "argmax prec: 0.935, rec: 0.674, F1: 0.783, AER: 0.214\n",
      "resnorm prec: 0.679, rec: 0.758, F1: 0.716, AER: 0.285\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.923, rec: 0.697, F1: 0.794, AER: 0.203\n",
      "my_gd_gdfa prec: 0.865, rec: 0.786, F1: 0.824, AER: 0.175\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:20<00:00, 38.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.644, rec: 0.792, F1: 0.71, AER: 0.29\n",
      "argmax prec: 0.846, rec: 0.696, F1: 0.764, AER: 0.236\n",
      "resnorm prec: 0.621, rec: 0.782, F1: 0.692, AER: 0.308\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.825, rec: 0.723, F1: 0.771, AER: 0.229\n",
      "my_gd_gdfa prec: 0.726, rec: 0.784, F1: 0.754, AER: 0.246\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [00:47<00:00, 46.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.517, rec: 0.751, F1: 0.612, AER: 0.387\n",
      "argmax prec: 0.769, rec: 0.611, F1: 0.681, AER: 0.319\n",
      "resnorm prec: 0.508, rec: 0.742, F1: 0.603, AER: 0.397\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.754, rec: 0.648, F1: 0.697, AER: 0.303\n",
      "my_gd_gdfa prec: 0.602, rec: 0.692, F1: 0.644, AER: 0.356\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:09<00:00, 27.67it/s]\n",
      " 54%|█████▍    | 30001/55527 [1:05:59<116:55:53, 16.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.782, rec: 0.753, F1: 0.767, AER: 0.232\n",
      "argmax prec: 0.937, rec: 0.674, F1: 0.784, AER: 0.213\n",
      "resnorm prec: 0.636, rec: 0.771, F1: 0.697, AER: 0.307\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.916, rec: 0.704, F1: 0.796, AER: 0.201\n",
      "my_gd_gdfa prec: 0.86, rec: 0.788, F1: 0.822, AER: 0.176\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:20<00:00, 37.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.644, rec: 0.797, F1: 0.712, AER: 0.288\n",
      "argmax prec: 0.854, rec: 0.695, F1: 0.766, AER: 0.234\n",
      "resnorm prec: 0.503, rec: 0.808, F1: 0.62, AER: 0.38\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.816, rec: 0.733, F1: 0.772, AER: 0.228\n",
      "my_gd_gdfa prec: 0.719, rec: 0.788, F1: 0.752, AER: 0.248\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [00:48<00:00, 45.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.51, rec: 0.748, F1: 0.606, AER: 0.393\n",
      "argmax prec: 0.779, rec: 0.607, F1: 0.682, AER: 0.318\n",
      "resnorm prec: 0.405, rec: 0.779, F1: 0.533, AER: 0.467\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.745, rec: 0.658, F1: 0.699, AER: 0.301\n",
      "my_gd_gdfa prec: 0.598, rec: 0.696, F1: 0.643, AER: 0.357\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:09<00:00, 26.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.781, rec: 0.754, F1: 0.767, AER: 0.232\n",
      "argmax prec: 0.938, rec: 0.672, F1: 0.783, AER: 0.214\n",
      "resnorm prec: 0.549, rec: 0.797, F1: 0.65, AER: 0.359\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.909, rec: 0.713, F1: 0.799, AER: 0.198\n",
      "my_gd_gdfa prec: 0.855, rec: 0.793, F1: 0.823, AER: 0.176\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:20<00:00, 38.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.64, rec: 0.793, F1: 0.708, AER: 0.292\n",
      "argmax prec: 0.854, rec: 0.694, F1: 0.766, AER: 0.234\n",
      "resnorm prec: 0.58, rec: 0.786, F1: 0.667, AER: 0.332\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.833, rec: 0.726, F1: 0.776, AER: 0.224\n",
      "my_gd_gdfa prec: 0.732, rec: 0.783, F1: 0.757, AER: 0.243\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [00:47<00:00, 46.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.51, rec: 0.748, F1: 0.606, AER: 0.393\n",
      "argmax prec: 0.784, rec: 0.608, F1: 0.685, AER: 0.315\n",
      "resnorm prec: 0.448, rec: 0.755, F1: 0.562, AER: 0.437\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.766, rec: 0.654, F1: 0.706, AER: 0.294\n",
      "my_gd_gdfa prec: 0.609, rec: 0.694, F1: 0.649, AER: 0.351\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:09<00:00, 27.70it/s]\n",
      " 90%|█████████ | 50001/55527 [1:49:33<26:24:56, 17.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.779, rec: 0.752, F1: 0.765, AER: 0.234\n",
      "argmax prec: 0.941, rec: 0.673, F1: 0.785, AER: 0.212\n",
      "resnorm prec: 0.596, rec: 0.773, F1: 0.673, AER: 0.333\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.924, rec: 0.706, F1: 0.8, AER: 0.197\n",
      "my_gd_gdfa prec: 0.864, rec: 0.787, F1: 0.824, AER: 0.174\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55527/55527 [2:00:41<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 70410157.1758976\n",
      "cluster loss: 8984.427734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:20<00:00, 37.34it/s]\n",
      "  0%|          | 3/2225 [00:00<01:22, 26.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.641, rec: 0.792, F1: 0.709, AER: 0.291\n",
      "argmax prec: 0.852, rec: 0.693, F1: 0.764, AER: 0.235\n",
      "resnorm prec: 0.616, rec: 0.782, F1: 0.689, AER: 0.311\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.834, rec: 0.723, F1: 0.775, AER: 0.226\n",
      "my_gd_gdfa prec: 0.732, rec: 0.783, F1: 0.757, AER: 0.244\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [00:47<00:00, 46.84it/s]\n",
      "  2%|▏         | 4/250 [00:00<00:06, 38.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.512, rec: 0.749, F1: 0.608, AER: 0.392\n",
      "argmax prec: 0.783, rec: 0.612, F1: 0.687, AER: 0.313\n",
      "resnorm prec: 0.502, rec: 0.748, F1: 0.601, AER: 0.399\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.768, rec: 0.652, F1: 0.705, AER: 0.295\n",
      "my_gd_gdfa prec: 0.61, rec: 0.692, F1: 0.648, AER: 0.351\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:09<00:00, 27.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.782, rec: 0.751, F1: 0.766, AER: 0.233\n",
      "argmax prec: 0.935, rec: 0.673, F1: 0.783, AER: 0.215\n",
      "resnorm prec: 0.622, rec: 0.775, F1: 0.69, AER: 0.314\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.916, rec: 0.704, F1: 0.796, AER: 0.2\n",
      "my_gd_gdfa prec: 0.859, rec: 0.785, F1: 0.82, AER: 0.178\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/55527 [00:00<2:40:21,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------epoch 2 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:20<00:00, 38.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.634, rec: 0.784, F1: 0.701, AER: 0.299\n",
      "argmax prec: 0.851, rec: 0.689, F1: 0.761, AER: 0.238\n",
      "resnorm prec: 0.545, rec: 0.79, F1: 0.645, AER: 0.355\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.821, rec: 0.728, F1: 0.772, AER: 0.228\n",
      "my_gd_gdfa prec: 0.726, rec: 0.784, F1: 0.754, AER: 0.246\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [00:48<00:00, 46.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.505, rec: 0.739, F1: 0.6, AER: 0.4\n",
      "argmax prec: 0.779, rec: 0.607, F1: 0.682, AER: 0.318\n",
      "resnorm prec: 0.432, rec: 0.763, F1: 0.552, AER: 0.448\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.748, rec: 0.657, F1: 0.7, AER: 0.3\n",
      "my_gd_gdfa prec: 0.601, rec: 0.695, F1: 0.645, AER: 0.355\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:08<00:00, 27.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.78, rec: 0.75, F1: 0.765, AER: 0.235\n",
      "argmax prec: 0.94, rec: 0.674, F1: 0.785, AER: 0.212\n",
      "resnorm prec: 0.586, rec: 0.779, F1: 0.669, AER: 0.337\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.915, rec: 0.71, F1: 0.8, AER: 0.197\n",
      "my_gd_gdfa prec: 0.859, rec: 0.79, F1: 0.823, AER: 0.175\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:20<00:00, 38.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.636, rec: 0.788, F1: 0.704, AER: 0.296\n",
      "argmax prec: 0.853, rec: 0.691, F1: 0.764, AER: 0.236\n",
      "resnorm prec: 0.536, rec: 0.794, F1: 0.64, AER: 0.36\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.819, rec: 0.731, F1: 0.773, AER: 0.228\n",
      "my_gd_gdfa prec: 0.724, rec: 0.786, F1: 0.754, AER: 0.246\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [00:47<00:00, 46.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.503, rec: 0.739, F1: 0.599, AER: 0.401\n",
      "argmax prec: 0.781, rec: 0.605, F1: 0.682, AER: 0.318\n",
      "resnorm prec: 0.438, rec: 0.763, F1: 0.557, AER: 0.443\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.755, rec: 0.658, F1: 0.703, AER: 0.297\n",
      "my_gd_gdfa prec: 0.605, rec: 0.694, F1: 0.646, AER: 0.353\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:09<00:00, 26.64it/s]\n",
      " 36%|███▌      | 20001/55527 [42:50<175:58:37, 17.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.777, rec: 0.751, F1: 0.764, AER: 0.235\n",
      "argmax prec: 0.939, rec: 0.673, F1: 0.784, AER: 0.213\n",
      "resnorm prec: 0.588, rec: 0.78, F1: 0.671, AER: 0.336\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.917, rec: 0.709, F1: 0.8, AER: 0.197\n",
      "my_gd_gdfa prec: 0.86, rec: 0.787, F1: 0.822, AER: 0.176\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:20<00:00, 37.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.634, rec: 0.786, F1: 0.702, AER: 0.298\n",
      "argmax prec: 0.852, rec: 0.688, F1: 0.761, AER: 0.238\n",
      "resnorm prec: 0.585, rec: 0.781, F1: 0.669, AER: 0.331\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.826, rec: 0.727, F1: 0.773, AER: 0.227\n",
      "my_gd_gdfa prec: 0.729, rec: 0.783, F1: 0.755, AER: 0.245\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [00:48<00:00, 45.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.505, rec: 0.742, F1: 0.601, AER: 0.399\n",
      "argmax prec: 0.785, rec: 0.607, F1: 0.685, AER: 0.315\n",
      "resnorm prec: 0.475, rec: 0.749, F1: 0.581, AER: 0.419\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.764, rec: 0.656, F1: 0.706, AER: 0.294\n",
      "my_gd_gdfa prec: 0.61, rec: 0.693, F1: 0.649, AER: 0.351\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:09<00:00, 27.73it/s]\n",
      " 54%|█████▍    | 30001/55527 [1:04:21<113:52:21, 16.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.78, rec: 0.753, F1: 0.766, AER: 0.233\n",
      "argmax prec: 0.94, rec: 0.672, F1: 0.784, AER: 0.214\n",
      "resnorm prec: 0.631, rec: 0.769, F1: 0.693, AER: 0.311\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.92, rec: 0.706, F1: 0.799, AER: 0.198\n",
      "my_gd_gdfa prec: 0.862, rec: 0.786, F1: 0.822, AER: 0.176\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:20<00:00, 37.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.639, rec: 0.791, F1: 0.707, AER: 0.293\n",
      "argmax prec: 0.853, rec: 0.691, F1: 0.764, AER: 0.237\n",
      "resnorm prec: 0.564, rec: 0.794, F1: 0.66, AER: 0.341\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.817, rec: 0.734, F1: 0.773, AER: 0.227\n",
      "my_gd_gdfa prec: 0.725, rec: 0.788, F1: 0.755, AER: 0.245\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [00:49<00:00, 45.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.51, rec: 0.747, F1: 0.606, AER: 0.394\n",
      "argmax prec: 0.783, rec: 0.609, F1: 0.685, AER: 0.315\n",
      "resnorm prec: 0.468, rec: 0.761, F1: 0.58, AER: 0.42\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.753, rec: 0.665, F1: 0.706, AER: 0.294\n",
      "my_gd_gdfa prec: 0.605, rec: 0.7, F1: 0.649, AER: 0.351\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:09<00:00, 27.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.785, rec: 0.756, F1: 0.77, AER: 0.229\n",
      "argmax prec: 0.938, rec: 0.672, F1: 0.783, AER: 0.214\n",
      "resnorm prec: 0.629, rec: 0.777, F1: 0.695, AER: 0.309\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.917, rec: 0.71, F1: 0.8, AER: 0.197\n",
      "my_gd_gdfa prec: 0.86, rec: 0.788, F1: 0.822, AER: 0.176\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:20<00:00, 38.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.637, rec: 0.789, F1: 0.705, AER: 0.295\n",
      "argmax prec: 0.855, rec: 0.691, F1: 0.764, AER: 0.236\n",
      "resnorm prec: 0.564, rec: 0.79, F1: 0.658, AER: 0.342\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.825, rec: 0.73, F1: 0.775, AER: 0.226\n",
      "my_gd_gdfa prec: 0.729, rec: 0.786, F1: 0.756, AER: 0.244\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [00:48<00:00, 45.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.504, rec: 0.74, F1: 0.6, AER: 0.4\n",
      "argmax prec: 0.784, rec: 0.605, F1: 0.683, AER: 0.317\n",
      "resnorm prec: 0.459, rec: 0.755, F1: 0.571, AER: 0.429\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.759, rec: 0.656, F1: 0.704, AER: 0.296\n",
      "my_gd_gdfa prec: 0.607, rec: 0.694, F1: 0.648, AER: 0.353\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:09<00:00, 27.58it/s]\n",
      " 90%|█████████ | 50000/55527 [1:47:18<35:52:35, 23.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.781, rec: 0.755, F1: 0.768, AER: 0.232\n",
      "argmax prec: 0.942, rec: 0.673, F1: 0.785, AER: 0.212\n",
      "resnorm prec: 0.609, rec: 0.775, F1: 0.682, AER: 0.323\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.922, rec: 0.71, F1: 0.802, AER: 0.194\n",
      "my_gd_gdfa prec: 0.863, rec: 0.786, F1: 0.823, AER: 0.175\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55527/55527 [1:58:22<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 43956591.92072964\n",
      "cluster loss: 5718.51416015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:20<00:00, 39.00it/s]\n",
      "  0%|          | 4/2225 [00:00<01:06, 33.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.637, rec: 0.787, F1: 0.704, AER: 0.296\n",
      "argmax prec: 0.854, rec: 0.692, F1: 0.765, AER: 0.235\n",
      "resnorm prec: 0.614, rec: 0.777, F1: 0.686, AER: 0.314\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.827, rec: 0.728, F1: 0.774, AER: 0.226\n",
      "my_gd_gdfa prec: 0.73, rec: 0.784, F1: 0.756, AER: 0.244\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [00:47<00:00, 47.11it/s]\n",
      "  2%|▏         | 4/250 [00:00<00:06, 36.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.505, rec: 0.739, F1: 0.6, AER: 0.4\n",
      "argmax prec: 0.782, rec: 0.608, F1: 0.684, AER: 0.316\n",
      "resnorm prec: 0.518, rec: 0.732, F1: 0.607, AER: 0.393\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.763, rec: 0.653, F1: 0.704, AER: 0.296\n",
      "my_gd_gdfa prec: 0.608, rec: 0.69, F1: 0.646, AER: 0.353\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:08<00:00, 28.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.783, rec: 0.756, F1: 0.769, AER: 0.23\n",
      "argmax prec: 0.942, rec: 0.672, F1: 0.784, AER: 0.212\n",
      "resnorm prec: 0.679, rec: 0.76, F1: 0.717, AER: 0.285\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.924, rec: 0.704, F1: 0.799, AER: 0.198\n",
      "my_gd_gdfa prec: 0.865, rec: 0.785, F1: 0.823, AER: 0.176\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "features_edge = [] #TODO remove me\n",
    "#features_edge = train_dataset.features_edge[:]\n",
    "#x_edge_vals2 = x_edge_vals[:, :]\n",
    "#features = train_dataset.features\n",
    "gnn_dataset = gnn_dataset_train\n",
    "data_loader = DataLoader(gnn_dataset_train, batch_size=1, collate_fn=collate_fun, shuffle=True)\n",
    "\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "drop_out = 0\n",
    "pos_noise = 0.0\n",
    "neg_noise = 0.0\n",
    "n_head = 1\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "channels = 512\n",
    "\n",
    "in_dim = sum(t.out_dim for t in features)\n",
    "decoder_in_dim = n_head * channels * 2 + sum(t.out_dim for t in features_edge) \n",
    "print('edge features size: ', sum(t.out_dim for t in features_edge))\n",
    "#discriminator = Discriminator(channels*n_head, channels * (n_head+1), channels*n_head)\n",
    "#discriminator_optimizer = torch.optim.AdamW(discriminator.parameters(), lr=0.0007)\n",
    "#encoder2 = Encoder2(channels, int(channels/2)).to(dev)\n",
    "decoder = Decoder(decoder_in_dim, int(decoder_in_dim/2), features_edge, n_cluster=64)\n",
    "model = pyg_nn.GAE(Encoder(in_dim, channels, features, n_head, edge_feature_dim=len(features_edge)), decoder).to(dev)\n",
    "#model.encoder2 = encoder2\n",
    "#model = pyg_nn.GAE(DeeperGCN(in_dim, len(features_edge), channels, 10, features), decoder=decoder).to(dev)\n",
    "#model = pyg_nn.GAE(Encoder(in_dim, channels, features, n_head)).to(dev)\n",
    "\n",
    "print(\"sending input to gpu\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "optimizer.add_param_group({'params': word_vectors})\n",
    "\n",
    "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S-\") + f\"samett-{channels}chs-feat{train_dataset.num_node_features}-\")\n",
    "\n",
    "torch.set_printoptions(edgeitems=5)\n",
    "print(\"model params - decoder params - conv1\", sum(p.numel() for p in model.parameters()), sum(p.numel() for p in decoder.parameters()))\n",
    "\n",
    "for epoch in range(1, 3):\n",
    "    print(f\"\\n----------------epoch {epoch} ---------------\")\n",
    "    \n",
    "    #if epoch % 1 == 0:\n",
    "    #    train_neg_edge_index = gutils.get_negative_edges(train_verses, small_editions, train_dataset.nodes_map,  verse_alignments_inter).to(dev)\n",
    "        #edge_index_seq_sent_neg = get_negative_edges_seq(train_dataset.nodes_map).to(dev)\n",
    "\n",
    "    train(epoch)\n",
    "    save_model(model, f'epoch{epoch}')\n",
    "    clean_memory()\n",
    "    if epoch % 1 == 0:\n",
    "        #alignment_test(epoch, test_dataset.edge_index, editf1, editf2, test_verses[:30], test_nodes_map,\n",
    "        #    dev, model, x_test, pros, surs, verse_alignments_inter, verse_alignments_gdfa, writer, gnn_dataset.verse_info)\n",
    "        #eval_utils.alignment_test(epoch, test_dataset.edge_index, editf1, editf2, test_verses[:], test_nodes_map,\n",
    "        #    dev, model, x_test, pros, surs, verse_alignments_inter, verse_alignments_gdfa, writer, gnn_dataset.verse_info)\n",
    "\n",
    "        eval_utils.alignment_test(epoch, grc_test_dataset.edge_index, editf_fin, editf_grc, grc_test_verses[:], grc_test_dataset.nodes_map,\n",
    "                        dev, model, grc_test_dataset.x, pros_grc, surs_grc, grc_test_verse_alignments_inter, grc_test_verse_alignments_gdfa, writer, gnn_dataset_grc.verse_info)\n",
    "\n",
    "        eval_utils.alignment_test(epoch, heb_test_dataset.edge_index, editf_fin, editf_heb, heb_test_verses[:], heb_test_dataset.nodes_map,\n",
    "                                dev, model, heb_test_dataset.x, pros_heb, surs_heb, heb_test_verse_alignments_inter, heb_test_verse_alignments_gdfa, writer, gnn_dataset_heb.verse_info)\n",
    "\n",
    "        eval_utils.alignment_test(epoch, blinker_test_dataset.edge_index, editf12, editf22, blinker_verses, blinker_test_dataset.nodes_map,\n",
    "                    dev, model, blinker_test_dataset.x, pros_blinker, surs_blinker, blinker_verse_alignments_inter, blinker_verse_alignments_gdfa, writer, gnn_dataset_blinker.verse_info)\n",
    "        # auc, ap = test(edge_index_seq_sent, edge_index_seq_sent_neg, epoch)\n",
    "        # print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))\n",
    "    \n",
    "    clean_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-95c585d68432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkhar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgav\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sag' is not defined"
     ]
    }
   ],
   "source": [
    "i = sag\n",
    "batch = khar\n",
    "verse = gav\n",
    "print(i, verse)\n",
    "\n",
    "keys = list(gnn_dataset.verse_info.keys())\n",
    "\n",
    "gnn_dataset.verse_info[verse]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S-\") + f\"samett-chs-feat-\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7000/7000 [13:08<00:00,  8.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 5223590.213680267\n",
      "cluster loss: 700.0615844726562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:24<00:00, 31.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.645, rec: 0.794, F1: 0.712, AER: 0.288\n",
      "argmax prec: 0.873, rec: 0.68, F1: 0.765, AER: 0.236\n",
      "resnorm prec: 0.651, rec: 0.779, F1: 0.709, AER: 0.291\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.843, rec: 0.723, F1: 0.778, AER: 0.222\n",
      "my_gd_gdfa prec: 0.736, rec: 0.781, F1: 0.758, AER: 0.242\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1180/1180 [02:09<00:00,  9.15it/s]\n",
      "  1%|          | 3/250 [00:00<00:09, 26.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 801227.0793972015\n",
      "cluster loss: 104.71701049804688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:10<00:00, 24.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.79, rec: 0.76, F1: 0.775, AER: 0.224\n",
      "argmax prec: 0.952, rec: 0.657, F1: 0.777, AER: 0.219\n",
      "resnorm prec: 0.775, rec: 0.752, F1: 0.763, AER: 0.236\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.931, rec: 0.705, F1: 0.802, AER: 0.194\n",
      "my_gd_gdfa prec: 0.865, rec: 0.785, F1: 0.823, AER: 0.175\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6635/6635 [11:02<00:00, 10.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3412551.7662324905\n",
      "cluster loss: 497.5572509765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [00:56<00:00, 39.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.513, rec: 0.747, F1: 0.608, AER: 0.392\n",
      "argmax prec: 0.825, rec: 0.608, F1: 0.7, AER: 0.3\n",
      "resnorm prec: 0.529, rec: 0.747, F1: 0.619, AER: 0.381\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.788, rec: 0.661, F1: 0.719, AER: 0.281\n",
      "my_gd_gdfa prec: 0.615, rec: 0.693, F1: 0.652, AER: 0.348\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from gnn_utils import eval_utils\n",
    "importlib.reload(eval_utils)\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "data_loader_blinker = DataLoader(gnn_dataset_blinker, batch_size=1, collate_fn=collate_fun, shuffle=True)\n",
    "data_loader_heb = DataLoader(gnn_dataset_heb, batch_size=1, collate_fn=collate_fun, shuffle=True)\n",
    "data_loader_grc = DataLoader(gnn_dataset_grc, batch_size=1, collate_fn=collate_fun, shuffle=True)\n",
    "\n",
    "model = torch.load('/mounts/work/ayyoob/models/gnn/checkpoint/gnn_512_flggll_word_halfTrain_nofeatlinear_encoderlineear_decoderonelayer20210910-235352-.pickle').to(dev)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "data_loader = data_loader_grc\n",
    "gnn_dataset = gnn_dataset_grc\n",
    "train(1)\n",
    "clean_memory()\n",
    "eval_utils.alignment_test(1, grc_test_dataset.edge_index, editf_fin, editf_grc, grc_test_verses[:], grc_test_dataset.nodes_map,\n",
    "                        dev, model, grc_test_dataset.x, pros_grc, surs_grc, grc_test_verse_alignments_inter, grc_test_verse_alignments_gdfa, writer, gnn_dataset_grc.verse_info)\n",
    "clean_memory()\n",
    "\n",
    "data_loader = data_loader_blinker\n",
    "gnn_dataset = gnn_dataset_blinker\n",
    "model = torch.load('/mounts/work/ayyoob/models/gnn/checkpoint/gnn_512_flggll_word_halfTrain_nofeatlinear_encoderlineear_decoderonelayer20210910-235352-.pickle').to(dev)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "train(1)\n",
    "clean_memory()\n",
    "eval_utils.alignment_test(1, blinker_test_dataset.edge_index, editf12, editf22, blinker_verses[:], blinker_test_dataset.nodes_map,\n",
    "                    dev, model, blinker_test_dataset.x, pros_blinker, surs_blinker, blinker_verse_alignments_inter, blinker_verse_alignments_gdfa, writer, gnn_dataset_blinker.verse_info)\n",
    "clean_memory()\n",
    "\n",
    "model = torch.load('/mounts/work/ayyoob/models/gnn/checkpoint/gnn_512_flggll_word_halfTrain_nofeatlinear_encoderlineear_decoderonelayer_onlyheb_epoch220210912-043444-.pickle').to(dev)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "data_loader = data_loader_heb\n",
    "gnn_dataset = gnn_dataset_heb\n",
    "\n",
    "train(1)\n",
    "clean_memory()\n",
    "eval_utils.alignment_test(1, heb_test_dataset.edge_index, editf_fin, editf_heb, heb_test_verses[:], heb_test_dataset.nodes_map,\n",
    "                        dev, model, heb_test_dataset.x, pros_heb, surs_heb, heb_test_verse_alignments_inter, heb_test_verse_alignments_gdfa, writer, gnn_dataset_heb.verse_info)\n",
    "clean_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/mounts/work/ayyoob/models/gnn/checkpoint/gnn_512_flggll_word_halfTrain_nofeatlinear_encoderlineear_decoderonelayer20210910-235352-.pickle').to(dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [01:10<00:00, 31.72it/s]\n",
      "  0%|          | 3/783 [00:00<00:33, 23.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.512, rec: 0.744, F1: 0.607, AER: 0.394\n",
      "argmax prec: 0.814, rec: 0.6, F1: 0.691, AER: 0.309\n",
      "resnorm prec: 0.574, rec: 0.729, F1: 0.642, AER: 0.358\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.795, rec: 0.639, F1: 0.709, AER: 0.292\n",
      "my_gd_gdfa prec: 0.613, rec: 0.677, F1: 0.643, AER: 0.357\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:30<00:00, 25.89it/s]\n",
      "  1%|          | 3/250 [00:00<00:10, 24.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.634, rec: 0.783, F1: 0.701, AER: 0.299\n",
      "argmax prec: 0.869, rec: 0.675, F1: 0.76, AER: 0.24\n",
      "resnorm prec: 0.632, rec: 0.776, F1: 0.697, AER: 0.303\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.84, rec: 0.721, F1: 0.776, AER: 0.224\n",
      "my_gd_gdfa prec: 0.733, rec: 0.778, F1: 0.755, AER: 0.245\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:12<00:00, 20.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.783, rec: 0.755, F1: 0.769, AER: 0.231\n",
      "argmax prec: 0.954, rec: 0.655, F1: 0.777, AER: 0.221\n",
      "resnorm prec: 0.712, rec: 0.76, F1: 0.735, AER: 0.266\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.929, rec: 0.706, F1: 0.802, AER: 0.194\n",
      "my_gd_gdfa prec: 0.864, rec: 0.786, F1: 0.823, AER: 0.175\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    }
   ],
   "source": [
    "from gnn_utils import eval_utils\n",
    "importlib.reload(eval_utils)\n",
    "\n",
    "clean_memory()\n",
    "eval_utils.alignment_test(1, heb_test_dataset.edge_index, editf_fin, editf_heb, heb_test_verses, heb_test_dataset.nodes_map,\n",
    "                        dev, model, heb_test_dataset.x, pros_heb, surs_heb, heb_test_verse_alignments_inter, heb_test_verse_alignments_gdfa, writer, gnn_dataset_heb.verse_info)\n",
    "\n",
    "eval_utils.alignment_test(1, grc_test_dataset.edge_index, editf_fin, editf_grc, grc_test_verses, grc_test_dataset.nodes_map,\n",
    "                        dev, model, grc_test_dataset.x, pros_grc, surs_grc, grc_test_verse_alignments_inter, grc_test_verse_alignments_gdfa, writer, gnn_dataset_grc.verse_info)\n",
    "\n",
    "eval_utils.alignment_test(1, blinker_test_dataset.edge_index, editf12, editf22, blinker_verses, blinker_test_dataset.nodes_map,\n",
    "                    dev, model, blinker_test_dataset.x, pros_blinker, surs_blinker, blinker_verse_alignments_inter, blinker_verse_alignments_gdfa, writer, gnn_dataset_blinker.verse_info)\n",
    "\n",
    "clean_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [01:04<00:00, 34.52it/s]\n",
      "  0%|          | 3/783 [00:00<00:30, 25.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.512, rec: 0.744, F1: 0.607, AER: 0.394\n",
      "argmax prec: 0.814, rec: 0.6, F1: 0.691, AER: 0.309\n",
      "resnorm prec: 0.574, rec: 0.729, F1: 0.642, AER: 0.358\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.765, rec: 0.654, F1: 0.705, AER: 0.295\n",
      "my_gd_gdfa prec: 0.608, rec: 0.694, F1: 0.648, AER: 0.352\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:27<00:00, 28.19it/s]\n",
      "  1%|          | 3/250 [00:00<00:08, 28.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.634, rec: 0.783, F1: 0.701, AER: 0.299\n",
      "argmax prec: 0.869, rec: 0.675, F1: 0.76, AER: 0.24\n",
      "resnorm prec: 0.632, rec: 0.776, F1: 0.697, AER: 0.303\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.822, rec: 0.727, F1: 0.772, AER: 0.228\n",
      "my_gd_gdfa prec: 0.724, rec: 0.784, F1: 0.753, AER: 0.247\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:11<00:00, 22.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.783, rec: 0.755, F1: 0.769, AER: 0.231\n",
      "argmax prec: 0.954, rec: 0.655, F1: 0.777, AER: 0.221\n",
      "resnorm prec: 0.712, rec: 0.76, F1: 0.735, AER: 0.266\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.927, rec: 0.708, F1: 0.803, AER: 0.194\n",
      "my_gd_gdfa prec: 0.866, rec: 0.787, F1: 0.825, AER: 0.174\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from gnn_utils import eval_utils\n",
    "importlib.reload(eval_utils)\n",
    "\n",
    "clean_memory()\n",
    "eval_utils.alignment_test(1, heb_test_dataset.edge_index, editf_fin, editf_heb, heb_test_verses, heb_test_dataset.nodes_map,\n",
    "                        dev, model, heb_test_dataset.x, pros_heb, surs_heb, heb_test_verse_alignments_inter, heb_test_verse_alignments_gdfa, writer, gnn_dataset_heb.verse_info)\n",
    "\n",
    "eval_utils.alignment_test(1, grc_test_dataset.edge_index, editf_fin, editf_grc, grc_test_verses, grc_test_dataset.nodes_map,\n",
    "                        dev, model, grc_test_dataset.x, pros_grc, surs_grc, grc_test_verse_alignments_inter, grc_test_verse_alignments_gdfa, writer, gnn_dataset_grc.verse_info)\n",
    "\n",
    "eval_utils.alignment_test(1, blinker_test_dataset.edge_index, editf12, editf22, blinker_verses, blinker_test_dataset.nodes_map,\n",
    "                    dev, model, blinker_test_dataset.x, pros_blinker, surs_blinker, blinker_verse_alignments_inter, blinker_verse_alignments_gdfa, writer, gnn_dataset_blinker.verse_info)\n",
    "\n",
    "clean_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/6202 [00:00<06:39, 15.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align heb \n",
      "going to align train \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6202/6202 [05:18<00:00, 19.46it/s]\n",
      "  3%|▎         | 2/73 [00:00<00:04, 17.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align blinker \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:04<00:00, 18.16it/s]\n",
      "  0%|          | 2/783 [00:00<00:42, 18.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align grc \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:39<00:00, 20.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going save alignments for hin-x-bible-newworld\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/2168 [00:00<01:16, 28.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align heb \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2168/2168 [01:47<00:00, 20.10it/s]\n",
      "  0%|          | 1/23531 [00:00<44:58,  8.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align train \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23531/23531 [19:06<00:00, 20.52it/s]\n",
      "  0%|          | 1/243 [00:00<00:28,  8.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align blinker \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 243/243 [00:12<00:00, 19.35it/s]\n",
      "  0%|          | 2/782 [00:00<00:40, 19.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align grc \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:37<00:00, 20.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going save alignments for ita-x-bible-2009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/6203 [00:00<05:32, 18.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align heb \n",
      "going to align train \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6203/6203 [05:06<00:00, 20.22it/s]\n",
      "  3%|▎         | 2/73 [00:00<00:04, 16.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align blinker \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:03<00:00, 18.78it/s]\n",
      "  0%|          | 2/783 [00:00<00:40, 19.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align grc \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:37<00:00, 20.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going save alignments for prs-x-bible-goodnews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/6203 [00:00<05:42, 18.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align heb \n",
      "going to align train \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6203/6203 [05:12<00:00, 19.85it/s]\n",
      "  3%|▎         | 2/73 [00:00<00:04, 16.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align blinker \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:04<00:00, 18.00it/s]\n",
      "  0%|          | 2/783 [00:00<00:40, 19.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align grc \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:38<00:00, 20.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going save alignments for ron-x-bible-2006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/2172 [00:00<01:12, 29.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align heb \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2172/2172 [01:57<00:00, 18.43it/s]\n",
      "  0%|          | 1/23560 [00:00<45:56,  8.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align train \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23560/23560 [20:43<00:00, 18.94it/s]\n",
      "  0%|          | 1/246 [00:00<00:32,  7.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align blinker \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 246/246 [00:13<00:00, 17.61it/s]\n",
      "  0%|          | 2/783 [00:00<00:45, 17.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to align grc \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:40<00:00, 19.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going save alignments for spa-x-bible-newworld\n"
     ]
    }
   ],
   "source": [
    "# produce for uruba\n",
    "importlib.reload(eval_utils)\n",
    "editf_yor = 'yor-x-bible-2010'\n",
    "editf_others = ['eng-x-bible-mixed', 'deu-x-bible-newworld', 'ces-x-bible-newworld', 'fra-x-bible-louissegond', 'hin-x-bible-newworld',\n",
    "                'ita-x-bible-2009', 'prs-x-bible-goodnews', 'ron-x-bible-2006', 'spa-x-bible-newworld']\n",
    "\n",
    "#def get_pruned_verse_alignments(args):\n",
    "#    verse, current_editions = args\n",
    "    \n",
    "#    #verse_aligns_inter = autils.get_verse_alignments(verse)\n",
    "#    verse_aligns_gdfa = autils.get_verse_alignments(verse, gdfa=True)\n",
    "\n",
    "#    #autils.prune_non_necessary_alignments(verse_aligns_inter, current_editions)\n",
    "#    autils.prune_non_necessary_alignments(verse_aligns_gdfa, current_editions)\n",
    "\n",
    "#    gc.collect()\n",
    "#    return verse_aligns_gdfa\n",
    "    \n",
    "\n",
    "#verse_alignments_gdfa = {}\n",
    "#args = []\n",
    "#editfs = editf_others[:]\n",
    "#editfs.append(editf_yor)\n",
    "#for i,verse in enumerate(train_verses):\n",
    "#    args.append((verse, editfs))\n",
    "\n",
    "#print('going to get alignments')\n",
    "#with Pool(20) as p:\n",
    "#    all_res = p.map(get_pruned_verse_alignments, args)\n",
    "\n",
    "#for i,verse in enumerate(all_verses):\n",
    "#    verse_aligns_gdfa = all_res[i]\n",
    "    \n",
    "#    verse_alignments_gdfa[verse] = verse_aligns_gdfa\n",
    "\n",
    "for verse in train_dataset.nodes_map[editf_yor]:\n",
    "    if verse not in surs :\n",
    "        surs[verse] = set()\n",
    "        pros[verse] = set()\n",
    "\n",
    "#verse_alignments_gdfa = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_train_gdfa_yoruba.pickle\")\n",
    "\n",
    "for eidtf_t in editf_others:\n",
    "    res = {}\n",
    "\n",
    "    print('going to align heb ')\n",
    "    if eidtf_t in heb_test_dataset.nodes_map:\n",
    "        verses = set(heb_test_dataset.nodes_map[editf_yor].keys()).intersection(heb_test_dataset.nodes_map[eidtf_t].keys())\n",
    "        res_ = eval_utils.alignment_test(epoch, heb_test_dataset.edge_index, editf_yor, eidtf_t, list(verses), heb_test_dataset.nodes_map,\n",
    "                        dev, model, heb_test_dataset.x, pros_heb, surs_heb, heb_test_verse_alignments_inter, heb_test_verse_alignments_gdfa,\n",
    "                        writer, gnn_dataset_heb.verse_info, calc_numbers=False)\n",
    "        clean_memory()\n",
    "        res.update(res_)\n",
    "\n",
    "    print('going to align train ')\n",
    "    verses = set(train_dataset.nodes_map[editf_yor].keys()).intersection(train_dataset.nodes_map[eidtf_t].keys())\n",
    "    res_ = eval_utils.alignment_test(epoch, train_dataset.edge_index, editf_yor, eidtf_t, list(verses - set(masked_verses)), train_dataset.nodes_map,\n",
    "                    dev, model, train_dataset.x, pros, surs, verse_alignments_inter, verse_alignments_gdfa, writer, gnn_dataset_train.verse_info, calc_numbers=False)\n",
    "    clean_memory()\n",
    "    res.update(res_)\n",
    "\n",
    "    print('going to align blinker ')\n",
    "    verses = set(blinker_test_dataset.nodes_map[editf_yor].keys()).intersection(blinker_test_dataset.nodes_map[eidtf_t].keys())\n",
    "    res_ = eval_utils.alignment_test(epoch, blinker_test_dataset.edge_index, editf_yor, eidtf_t, list(verses), blinker_test_dataset.nodes_map,\n",
    "                    dev, model, blinker_test_dataset.x, pros_blinker, surs_blinker, blinker_verse_alignments_inter, blinker_verse_alignments_gdfa,\n",
    "                    writer, gnn_dataset_blinker.verse_info, calc_numbers=False)\n",
    "    clean_memory()\n",
    "    res.update(res_)\n",
    "\n",
    "    print('going to align grc ')\n",
    "    verses = set(grc_test_dataset.nodes_map[editf_yor].keys()).intersection(grc_test_dataset.nodes_map[eidtf_t].keys())\n",
    "    res_ = eval_utils.alignment_test(epoch, grc_test_dataset.edge_index, editf_yor, eidtf_t, list(verses), grc_test_dataset.nodes_map,\n",
    "                    dev, model, grc_test_dataset.x, pros_grc, surs_grc, grc_test_verse_alignments_inter, grc_test_verse_alignments_gdfa,\n",
    "                    writer, gnn_dataset_grc.verse_info, calc_numbers=False)\n",
    "    clean_memory()\n",
    "    res.update(res_)\n",
    "\n",
    "    \n",
    "    \n",
    "    print(f'going save alignments for {eidtf_t}')\n",
    "    torch.save(res, f'/mounts/work/ayyoob/results/gnn_align/yoruba/{eidtf_t}_alignments.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global model, decoder\n",
    "#1/0\n",
    "\n",
    "decoder = None\n",
    "model = None\n",
    "\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = blinker_test_dataset.features[:]\n",
    "#features_edge = train_dataset.features_edge[:]\n",
    "from pprint import pprint\n",
    "#print('indim',in_dim)\n",
    "#features[-1].out_dim = 50\n",
    "for i in features:\n",
    "    #if i.type==3:\n",
    "    #    i.out_dim=4\n",
    "    print(vars(i))\n",
    "\n",
    "sum(p.out_dim for p in features)\n",
    "#train_dataset.features.pop()\n",
    "#train_dataset.features[0] = afeatures.OneHotFeature(20, 83, 'editf')\n",
    "#train_dataset.features[1] = afeatures.OneHotFeature(32, 150, 'position')\n",
    "#train_dataset.features[2] = afeatures.FloatFeature(4, 'degree_centrality')\n",
    "#train_dataset.features[3] = afeatures.FloatFeature(4, 'closeness_centrality')\n",
    "#train_dataset.features[4] = afeatures.FloatFeature(4, 'betweenness_centrality')\n",
    "#train_dataset.features[5] = afeatures.FloatFeature(4, 'load_centrality')\n",
    "#train_dataset.features[6] = afeatures.FloatFeature(4, 'harmonic_centrality')\n",
    "#train_dataset.features[7] = afeatures.OneHotFeature(32, 250, 'greedy_modularity_community')\n",
    "##train_dataset.features.append(afeatures.MappingFeature(100, 'word'))\n",
    "#torch.save(train_dataset, \"/mounts/work/ayyoob/models/gnn/dataset_helfi_train_community_word.pickle\")\n",
    "#torch.save(train_dataset.features[-3], \"./features.tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of deleted edges by each community detection method\n",
    "# from networkx.algorithms.community import greedy_modularity_communities, asyn_lpa_communities, label_propagation_communities, asyn_fluidc\n",
    "\n",
    "# tmp_verses = [all_verses[2]]\n",
    "# tmp_editions = small_editions[:10]\n",
    "# tmp_dataset, tmp_nodes_map = create_dataset(tmp_verses, verse_alignments_inter, tmp_editions)\n",
    "\n",
    "# tmp_g = pyg_utils.convert.to_networkx(tmp_dataset, to_undirected=True)\n",
    "# def count_deleted_edges(tmp_dataset, c):\n",
    "#     deleted_edges = 0\n",
    "#     for i in range(0, len(tmp_dataset.edge_index[0]), 2):\n",
    "#         for comp in c:\n",
    "#             if tmp_dataset.edge_index[0][i].item() in comp and tmp_dataset.edge_index[1][i].item() not in comp:\n",
    "#                 deleted_edges += 1\n",
    "    \n",
    "#     return deleted_edges\n",
    "\n",
    "# print(\"eng token count: \", tmp_nodes_map['eng-x-bible-mixed'][tmp_verses[0]])\n",
    "# print(\"original connected components\",nx.number_connected_components(tmp_g))\n",
    "\n",
    "# c = list(greedy_modularity_communities(tmp_g))\n",
    "# print(\"new connected_components\", len(c))\n",
    "# print(\"deleted edges: \", count_deleted_edges(tmp_dataset, c))\n",
    "\n",
    "# c = list(asyn_lpa_communities(tmp_g))\n",
    "# print(\"asyn_lpa_communities number of components\", len(c))\n",
    "# print(\"deleted edges: \", count_deleted_edges(tmp_dataset, c))\n",
    "\n",
    "# c = list(label_propagation_communities(tmp_g))\n",
    "# print(\"label_propagation_communities number of components\", len(c))\n",
    "# print(\"deleted edges: \", count_deleted_edges(tmp_dataset, c))\n",
    "\n",
    "# cents = nx.edge_betweenness_centrality(tmp_g)\n",
    "# vals = sorted(list(cents.values()))\n",
    "# print(vals[0], vals[10], vals[100], vals[1000], vals[2000], vals[3000], vals[10000])\n",
    "# print(vals[-1], vals[-10], vals[-100], vals[-1000], vals[-2000], vals[-3000], vals[-10000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure different community detection algorithms\n",
    "# from networkx.algorithms.community import greedy_modularity_communities, asyn_lpa_communities, label_propagation_communities, asyn_fluidc\n",
    "\n",
    "# def remove_bad_community_edges(nodes_map, verses, edition_files, alignments):\n",
    "#     edges_tmp = [[],[]]\n",
    "#     res_edges = [[],[]]\n",
    "#     for verse in verses:\n",
    "#         utils.LOG.info(f\"extracting edge features for {verse}\")\n",
    "#         for i,editf1 in enumerate(edition_files):\n",
    "#             for j,editf2 in enumerate(edition_files[i+1:]):\n",
    "#                 aligns = autils.get_aligns(editf1, editf2, alignments[verse])\n",
    "#                 if aligns != None:\n",
    "#                     for align in aligns:\n",
    "#                         n1, node_count = node_nom(verse, editf1, align[0], 0, nodes_map, None, None)\n",
    "#                         n2, node_count = node_nom(verse, editf2, align[1], 0, nodes_map, None, None)\n",
    "#                         edges_tmp[0].extend([n1, n2])\n",
    "#                         edges_tmp[1].extend([n2, n1])\n",
    "\n",
    "#         gnx = convert_to_netx(edges_tmp)\n",
    "#         print('detecting communities')\n",
    "#         coms = greedy_modularity_communities(gnx)\n",
    "\n",
    "#         print('finding good edges')\n",
    "#         for i in range(0, len(edges_tmp[0]), 2):\n",
    "#             for c in coms:\n",
    "#                 if edges_tmp[0][i] in c and edges_tmp[0][i+1] in c:\n",
    "#                     res_edges[0].extend([edges_tmp[0][i], edges_tmp[0][i+1]])\n",
    "#                     res_edges[1].extend([edges_tmp[0][i+1], edges_tmp[0][i]])\n",
    "#         edges_tmp = [[],[]]\n",
    "#     print('to keep edges:', len(res_edges[0]))\n",
    "#     return torch.tensor(res_edges, dtype=torch.long)\n",
    "\n",
    "# # old_edge_index = train_dataset.edge_index\n",
    "# # new_edge_index = remove_bad_community_edges(train_dataset.nodes_map, train_verses, small_editions, verse_alignments_inter)\n",
    "# # train_dataset.edge_index = new_edge_index\n",
    "\n",
    "# # with open(\"./dataset_greedy_modularity_communities.pickle\", 'rb') as inf:\n",
    "# #     train_dataset = pickle.load(inf)\n",
    "\n",
    "# test_dataset = train_dataset\n",
    "\n",
    "# print('orig edge count', old_edge_index.shape)\n",
    "# print('new edge count', train_dataset.edge_index.shape)\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_map = train_dataset.nodes_map\n",
    "bad_edition_files = []\n",
    "for edit in nodes_map:\n",
    "    bad_count = 0\n",
    "    for verse in nodes_map[edit]:\n",
    "        if len(nodes_map[edit][verse].keys()) < 2:\n",
    "            bad_count += 1\n",
    "        if bad_count > 1:\n",
    "            bad_edition_files.append(edit)\n",
    "            break\n",
    "print(bad_edition_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_japanese_nodes = set()\n",
    "nodes_map = train_dataset.nodes_map\n",
    "\n",
    "for bad_editionf in bad_edition_files:\n",
    "    for verse in nodes_map[bad_editionf]:\n",
    "        for item in nodes_map[bad_editionf][verse].items():\n",
    "            all_japanese_nodes.add(item[1])\n",
    "\n",
    "print(\" all japansese nodes: \", len(all_japanese_nodes))\n",
    "edge_index = train_dataset.edge_index.to('cpu')\n",
    "remaining_edges_index = []\n",
    "for i in tqdm(range(0, edge_index.shape[1], 2)):\n",
    "    if edge_index[0, i].item() not in all_japanese_nodes and edge_index[0, i+1].item() not in all_japanese_nodes:\n",
    "        remaining_edges_index.extend([i, i+1])\n",
    "\n",
    "print('original total edges count', edge_index.shape)\n",
    "print('remaining edge count', len(remaining_edges_index))\n",
    "train_dataset.edge_index = edge_index[:, remaining_edges_index]\n",
    "train_dataset.edge_index.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import greedy_modularity_communities, asyn_lpa_communities, label_propagation_communities, asyn_fluidc\n",
    "\n",
    "def get_community_edges(c, verse):\n",
    "    res = []\n",
    "    for n1 in all_nodes_map[editf1][verse].items():\n",
    "        for n2 in all_nodes_map[editf2][verse].items():\n",
    "            for com in c:\n",
    "                if n1[1] in com and n2[1] in com:\n",
    "                    res.append((n1[0], n2[0]))\n",
    "    \n",
    "    return res\n",
    "\n",
    "def intersect(e1, e2):\n",
    "    res = set()\n",
    "    for item in e1:\n",
    "        if item in e2:\n",
    "            res.add(item)\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "measures = {}\n",
    "measures['intersection']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "measures['gdfa']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "measures['c1_all']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "measures['c1_inter']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "measures['c2_all']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "measures['c2_inter']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "measures['c3_all']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "measures['c3_inter']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "numbers = [100, 300, 600, 1000, 1300, 1600, 2000, 3000, 4000, 5000]\n",
    "for i in numbers:\n",
    "    measures[i] = {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "    measures[i+1] = {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "coms = {}\n",
    "for verse in test_verses:\n",
    "    inter_edges = autils.get_aligns(editf1, editf2, verse_alignments_inter[verse])\n",
    "\n",
    "    all_dataset, all_nodes_map = create_dataset([verse], verse_alignments_inter, small_editions)\n",
    "    print(\"converting\")\n",
    "    g = pyg_utils.convert.to_networkx(all_dataset, to_undirected=True)\n",
    "    print(\"detecting community 1\")\n",
    "    c1 = list(greedy_modularity_communities(g))\n",
    "    print(\"detecting community 2\")\n",
    "    c2 = list(asyn_lpa_communities(g))\n",
    "    print(\"detecting community 3\")\n",
    "    c3 = list(label_propagation_communities(g))\n",
    "\n",
    "    print(\"detecting community 4\")\n",
    "    edge_betweenness = [y[0] for y in sorted(nx.edge_betweenness_centrality(g).items(), key=lambda x: x[1], reverse=True)]\n",
    "    \n",
    "    print('orig communities', nx.number_connected_components(g))\n",
    "    prev_i = 0\n",
    "    for i in numbers:\n",
    "        for j in range(prev_i, i):\n",
    "            g.remove_edge(*edge_betweenness[j])\n",
    "        prev_i = i\n",
    "        com = list(nx.connected_components(g))\n",
    "        edges = get_community_edges(com, verse)\n",
    "        autils.calc_and_update_alignment_score(edges, pros[verse], surs[verse], measures[i])\n",
    "        autils.calc_and_update_alignment_score(intersect(edges, inter_edges), pros[verse], surs[verse], measures[i+1]) \n",
    "        print(f'communities {i}', nx.number_connected_components(g))\n",
    "\n",
    "    c1_edges = get_community_edges(c1, verse)\n",
    "    c2_edges = get_community_edges(c2, verse)\n",
    "    c3_edges = get_community_edges(c3, verse)\n",
    "    print('c1 communities', len(c1))\n",
    "    print('c2 communities', len(c2))\n",
    "    print('c3 communities', len(c3))\n",
    "\n",
    "\n",
    "\n",
    "    autils.calc_and_update_alignment_score(inter_edges, pros[verse], surs[verse], measures['intersection'])\n",
    "    autils.calc_and_update_alignment_score(autils.get_aligns(editf1, editf2, verse_alignments_gdfa[verse]), pros[verse], surs[verse], measures['gdfa'])\n",
    "\n",
    "    autils.calc_and_update_alignment_score(c1_edges, pros[verse], surs[verse], measures['c1_all'])\n",
    "    autils.calc_and_update_alignment_score(c2_edges, pros[verse], surs[verse], measures['c2_all'])\n",
    "    autils.calc_and_update_alignment_score(c3_edges, pros[verse], surs[verse], measures['c3_all'])\n",
    "\n",
    "    autils.calc_and_update_alignment_score(intersect(c1_edges, inter_edges), pros[verse], surs[verse], measures['c1_inter'])\n",
    "    autils.calc_and_update_alignment_score(intersect(c2_edges, inter_edges), pros[verse], surs[verse], measures['c2_inter'])\n",
    "    autils.calc_and_update_alignment_score(intersect(c3_edges, inter_edges), pros[verse], surs[verse], measures['c3_inter'])\n",
    "\n",
    "    for item in measures:\n",
    "        print(item, measures[item])\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e87ea973a27bc0df729a3cffeb7c69a8289035b60e563cd26286fedfe440ad07"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('multalign_graph': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
