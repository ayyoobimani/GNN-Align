{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('multalign_graph': conda)"
  },
  "interpreter": {
   "hash": "e87ea973a27bc0df729a3cffeb7c69a8289035b60e563cd26286fedfe440ad07"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch, sys\n",
    "sys.path.insert(0, '../')\n",
    "from my_utils import gpu_utils\n",
    "import importlib, gc\n",
    "from my_utils.alignment_features import *\n",
    "import my_utils.alignment_features as afeatures\n",
    "importlib.reload(afeatures)\n",
    "import gnn_utils.graph_utils as gutils"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/mounts/Users/student/ayyoob/anaconda3/envs/multalign_graph/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# !pip install torch-geometric\n",
    "# !pip install tensorboardX\n",
    "\n",
    "# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "# !unzip ngrok-stable-linux-amd64.zip\n",
    "\n",
    "#  print(torch.version.cuda)\n",
    "#  print(torch.__version__)    \n",
    "\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from my_utils import align_utils as autils, utils\n",
    "import argparse\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "\n",
    "# set random seed\n",
    "config_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc-ui-demo/config_pbc.ini\"\n",
    "utils.setup(config_file)\n",
    "\n",
    "params = argparse.Namespace()\n",
    "\n",
    "\n",
    "params.gold_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-grc-fin-gold-alignments_train.txt\"\n",
    "pros, surs = autils.load_gold(params.gold_file)\n",
    "all_verses = list(pros.keys())\n",
    "params.gold_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-heb-fin-gold-alignments_train.txt\"\n",
    "pros, surs = autils.load_gold(params.gold_file)\n",
    "all_verses.extend(list(pros.keys()))\n",
    "all_verses = list(set(all_verses))\n",
    "print(len(all_verses))\n",
    "\n",
    "params.editions_file =  \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi_lang_list.txt\"\n",
    "editions, langs = autils.load_simalign_editions(params.editions_file)\n",
    "current_editions = [editions[lang] for lang in langs]\n",
    "\n",
    "def get_pruned_verse_alignments(args):\n",
    "    verse, current_editions = args\n",
    "    \n",
    "    verse_aligns_inter = autils.get_verse_alignments(verse)\n",
    "    verse_aligns_gdfa = autils.get_verse_alignments(verse, gdfa=True)\n",
    "\n",
    "    autils.prune_non_necessary_alignments(verse_aligns_inter, current_editions)\n",
    "    autils.prune_non_necessary_alignments(verse_aligns_gdfa, current_editions)\n",
    "\n",
    "    gc.collect()\n",
    "    return verse_aligns_inter, verse_aligns_gdfa\n",
    "    \n",
    "\n",
    "verse_alignments_inter = {}\n",
    "verse_alignments_gdfa = {}\n",
    "args = []\n",
    "for i,verse in enumerate(all_verses):\n",
    "    args.append((verse, current_editions[:]))\n",
    "\n",
    "#print('going to get alignments')\n",
    "#with Pool(20) as p:\n",
    "#    all_res = p.map(get_pruned_verse_alignments, args)\n",
    "\n",
    "#for i,verse in enumerate(all_verses):\n",
    "#    verse_aligns_inter, verse_aligns_gdfa = all_res[i]\n",
    "    \n",
    "    #verse_alignments_inter[verse] = verse_aligns_inter\n",
    "    #verse_alignments_gdfa[verse] = verse_aligns_gdfa\n",
    "\n",
    "#utils.LOG.info(\"done reading alignments\")\n",
    "#torch.save(verse_alignments_inter, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_train_inter.pickle\")\n",
    "#torch.save(verse_alignments_gdfa, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_train_gdfa.pickle\")\n",
    "#utils.LOG.info('done saving pruned alignments')\n",
    "\n",
    "print('reading inter verse alignments')\n",
    "#verse_alignments_inter = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_train_inter_8000.pickle\")\n",
    "gc.collect()\n",
    "print('done reading inter verse alignments')\n",
    "\n",
    "#for verse in all_verses[:]:\n",
    "#    if len(verse_alignments_inter[verse].keys()) < 10:\n",
    "#        all_verses.remove(verse)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "24159\n",
      "reading inter verse alignments\n",
      "done reading inter verse alignments\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_channels, hidden_channels)\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.lin3(x)\n",
    "        return x\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class Encoder2(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Encoder2, self).__init__()\n",
    "        self.conv1 = pyg_nn.GATConv(in_channels, 2*out_channels)\n",
    "        self.conv2 = pyg_nn.GATConv(2 * out_channels , out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.elu(self.conv1(x, edge_index, ))\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features, n_head = 2, edge_feature_dim = 0,):\n",
    "        super(Encoder, self).__init__()\n",
    "        #self.lin = nn.Linear(in_channels, out_channels)\n",
    "        self.conv1 = pyg_nn.GATConv(in_channels, 2*out_channels, heads= n_head)\n",
    "        self.conv2 = pyg_nn.GATConv(2 * n_head *  out_channels , out_channels, heads= 1)\n",
    "        #self.conv3 = pyg_nn.GATConv(2 * n_head *  out_channels , out_channels, heads= n_head)\n",
    "        self.f_embedding = nn.Linear(in_channels, in_channels)\n",
    "        \n",
    "\n",
    "        self.feature_encoder = afeatures.FeatureEncoding(features, word_vectors)\n",
    "        #self.already_inited = False\n",
    "        #self.prev_edge_index = None\n",
    "        #self.prev_edge_attr = None\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.feature_encoder(x, dev)\n",
    "        x = F.relu(self.f_embedding(x))\n",
    "        #if not self.already_inited or self.prev_edge_index.data_ptr() != edge_index.data_ptr():\n",
    "        #    edge_index_np = edge_index.cpu().numpy()\n",
    "        #    val_indices = x_edge_np[edge_index_np[0, :], edge_index_np[1, :]]\n",
    "        #    vals = x_edge_vals[val_indices, :]\n",
    "        #    vals = vals.reshape((vals.shape[1], vals.shape[2]))\n",
    "        #    self.prev_edge_attr = vals.to(dev)\n",
    "        #    self.prev_edge_index = edge_index\n",
    "        #    self.already_inited = True\n",
    "        #x = self.lin(x)\n",
    "        x = F.elu(self.conv1(x, edge_index, ))\n",
    "        #x = self.conv_gin(x, edge_index)\n",
    "        #x = F.elu(self.conv2(x, edge_index))\n",
    "        return self.conv2(x, edge_index)#, self.conv3(x, edge_index)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def clean_memory():\n",
    "    gc.collect()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def train(epoch):\n",
    "    global optimizer\n",
    "    total_loss = 0\n",
    "    cluster_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    #for i in tqdm(range(int(train_pos_edge_index_permed.shape[1]/batch_size)+1)):\n",
    "    for i,batch_ in enumerate(tqdm(data_loader)):\n",
    "        for verse in batch_:\n",
    "            if verse in masked_verses:\n",
    "                continue\n",
    "            batch = batch_[verse]\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            x = batch['x'].to(dev)\n",
    "            edge_index = batch['edge_index'].to(dev)\n",
    "            if torch.max(edge_index) >= x.shape[0]:\n",
    "                print(torch.max(edge_index), x.shape)\n",
    "                print(batch)\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                z = model.encode(x, edge_index)\n",
    "            except Exception as e:\n",
    "                global sag, khar, gav\n",
    "                sag, khar, gav =  (i, batch_, verse)\n",
    "                print(e)\n",
    "                1/0\n",
    "            #z1 = encoder2(z, torch.tensor(batch['intra_sent_edges'], dtype=torch.long).to(dev))\n",
    "            #z = torch.cat((z,z1), dim=1)\n",
    "            #for j in range(5):\n",
    "            #    discriminator_optimizer.zero_grad()\n",
    "            #    discriminator_loss = model.discriminator_loss(z) / (int(train_pos_edge_index_permed.shape[1]/batch_size)+1)\n",
    "            #    discriminator_loss.backward()\n",
    "            #    discriminator_optimizer.step()\n",
    "            \n",
    "            pos = torch.tensor(batch['pos'], dtype=torch.long).to(dev)\n",
    "            neg = torch.tensor(batch['neg'], dtype=torch.long).to(dev)\n",
    "            #nodes = torch.tensor(list(batch['nodes']), dtype=torch.long).to(dev)\n",
    "\n",
    "            loss1 = model.recon_loss( z, pos, neg) #TODO try providing better neg edges\n",
    "            #ortho_loss, mincut_loss, entropy_loss = model.decoder.clustering_loss(z, nodes, batch['adjacency'])\n",
    "            \n",
    "            loss =   loss1 * pos.shape[1] #+ ortho_loss + mincut_loss #+ 0.05 * entropy_loss #* pos.shape[1]/train_neg_edge_index.shape[1] #+ model.reg_loss(z)/(int(train_pos_edge_index_permed.shape[1]/batch_size)+1)# + (1 / x.shape[0]) * model.kl_loss()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() \n",
    "            cluster_loss += loss1\n",
    "\n",
    "            \n",
    "            #if i  % 3000 == 2999:\n",
    "            #    #alignment_test(epoch, test_dataset.edge_index, editf1, editf2, test_verses, test_nodes_map,\n",
    "            #    #        dev, model, x_test, pros, surs, verse_alignments_inter, verse_alignments_gdfa, writer, gnn_dataset.verse_info)\n",
    "\n",
    "            #    clean_memory()\n",
    "                \n",
    "            #    eval_utils.alignment_test(epoch, grc_test_dataset.edge_index, editf_fin, editf_grc, grc_test_verses[:], grc_test_dataset.nodes_map,\n",
    "            #            dev, model, grc_test_dataset.x, pros_grc, surs_grc, grc_test_verse_alignments_inter, grc_test_verse_alignments_gdfa, writer, gnn_dataset_grc.verse_info)\n",
    "\n",
    "            #    eval_utils.alignment_test(epoch, heb_test_dataset.edge_index, editf_fin, editf_heb, heb_test_verses[:], heb_test_dataset.nodes_map,\n",
    "            #                            dev, model, heb_test_dataset.x, pros_heb, surs_heb, heb_test_verse_alignments_inter, heb_test_verse_alignments_gdfa, writer, gnn_dataset_heb.verse_info)\n",
    "\n",
    "            #    eval_utils.alignment_test(epoch, blinker_test_dataset.edge_index, editf12, editf22, blinker_verses, blinker_test_dataset.nodes_map,\n",
    "            #                dev, model, blinker_test_dataset.x, pros_blinker, surs_blinker, blinker_verse_alignments_inter, blinker_verse_alignments_gdfa, writer, gnn_dataset_blinker.verse_info)\n",
    "                \n",
    "            #    clean_memory()   \n",
    "            #    # decoder.set_objective('sequence_prediction')\n",
    "            #    # auc, ap = test(edge_index_seq_sent, edge_index_seq_sent_neg, epoch)\n",
    "            #    # print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))\n",
    "            #    # if epoch > 4:\n",
    "            #    #     decoder.set_objective('link_prediction')\n",
    "            #    model.train()\n",
    "            \n",
    "            #if (i+1)*batch_size > train_pos_edge_index.shape[1]:\n",
    "            #    break\n",
    "\n",
    "            #if i % 51 == 0:\n",
    "            #    clean_memory()\n",
    "    \n",
    "    writer.add_scalar(\"loss\", total_loss, epoch)\n",
    "    print(f\"train loss: {total_loss}\")\n",
    "    print(f\"cluster loss: {cluster_loss}\")\n",
    "\n",
    "def test(pos_edge_index, neg_edge_index, epoch):\n",
    "    model.eval()\n",
    "    tot_auc = tot_ap = 0\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(x_test, torch.cat((train_pos_edge_index, neg_edge_index), dim=1).to(dev))\n",
    "        \n",
    "        neg_pos_coeff = neg_edge_index.shape[1]/ pos_edge_index.shape[1]\n",
    "        for i in (range(int(pos_edge_index.shape[1]/batch_size)+1)):\n",
    "            auc,ap = model.test(z, pos_edge_index[:, i*batch_size:(i+1)*batch_size].to(dev),\n",
    "                                neg_edge_index[:, int(i*batch_size*neg_pos_coeff):int((i+1)*batch_size*neg_pos_coeff)].to(dev))\n",
    "\n",
    "            tot_auc += auc * pos_edge_index[:, i*batch_size:(i+1)*batch_size].shape[1]\n",
    "            tot_ap += ap *  pos_edge_index[:, i*batch_size:(i+1)*batch_size].shape[1]\n",
    "\n",
    "\n",
    "    return tot_auc/pos_edge_index.shape[1], tot_ap/pos_edge_index.shape[1]\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "EPS = 1e-15\n",
    "\n",
    "def _diag(x):\n",
    "    eye = torch.eye(x.size(0)).type_as(x)\n",
    "    out = eye * x.unsqueeze(1).expand(x.size(0), x.size(0))\n",
    "    return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, edge_features, n_cluster=32):\n",
    "        super(Decoder, self).__init__()\n",
    "        #self.feature_encoder = afeatures.FeatureEncoding(edge_features)\n",
    "        self.features_size = sum([x.out_dim for x in edge_features])\n",
    "        self.representataion_size = (input_size - self.features_size)\n",
    "\n",
    "        self.transfer = nn.Sequential(nn.Linear(input_size, hidden_size*2), nn.ReLU(), nn.Dropout(drop_out),\n",
    "                            nn.Linear(hidden_size*2, hidden_size), nn.ReLU(), nn.Dropout(drop_out),\n",
    "                            nn.Linear(hidden_size, 1))\n",
    "\n",
    "        #self.transfer = nn.Sequential(nn.ELU(), nn.Linear(n_cluster*2, 1), nn.ELU())\n",
    "\n",
    "        #self.n_cluster = n_cluster                \n",
    "        #self.cluster = nn.Sequential(nn.Linear(int((input_size - len(edge_features))/2), hidden_size*2), nn.ELU(), nn.Linear(hidden_size*2, 2*n_cluster))\n",
    "        #self.actual_cluster = nn.Linear(2*n_cluster, n_cluster)\n",
    "        #self.cos = nn.CosineSimilarity(dim=1)        \n",
    "        #self.dist = nn.PairwiseDistance()\n",
    "        #self.gnn_transform = nn.Sequential(nn.Linear(self.representataion_size, hidden_size), nn.ReLU(), nn.Dropout(drop_out))\n",
    "        self.counter = 0\n",
    "\n",
    "        self.objective = 'link_prediction'\n",
    "    def forward(self, z, edge_index, sigmoid = True):\n",
    "        if self.features_size > 0:\n",
    "            if self.objective == 'link_prediction':\n",
    "                edge_index_np = edge_index.cpu().numpy()\n",
    "                val_indices = x_edge_np[edge_index_np[0, :], edge_index_np[1, :]]\n",
    "                val_indices = np.squeeze(np.asarray(val_indices))\n",
    "                vals = x_edge_vals2[val_indices, :]\n",
    "            elif self.objective == 'sequence_prediction':\n",
    "                vals = torch.zeros((edge_index.shape[1], self.features_size)).to(dev)\n",
    "\n",
    "\n",
    "            features = self.feature_encoder(vals.to(dev), dev)\n",
    "            #features = vals.to(dev)\n",
    "            h1 = z[edge_index[0, :]]\n",
    "            h2 = z[edge_index[1, :]]\n",
    "\n",
    "            self.counter += 1\n",
    "\n",
    "            #rep = self.gnn_transform(torch.cat((h1, h2), dim=1))\n",
    "            res = self.transfer(torch.cat((self.cluster(h1), self.cluster(h2), features), dim=1))\n",
    "            #res = self.transfer(features)\n",
    "        else:\n",
    "            h1 = z[edge_index[0, :]]\n",
    "            h2 = z[edge_index[1, :]]\n",
    "\n",
    "            \n",
    "            res = self.transfer(torch.cat((h1, h2), dim=-1))\n",
    "\n",
    "            #res = self.transfer(torch.cat((self.cluster(h1), self.cluster(h2)), dim=1))\n",
    "            #res = torch.sum(torch.pow(F.softmax(self.cluster(h1)/1, dim=1) - F.softmax(self.cluster(h2)/1, dim=1), 2), dim=1)\n",
    "            #res = self.cos(self.cluster(h1), self.cluster(h2))\n",
    "            #res = - self.dist(self.cluster(h1), self.cluster(h2))\n",
    "            #print(res)\n",
    "        res = torch.sigmoid(res) if sigmoid else res\n",
    "        return res\n",
    "\n",
    "    def set_objective(self, objective):\n",
    "        self.objective = objective\n",
    "        \n",
    "    def clustering_loss(self, z, nodes, adjacency):\n",
    "        s = self.actual_cluster(torch.relu(self.cluster(z[nodes])))\n",
    "        s = torch.softmax(s, dim=-1)\n",
    "        entropy_loss = (-s * torch.log(s + EPS)).sum(dim=-1).mean()\n",
    "\n",
    "        ss = torch.matmul(s.transpose(0, 1), s)\n",
    "        i_s = torch.eye(self.n_cluster).type_as(ss)\n",
    "        ortho_loss = torch.norm(\n",
    "            ss / torch.norm(ss, dim=(-1, -2), keepdim=True) -\n",
    "            i_s / torch.norm(i_s), dim=(-1, -2))\n",
    "        ortho_loss = torch.mean(ortho_loss)\n",
    "\n",
    "        adjacency = adjacency.to(dev).float()\n",
    "        out_adj = torch.matmul(s.transpose(0, 1),torch.sparse.mm(adjacency, s))\n",
    "        # MinCUT regularization.\n",
    "        mincut_num = torch.trace(out_adj)\n",
    "        #d_flat = torch.einsum('ij->i', adjacency) # FIXME since I don't consider the whole adjacency matrix this could be a source of problem\n",
    "        d_flat = torch.sparse.sum(adjacency, dim=1).to_dense()\n",
    "        d = _diag(d_flat)\n",
    "        mincut_den = torch.trace(\n",
    "            torch.matmul(torch.matmul(s.transpose(0, 1), d), s))\n",
    "        mincut_loss = -(mincut_num / mincut_den)\n",
    "        mincut_loss = torch.mean(mincut_loss)\n",
    "\n",
    "        return ortho_loss, mincut_loss, entropy_loss\n",
    "    \n",
    "    def get_alignments(self, z, edge_index):\n",
    "        h1 = z[edge_index[0, :]]\n",
    "        h2 = z[edge_index[1, :]]\n",
    "        \n",
    "        h1 = torch.softmax(self.cluster(h1), dim=1)\n",
    "        h2 = torch.softmax(self.cluster(h2), dim=1)\n",
    "\n",
    "        h1_max = torch.argmax(h1, dim=1)\n",
    "        h2_max = torch.argmax(h2, dim=1)\n",
    "\n",
    "        h1_cluster = torch.zeros(*h1.shape)\n",
    "        h2_cluster = torch.zeros(*h2.shape)\n",
    "\n",
    "        h1_cluster[range(h1.size(0)), h1_max] = 1\n",
    "        h2_cluster[range(h2.size(0)), h2_max] = 1\n",
    "        res = torch.max(h1_cluster * h2_cluster, dim=1).values\n",
    "\n",
    "        #res = h1 * h2\n",
    "        #res = torch.sum(res, dim = 1)\n",
    "        return torch.unsqueeze(res, dim=1)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-722b3905bcf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m#res = torch.sum(res, dim = 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pickle\n",
    "\n",
    "train_verses = all_verses[:]\n",
    "test_verses = all_verses[:] \n",
    "editf1 = 'fin-x-bible-helfi'\n",
    "editf2 = \"heb-x-bible-helfi\"\n",
    "\n",
    "\n",
    "if 'jpn-x-bible-newworld' in  current_editions[:]:\n",
    "     current_editions.remove('jpn-x-bible-newworld')\n",
    "if 'grc-x-bible-unaccented' in  current_editions[:]:\n",
    "     current_editions.remove('grc-x-bible-unaccented')\n",
    "def get_train_dataset():\n",
    "     return torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_helfi_train_community_word_8000.pickle\", map_location=torch.device('cpu'))\n",
    "#train_dataset = get_train_dataset()\n",
    "#train_dataset, train_nodes_map = create_dataset(train_verses, verse_alignments_inter, small_editions)\n",
    "features = train_dataset.features\n",
    "train_nodes_map = train_dataset.nodes_map\n",
    "#edge_index_intra_sent = train_dataset.edge_index_intra_sent\n",
    "#test_edge_index_intra_sent = edge_index_intra_sent\n",
    "\n",
    "# test_dataset, test_nodes_map = create_dataset(test_verses, verse_alignments_inter, small_editions)\n",
    "test_dataset, test_nodes_map = train_dataset, train_nodes_map\n",
    "test_verses = train_verses\n",
    "print(train_dataset.x.shape)\n",
    "\n",
    "# gutils.augment_features(test_dataset)\n",
    "# x_edge, features_edge = gutils.create_edge_attribs(train_nodes_map, train_verses, small_editions, verse_alignments_inter, train_dataset.x.shape[0])\n",
    "# with open(\"./dataset.pickle\", 'wb') as of:\n",
    "#     pickle.dump(train_dataset, of)\n",
    "gc.collect()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10331043, 10])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec.load(\"/mounts/work/ayyoob/models/w2v/word2vec_helfi_langs_15e.model\")\n",
    "\n",
    "print(w2v_model.wv.vectors.shape)\n",
    "\n",
    "word_vectors = torch.from_numpy(w2v_model.wv.vectors).float()\n",
    "\n",
    "print(word_vectors.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2354770, 100)\n",
      "torch.Size([2354770, 100])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# edges_intra_sent, edges_seq_sent = get_inter_sentence_connections(train_dataset.nodes_map)\n",
    "# edge_index_seq_sent = torch.tensor(edges_seq_sent, dtype=torch.long)\n",
    "# train_dataset.edge_index_seq_sent = edge_index_seq_sent\n",
    "# torch.cuda.set_device(int(free_gpu1))\n",
    "# edge_index_intra_sent = torch.tensor(edges_intra_sent, dtype=torch.long).to(dev)\n",
    "# train_dataset.edge_index_intra_sent = edge_index_intra_sent\n",
    "# test_edge_index_intra_sent = train_dataset.edge_index_intra_sent\n",
    "# print(train_dataset.edge_index_intra_sent.shape)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pickle\n",
    "import torch\n",
    "## with open(\"./features_edge.pickle\", 'wb') as of:\n",
    "##     pickle.dump(features_edge, of)\n",
    "\n",
    "## print('done first')\n",
    "## with open(\"/mounts/work/ayyoob/models/gnn//x_edge.pickle\", 'wb') as of:\n",
    "##     pickle.dump(x_edge, of)\n",
    "\n",
    "#with open(\"./features_edge.pickle\", 'rb') as inf:\n",
    "#    features_edge = pickle.load(inf)\n",
    "\n",
    "\n",
    "## indices = [[],[]]\n",
    "## values = []\n",
    "\n",
    "## print('going to create sparse matrix representation')\n",
    "## for i in range(len(ss)):\n",
    "##     print(i)\n",
    "##     for j in range(len(ss)):\n",
    "##         if ss[i][j] != None and ss[i][j] != []:\n",
    "##             indices[0].append(i)\n",
    "##             indices[1].append(j)\n",
    "##             values.append(ss[i][j])\n",
    "\n",
    "\n",
    "## with open(\"./edge_attribs_sparse_indices.pickle\", 'wb') as of:\n",
    "##     pickle.dump(indices, of)\n",
    "\n",
    "## with open(\"./edge_attribs_sparse_values.pickle\", 'wb') as of:\n",
    "##     pickle.dump(values, of)\n",
    "\n",
    "## print('loading indices')\n",
    "## with open(\"./edge_attribs_sparse_indices.pickle\", 'rb') as inf:\n",
    "##     indices = pickle.load(inf)\n",
    "\n",
    "## print('loading values')\n",
    "## with open(\"./edge_attribs_sparse_values.pickle\", 'rb') as inf:\n",
    "##     values = pickle.load(inf)\n",
    "\n",
    "## print('creating sparse tensor')\n",
    "## s = torch.sparse_coo_tensor(indices, values, (67800, 67800, len(ff)), dtype=torch.float16)\n",
    "## print('saving sparse matrix')\n",
    "## torch.save(s, \"/mounts/work/ayyoob/models/gnn/edge_attribs_tensor16.pickle\")\n",
    "\n",
    "#print('loading sparse matrix')\n",
    "#x_edge = torch.load(\"/mounts/work/ayyoob/models/gnn/edge_attribs_tensor.pickle\")\n",
    "\n",
    "\n",
    "#train_dataset.features_edge = features_edge\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "#x_edge = x_edge.coalesce()\n",
    "#torch.cuda.set_device(1)\n",
    "#x_edge_vals = x_edge.values()\n",
    "#indices_np = x_edge.indices().numpy()\n",
    "#print(indices_np.shape)\n",
    "#x_edge_np = csr_matrix((np.arange(indices_np.shape[1]), (indices_np[0, :], indices_np[1,:])), shape=(67800, 67800))\n",
    "\n",
    "##x_edge_vals = x_edge_vals.cpu()\n",
    "##maxes = torch.max(x_edge_vals,0)\n",
    "##mins = torch.min(x_edge_vals,0)\n",
    "##x_edge_vals_d = torch.div(x_edge_vals, maxes.values)\n",
    "\n",
    "#print('creating targets')\n",
    "#targets = torch.zeros(indices_np.shape[1], dtype=torch.int64)\n",
    "#pos_indices = x_edge_np[train_dataset.edge_index.cpu().numpy()[0,:], train_dataset.edge_index.cpu().numpy()[1,:]]\n",
    "#pos_indices = np.squeeze(np.asarray(pos_indices))\n",
    "#targets[pos_indices] = 1\n",
    "#print(\"done\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    " # run on delta, extract w2v features\n",
    "#sys.path.insert(0, '../')\n",
    "#import pickle\n",
    "#from gensim.models import Word2Vec\n",
    "#from app.document_retrieval import DocumentRetriever\n",
    "#from my_utils import utils\n",
    "#config_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc-ui-demo/config_pbc.ini\"\n",
    "#utils.setup(config_file)\n",
    "#import torch\n",
    "#import my_utils.alignment_features as feat_utils\n",
    "\n",
    "#doc_retriever = DocumentRetriever()\n",
    "\n",
    "#model_w2v = Word2Vec.load(\"word2vec_83langs_15epoch.model\")\n",
    "#train_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_en_fr_full.pickle\")\n",
    "#nodes_map = train_dataset.nodes_map\n",
    "\n",
    "#x = [[] for i in range(train_dataset.x.shape[0])]\n",
    "#for edition_f in nodes_map:\n",
    "#    utils.LOG.info(f\"processing edition {edition_f}\")\n",
    "#    for verse in nodes_map[edition_f]:         #toknom nodecount\n",
    "#        line = doc_retriever.retrieve_document(f'{verse}@{edition_f}')\n",
    "#        line = line.strip().split()\n",
    "\n",
    "#        for tok in nodes_map[edition_f][verse]:\n",
    "#            w_emb = model_w2v.wv[f'{edition_f[:3]}:{line[tok]}']\n",
    "#            x[nodes_map[edition_f][verse][tok]].extend(w_emb)\n",
    "\n",
    "#x = torch.tensor(x, dtype=torch.float)\n",
    "#train_dataset.x = torch.cat((train_dataset.x, x), dim=1)\n",
    "#train_dataset.features.append(feat_utils.ForwardFeature(50, 100, 'W2v'))\n",
    "\n",
    "#print(x.shape, train_dataset.x.shape, len(train_dataset.features))\n",
    "\n",
    "#torch.save(train_dataset, \"/mounts/work/ayyoob/models/gnn/dataset_en_fr_full.pickle\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Add node embedding features\n",
    "#importlib.reload(gutils)\n",
    "#x_,features_ = gutils.get_embedding_node_features(train_dataset.nodes_map, train_verses, small_editions, verse_alignments_inter, x_edge_np, x_edge_vals.cpu().numpy())\n",
    "#train_dataset.x = torch.cat((train_dataset.x,x_), dim=1)\n",
    "#train_dataset.features.extend(features_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def get_blinker_dataset():\n",
    "    return torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_blinker_full_community_word.pickle\", map_location=torch.device('cpu'))\n",
    "    \n",
    "blinker_test_dataset = get_blinker_dataset()\n",
    "editf12 = \"eng-x-bible-mixed\"\n",
    "editf22 = 'fra-x-bible-louissegond'\n",
    "\n",
    "test_gold_eng_fra = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/eng_fra_pbc/eng-fra.gold\"\n",
    "\n",
    "pros_blinker, surs_blinker = autils.load_gold(test_gold_eng_fra)\n",
    "blinker_verses = list(pros_blinker.keys())\n",
    "\n",
    "#blinker_verse_alignments_inter = {}\n",
    "#blinker_verse_alignments_gdfa = {}\n",
    "#args = []\n",
    "#for i,verse in enumerate(blinker_verses):\n",
    "#    args.append((verse, current_editions))\n",
    "\n",
    "#with Pool(20) as p:\n",
    "#    all_res = p.map(get_pruned_verse_alignments, args)\n",
    "\n",
    "#for i,verse in enumerate(blinker_verses):\n",
    "#    verse_aligns_inter, verse_aligns_gdfa = all_res[i]\n",
    "    \n",
    "#    blinker_verse_alignments_inter[verse] = verse_aligns_inter\n",
    "#    blinker_verse_alignments_gdfa[verse] = verse_aligns_gdfa\n",
    "\n",
    "utils.LOG.info(\"done reading alignments\")\n",
    "#torch.save(blinker_verse_alignments_inter, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_inter.pickle\")\n",
    "#torch.save(blinker_verse_alignments_gdfa, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_gdfa.pickle\")\n",
    "utils.LOG.info('done saving pruned alignments')\n",
    "\n",
    "print('reading inter verse alignments')\n",
    "blinker_verse_alignments_inter = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_inter.pickle\")\n",
    "blinker_verse_alignments_gdfa = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_gdfa.pickle\")\n",
    "gc.collect()\n",
    "print('done reading inter verse alignments')\n",
    "\n",
    "verses_map = {}\n",
    "\n",
    "for edit in blinker_test_dataset.nodes_map:\n",
    "    for verse in blinker_test_dataset.nodes_map[edit]:\n",
    "        if verse not in verses_map:\n",
    "            for tok in blinker_test_dataset.nodes_map[edit][verse]:\n",
    "                verses_map[verse] = blinker_test_dataset.nodes_map[edit][verse][tok]\n",
    "                break\n",
    "\n",
    "sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "blinker_verses = [item[0] for item in sorted_verses]\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-11 11:13:09,970 - analytics - INFO - done reading alignments\n",
      "2021-09-11 11:13:09,972 - analytics - INFO - done saving pruned alignments\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "reading inter verse alignments\n",
      "done reading inter verse alignments\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#helfi_heb_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_helfi_heb_test_community_word.pickle\", map_location=torch.device('cpu'))\n",
    "#editf_fin = \"fin-x-bible-helfi\"\n",
    "#editf_heb = 'heb-x-bible-helfi'\n",
    "\n",
    "#test_gold_helfi_heb = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-heb-fin-gold-alignments_test.txt\"\n",
    "\n",
    "#pros_heb, surs_heb = autils.load_gold(test_gold_helfi_heb)\n",
    "#heb_verses = list(pros_blinker.keys())\n",
    "\n",
    "##blinker_verse_alignments_inter = {}\n",
    "##blinker_verse_alignments_gdfa = {}\n",
    "##args = []\n",
    "##for i,verse in enumerate(blinker_verses):\n",
    "##    args.append((verse, current_editions))\n",
    "\n",
    "##with Pool(20) as p:\n",
    "##    all_res = p.map(get_pruned_verse_alignments, args)\n",
    "\n",
    "##for i,verse in enumerate(blinker_verses):\n",
    "##    verse_aligns_inter, verse_aligns_gdfa = all_res[i]\n",
    "    \n",
    "##    blinker_verse_alignments_inter[verse] = verse_aligns_inter\n",
    "##    blinker_verse_alignments_gdfa[verse] = verse_aligns_gdfa\n",
    "\n",
    "#utils.LOG.info(\"done reading alignments\")\n",
    "##torch.save(blinker_verse_alignments_inter, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_inter.pickle\")\n",
    "##torch.save(blinker_verse_alignments_gdfa, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_gdfa.pickle\")\n",
    "#utils.LOG.info('done saving pruned alignments')\n",
    "\n",
    "#print('reading inter verse alignments')\n",
    "#blinker_verse_alignments_inter = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_inter.pickle\")\n",
    "#blinker_verse_alignments_gdfa = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_blinker_gdfa.pickle\")\n",
    "#gc.collect()\n",
    "#print('done reading inter verse alignments')\n",
    "\n",
    "#verses_map = {}\n",
    "\n",
    "#for edit in blinker_test_dataset.nodes_map:\n",
    "#    for verse in blinker_test_dataset.nodes_map[edit]:\n",
    "#        if verse not in verses_map:\n",
    "#            for tok in blinker_test_dataset.nodes_map[edit][verse]:\n",
    "#                verses_map[verse] = blinker_test_dataset.nodes_map[edit][verse][tok]\n",
    "#                break\n",
    "\n",
    "#sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "#blinker_verses = [item[0] for item in sorted_verses]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#importlib.reload(afeatures)\n",
    "#grc_test_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_helfi_grc_test_community_word.pickle\", map_location=torch.device('cpu'))\n",
    "#editf_fin = \"fin-x-bible-helfi\"\n",
    "#editf_grc = 'grc-x-bible-helfi'\n",
    "\n",
    "#test_gold_grc = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-grc-fin-gold-alignments_test.txt\"\n",
    "\n",
    "#pros_grc, surs_grc = autils.load_gold(test_gold_grc)\n",
    "#grc_verses = list(pros_grc.keys())\n",
    "\n",
    "\n",
    "#grc_test_verse_alignments_inter = {}\n",
    "#grc_test_verse_alignments_gdfa = {}\n",
    "#gc.collect()\n",
    "##args = []\n",
    "##for i,verse in enumerate(grc_verses):\n",
    "##    args.append((verse, current_editions))\n",
    "\n",
    "##with Pool(20) as p:\n",
    "##    all_res = p.map(get_pruned_verse_alignments, args)\n",
    "\n",
    "##for i,verse in enumerate(grc_verses):\n",
    "##    verse_aligns_inter, verse_aligns_gdfa = all_res[i]\n",
    "    \n",
    "##    grc_test_verse_alignments_inter[verse] = verse_aligns_inter\n",
    "##    grc_test_verse_alignments_gdfa[verse] = verse_aligns_gdfa\n",
    "\n",
    "#utils.LOG.info(\"done reading alignments\")\n",
    "##torch.save(grc_test_verse_alignments_inter, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_grc_inter.pickle\")\n",
    "##torch.save(grc_test_verse_alignments_gdfa, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_grc_gdfa.pickle\")\n",
    "#utils.LOG.info('done saving pruned alignments')\n",
    "\n",
    "#print('reading inter verse alignments')\n",
    "#grc_test_verse_alignments_inter = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_grc_inter.pickle\")\n",
    "#grc_test_verse_alignments_gdfa = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_grc_gdfa.pickle\")\n",
    "#gc.collect()\n",
    "#print('done reading inter verse alignments')\n",
    "\n",
    "#verses_map = {}\n",
    "\n",
    "#for edit in grc_test_dataset.nodes_map:\n",
    "#    for verse in grc_test_dataset.nodes_map[edit]:\n",
    "#        if verse not in verses_map:\n",
    "#            for tok in grc_test_dataset.nodes_map[edit][verse]:\n",
    "#                verses_map[verse] = grc_test_dataset.nodes_map[edit][verse][tok]\n",
    "#                break\n",
    "\n",
    "#sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "#grc_test_verses = [item[0] for item in sorted_verses]\n",
    "\n",
    "#gc.collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#heb_test_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_helfi_heb_test_community_word.pickle\", map_location=torch.device('cpu'))\n",
    "#editf_fin = \"fin-x-bible-helfi\"\n",
    "#editf_heb = 'heb-x-bible-helfi'\n",
    "\n",
    "#test_gold_heb = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-heb-fin-gold-alignments_test.txt\"\n",
    "\n",
    "#pros_heb, surs_heb = autils.load_gold(test_gold_heb)\n",
    "#heb_verses = list(pros_heb.keys())\n",
    "\n",
    "\n",
    "#heb_test_verse_alignments_inter = {}\n",
    "#heb_test_verse_alignments_gdfa = {}\n",
    "##args = []\n",
    "##for i,verse in enumerate(heb_verses):\n",
    "##    args.append((verse, current_editions))\n",
    "\n",
    "##with Pool(20) as p:\n",
    "##    all_res = p.map(get_pruned_verse_alignments, args)\n",
    "\n",
    "##for i,verse in enumerate(heb_verses):\n",
    "##    verse_aligns_inter, verse_aligns_gdfa = all_res[i]\n",
    "    \n",
    "##    heb_test_verse_alignments_inter[verse] = verse_aligns_inter\n",
    "##    heb_test_verse_alignments_gdfa[verse] = verse_aligns_gdfa\n",
    "\n",
    "##utils.LOG.info(\"done reading alignments\")\n",
    "##torch.save(heb_test_verse_alignments_inter, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_heb_inter.pickle\")\n",
    "##torch.save(heb_test_verse_alignments_gdfa, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_heb_gdfa.pickle\")\n",
    "##utils.LOG.info('done saving pruned alignments')\n",
    "\n",
    "#print('reading inter verse alignments')\n",
    "#heb_test_verse_alignments_inter = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_heb_inter.pickle\")\n",
    "#heb_test_verse_alignments_gdfa = torch.load(\"/mounts/work/ayyoob/models/gnn/pruned_alignments_heb_gdfa.pickle\")\n",
    "#gc.collect()\n",
    "#print('done reading inter verse alignments')\n",
    "\n",
    "#verses_map = {}\n",
    "\n",
    "#for edit in heb_test_dataset.nodes_map:\n",
    "#    for verse in heb_test_dataset.nodes_map[edit]:\n",
    "#        if verse not in verses_map:\n",
    "#            for tok in heb_test_dataset.nodes_map[edit][verse]:\n",
    "#                verses_map[verse] = heb_test_dataset.nodes_map[edit][verse][tok]\n",
    "#                break\n",
    "\n",
    "#sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "#heb_test_verses = [item[0] for item in sorted_verses]\n",
    "#gc.collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "verses_map = {}\n",
    "\n",
    "for edit in train_dataset.nodes_map:\n",
    "    for verse in train_dataset.nodes_map[edit]:\n",
    "        if verse not in verses_map:\n",
    "            for tok in train_dataset.nodes_map[edit][verse]:\n",
    "                verses_map[verse] = train_dataset.nodes_map[edit][verse][tok]\n",
    "                break\n",
    "\n",
    "sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "all_verses = [item[0] for item in sorted_verses]\n",
    "\n",
    "long_verses = set()\n",
    "\n",
    "for edit in train_dataset.nodes_map.keys():\n",
    "    for verse in train_dataset.nodes_map[edit]:\n",
    "        to_print = False\n",
    "        for tok in train_dataset.nodes_map[edit][verse]:\n",
    "            if tok > 150:\n",
    "                to_print = True\n",
    "        if to_print == True:\n",
    "            long_verses.add(verse)\n",
    "\n",
    "\n",
    "train_verses = all_verses[:]\n",
    "\n",
    "masked_verses = list(long_verses)\n",
    "masked_verses.extend(blinker_verses)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#verse_alignments_inter_1000 = {}\n",
    "#for verse in train_verses[:8000]:\n",
    "#    verse_alignments_inter_1000[verse] = verse_alignments_inter[verse]\n",
    "\n",
    "#max_node = 0\n",
    "#remaining_verses = train_verses[8000:]\n",
    "#for editf in train_dataset.nodes_map:\n",
    "#    for verse in remaining_verses:\n",
    "#        if verse in train_dataset.nodes_map[editf]:\n",
    "#            del train_dataset.nodes_map[editf][verse]\n",
    "\n",
    "#    if train_verses[7999] in train_dataset.nodes_map[editf]:\n",
    "#        max_ = max(list(train_dataset.nodes_map[editf][train_verses[7999]].values()))\n",
    "#        max_node = max(max_, max_node)\n",
    "\n",
    "#train_dataset.edge_index = train_dataset.edge_index[:, :int(879904916/2)]\n",
    "#train_dataset.x = train_dataset.x[:max_node+2, :]\n",
    "\n",
    "#torch.save(train_dataset, \"/mounts/work/ayyoob/models/gnn/dataset_helfi_train_community_word_8000.pickle\")\n",
    "#torch.save(verse_alignments_inter_1000, \"/mounts/work/ayyoob/models/gnn/pruned_alignments_train_inter_8000.pickle\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "class GNNDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset, verses, edit_files, alignments, group_size = 360):\n",
    "        self.verses = list(verses)\n",
    "        self.edit_files = list(edit_files)\n",
    "        self.nodes_map = dataset.nodes_map\n",
    "\n",
    "        self.items = self.calculate_size(self.nodes_map, self.verses, self.edit_files, group_size)\n",
    "        self.alignments = alignments\n",
    "        self.verse_info = {}\n",
    "        self.calculate_verse_stats(verses, edit_files, alignments, dataset)\n",
    "    \n",
    "    def calculate_size(self, nodes_map, verses, edit_files, group_size):\n",
    "        res = []\n",
    "        item = []\n",
    "        self.not_presented = []\n",
    "        for verse in verses:\n",
    "            if len(item) > 0:\n",
    "                res.append(item)\n",
    "                item = []\n",
    "            for i,editf1 in enumerate(edit_files):\n",
    "                if editf1 not in nodes_map:\n",
    "                    self.not_presented.append(editf1)\n",
    "                    continue\n",
    "                if verse in nodes_map[editf1]:\n",
    "                    for editf2 in edit_files[i+1:]:\n",
    "                        if editf2 not in nodes_map:\n",
    "                            self.not_presented.append(editf2)\n",
    "                            continue\n",
    "                        if verse in nodes_map[editf2]:\n",
    "                            item.append((verse, editf1, editf2))\n",
    "                            if len(item) >= group_size:\n",
    "                                res.append(item)\n",
    "                                item = []\n",
    "        \n",
    "        if len(item)>0:\n",
    "            res.append(item)\n",
    "        \n",
    "        print(f\"not presented: {set(self.not_presented)}\")\n",
    "\n",
    "        return res\n",
    "\n",
    "    def calculate_verse_stats(self,verses, edition_files, alignments, dataset):\n",
    "        min_edge = 0\n",
    "        for verse in tqdm(verses):\n",
    "            min_nodes = 99999999999999\n",
    "            max_nodes = 0\n",
    "            #utils.LOG.info(f\"adding {verse}\")\n",
    "            edges_tmp = [[],[]]\n",
    "            x_tmp = []\n",
    "            features = []\n",
    "            for i,editf1 in enumerate(edition_files):\n",
    "                for j,editf2 in enumerate(edition_files[i+1:]):\n",
    "                    aligns = autils.get_aligns(editf1, editf2, alignments[verse])\n",
    "                    if aligns != None:\n",
    "                        for align in aligns:\n",
    "                            try:\n",
    "                                n1,_ = gutils.node_nom(verse, editf1, align[0], None, dataset.nodes_map, x_tmp, edition_files, features)\n",
    "                                n2,_ = gutils.node_nom(verse, editf2, align[1], None, dataset.nodes_map, x_tmp, edition_files, features)\n",
    "                                edges_tmp[0].extend([n1, n2])\n",
    "\n",
    "                                max_nodes = max(n1, n2, max_nodes)\n",
    "                                min_nodes = min(n1, n2, min_nodes)\n",
    "                            except Exception as e:\n",
    "                                print(editf1, editf2, verse)\n",
    "                                raise(e)\n",
    "\n",
    "            self.verse_info[verse] = {}\n",
    "\n",
    "            self.verse_info[verse]['padding'] = min_nodes\n",
    "\n",
    "            self.verse_info[verse]['x'] = dataset.x[min_nodes:max_nodes+1,:]\n",
    "            \n",
    "            self.verse_info[verse]['edge_index'] = dataset.edge_index[:, min_edge : min_edge + len(edges_tmp[0])] - min_nodes\n",
    "\n",
    "            if torch.min(self.verse_info[verse]['edge_index']) != 0:\n",
    "                print(verse, min_nodes, max_nodes, min_edge, len(edges_tmp[0]))\n",
    "                print(torch.min(self.verse_info[verse]['edge_index']))\n",
    "            \n",
    "            if self.verse_info[verse]['x'].shape[0] != torch.max(self.verse_info[verse]['edge_index']) + 1 :\n",
    "                print(verse, min_nodes, max_nodes, min_edge, len(edges_tmp[0]))\n",
    "                print(torch.min(self.verse_info[verse]['edge_index']))\n",
    "            \n",
    "            min_edge = min_edge + len(edges_tmp[0])\n",
    "\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        #return self.length\n",
    "        return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        item = self.items[idx]\n",
    "\n",
    "        res_pos = [[],[]]\n",
    "        res_neg = [[],[]]\n",
    "        nodes = set()\n",
    "        for instance in item:\n",
    "            verse, editf1, editf2 = instance\n",
    "            aligns = autils.get_aligns(editf1, editf2, self.alignments[verse]) \n",
    "            if aligns != None:\n",
    "                for align in aligns:\n",
    "                    p1, p2 = align\n",
    "                    n1 = self.nodes_map[editf1][verse][p1] - self.verse_info[verse]['padding']\n",
    "                    n2 = self.nodes_map[editf2][verse][p2] - self.verse_info[verse]['padding']\n",
    "                    res_pos[0].extend([n1,n2])\n",
    "                    res_pos[1].extend([n2,n1])\n",
    "\n",
    "                    n2_ = random.choice( list(self.nodes_map[editf2][verse].values()) ) - self.verse_info[verse]['padding']\n",
    "                    n1_ = random.choice( list(self.nodes_map[editf1][verse].values()) ) - self.verse_info[verse]['padding']\n",
    "                    \n",
    "                    if n2_ != n2:\n",
    "                        res_neg[0].extend([n1, n2_])\n",
    "                        res_neg[1].extend([n2_, n1])\n",
    "                    \n",
    "                    if n1_ != n1:\n",
    "                        res_neg[0].extend([n1_, n2])\n",
    "                        res_neg[1].extend([n2, n1_])\n",
    "                    \n",
    "                    nodes.update([n1, n2, n1_, n2_])\n",
    "                \n",
    "        \n",
    "        return {'pos':res_pos, 'neg':res_neg, 'nodes':nodes, 'verse':verse, 'editf1':editf1, 'editf2':editf2}\n",
    "\n",
    "def collate_fun(input):\n",
    "    res = {}\n",
    "    #all_edits = {}\n",
    "    for item in input:\n",
    "        verse = item['verse'] \n",
    "        if verse not in res:\n",
    "            res[verse] = {'pos': [[],[]], 'neg' : [[],[]],\n",
    "                 'x':gnn_dataset.verse_info[verse]['x'], 'edge_index':gnn_dataset.verse_info[verse]['edge_index']\n",
    "                 ,'intra_sent_edges':[[],[]]}\n",
    "        \n",
    "        res[verse]['pos'][0].extend(item['pos'][0])\n",
    "        res[verse]['pos'][1].extend(item['pos'][1])\n",
    "\n",
    "        res[verse]['neg'][0].extend(item['neg'][0])\n",
    "        res[verse]['neg'][1].extend(item['neg'][1])\n",
    "\n",
    "        #if verse not in all_edits:\n",
    "        #    all_edits[verse] = []\n",
    "\n",
    "        #if item['editf1'] not in all_edits[verse]:\n",
    "        #    e = eval_utils.get_all_edges(verse, item['editf1'], train_dataset.nodes_map, gnn_dataset.verse_info)\n",
    "        #    res[verse]['intra_sent_edges'][0].extend(e[0])\n",
    "        #    res[verse]['intra_sent_edges'][1].extend(e[1])\n",
    "\n",
    "        #if item['editf2'] not in all_edits[verse]:\n",
    "        #    e = eval_utils.get_all_edges(verse, item['editf2'], train_dataset.nodes_map, gnn_dataset.verse_info)\n",
    "        #    res[verse]['intra_sent_edges'][0].extend(e[0])\n",
    "        #    res[verse]['intra_sent_edges'][1].extend(e[1])\n",
    "\n",
    "    #nodes = list(nodes)\n",
    "    #mapping = {node:pos for pos, node in enumerate(nodes)}\n",
    "    ##indices = [[i for i in range(len(res_pos[0]))],[i for i in range(len(res_pos[1]))]]\n",
    "    #indices = [[],[]]\n",
    "    ##adjacency = torch.zeros((len(nodes), len(nodes)), dtype=torch.float)\n",
    "    #for i in range(len(res_pos[0])):\n",
    "    ##    adjacency[mapping[res_pos[0][i]], mapping[res_pos[1][i]]] = 1\n",
    "    #    indices[0].append(mapping[res_pos[0][i]])\n",
    "    #    indices[1].append(mapping[res_pos[1][i]])\n",
    "\n",
    "    #adjacency = torch.sparse_coo_tensor(indices, [1 for i in range(len(res_pos[0]))], (len(nodes), len(nodes)))\n",
    "    return res\n",
    "\n",
    "\n",
    "#gnn_dataset_train = GNNDataset(train_dataset, train_verses, current_editions, verse_alignments_inter)\n",
    "gnn_dataset_blinker = GNNDataset(blinker_test_dataset, blinker_verses, current_editions, blinker_verse_alignments_inter)\n",
    "#gnn_dataset_heb = GNNDataset(heb_test_dataset, heb_test_verses, current_editions, heb_test_verse_alignments_inter)\n",
    "#gnn_dataset_grc = GNNDataset(grc_test_dataset, grc_test_verses, current_editions, grc_test_verse_alignments_inter)\n",
    "\n",
    "#len(gnn_dataset_train)\n",
    "gc.collect()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  1%|          | 3/250 [00:00<00:10, 23.49it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "not presented: set()\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 250/250 [00:14<00:00, 17.53it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from gnn_utils import eval_utils\n",
    "train_dataset.train_mask = train_dataset.val_mask = train_dataset.test_mask = train_dataset.y = None\n",
    "test_dataset.train_mask = test_dataset.val_mask = test_dataset.test_mask = test_dataset.y = None\n",
    "\n",
    "torch.cuda.set_device(0)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9140e8fad2e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgnn_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meval_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def save_model(model, name):\n",
    "    model.encoder.feature_encoder.feature_types[0] = afeatures.OneHotFeature(20, 83, 'editf')\n",
    "    model.encoder.feature_encoder.feature_types[1] = afeatures.OneHotFeature(32, 150, 'position')\n",
    "    model.encoder.feature_encoder.feature_types[2] = afeatures.FloatFeature(4, 'degree_centrality')\n",
    "    model.encoder.feature_encoder.feature_types[3] = afeatures.FloatFeature(4, 'closeness_centrality')\n",
    "    model.encoder.feature_encoder.feature_types[4] = afeatures.FloatFeature(4, 'betweenness_centrality')\n",
    "    model.encoder.feature_encoder.feature_types[5] = afeatures.FloatFeature(4, 'load_centrality')\n",
    "    model.encoder.feature_encoder.feature_types[6] = afeatures.FloatFeature(4, 'harmonic_centrality')\n",
    "    model.encoder.feature_encoder.feature_types[7] = afeatures.OneHotFeature(32, 250, 'greedy_modularity_community')\n",
    "    model.encoder.feature_encoder.feature_types[8] = afeatures.OneHotFeature(32, 250, 'community_2')\n",
    "    model.encoder.feature_encoder.feature_types[9] = afeatures.MappingFeature(100, 'word')\n",
    "    torch.save(model, f'/mounts/work/ayyoob/models/gnn/checkpoint/{name}' + datetime.now().strftime(\"%Y%m%d-%H%M%S-\") + '.pickle')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "verse_counts = {400:'trainsize_400_51220210911-015525-.pickle', 800:'trainsize_800_51220210911-032412-.pickle', \n",
    "1600:\"trainsize_1600_51220210911-045657-.pickle\", 3200:'trainsize_3200_51220210911-063319-.pickle', 6400:'trainsize_6400_51220210911-081557-.pickle'}\n",
    "\n",
    "for verse_count in verse_counts:\n",
    "    print(f'loading {verse_count}')\n",
    "    model = torch.load('/mounts/work/ayyoob/models/gnn/checkpoint/' + verse_counts[verse_count])\n",
    "\n",
    "    writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S-\") + f\"samett-chs-feat-\")\n",
    "\n",
    "    eval_utils.alignment_test(1, blinker_test_dataset.edge_index, editf12, editf22, blinker_verses, blinker_test_dataset.nodes_map,\n",
    "                dev, model, blinker_test_dataset.x, pros_blinker, surs_blinker, blinker_verse_alignments_inter, blinker_verse_alignments_gdfa, writer, gnn_dataset_blinker.verse_info)\n",
    "        \n",
    "    clean_memory()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading 400\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 250/250 [00:15<00:00, 15.84it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.752, rec: 0.735, F1: 0.743, AER: 0.256\n",
      "argmax prec: 0.932, rec: 0.664, F1: 0.775, AER: 0.221\n",
      "resnorm prec: 0.41, rec: 0.787, F1: 0.539, AER: 0.474\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.893, rec: 0.706, F1: 0.789, AER: 0.208\n",
      "my_gd_gdfa prec: 0.844, rec: 0.788, F1: 0.815, AER: 0.184\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "loading 800\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 250/250 [00:10<00:00, 24.97it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.777, rec: 0.751, F1: 0.764, AER: 0.236\n",
      "argmax prec: 0.938, rec: 0.671, F1: 0.782, AER: 0.215\n",
      "resnorm prec: 0.55, rec: 0.779, F1: 0.645, AER: 0.363\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.911, rec: 0.711, F1: 0.799, AER: 0.198\n",
      "my_gd_gdfa prec: 0.856, rec: 0.789, F1: 0.821, AER: 0.177\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "loading 1600\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 250/250 [00:10<00:00, 24.19it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.778, rec: 0.752, F1: 0.765, AER: 0.235\n",
      "argmax prec: 0.941, rec: 0.673, F1: 0.785, AER: 0.212\n",
      "resnorm prec: 0.577, rec: 0.776, F1: 0.662, AER: 0.345\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.913, rec: 0.711, F1: 0.799, AER: 0.198\n",
      "my_gd_gdfa prec: 0.856, rec: 0.787, F1: 0.82, AER: 0.178\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "loading 3200\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 250/250 [00:10<00:00, 24.81it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.781, rec: 0.753, F1: 0.767, AER: 0.232\n",
      "argmax prec: 0.939, rec: 0.672, F1: 0.783, AER: 0.214\n",
      "resnorm prec: 0.626, rec: 0.771, F1: 0.691, AER: 0.314\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.92, rec: 0.705, F1: 0.798, AER: 0.198\n",
      "my_gd_gdfa prec: 0.862, rec: 0.787, F1: 0.823, AER: 0.176\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "loading 6400\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 250/250 [00:09<00:00, 25.11it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.786, rec: 0.757, F1: 0.771, AER: 0.228\n",
      "argmax prec: 0.941, rec: 0.673, F1: 0.785, AER: 0.213\n",
      "resnorm prec: 0.649, rec: 0.776, F1: 0.707, AER: 0.297\n",
      "itermax2-.9 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.95 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "itermax2-.8 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "my_gd prec: 0.916, rec: 0.712, F1: 0.801, AER: 0.196\n",
      "my_gd_gdfa prec: 0.86, rec: 0.789, F1: 0.823, AER: 0.175\n",
      "new1 prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n",
      "new_mygd_gdfa prec: 0.0, rec: 0.0, F1: 0.0, AER: 1.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tqdm import tqdm\n",
    "features_edge = []\n",
    "drop_out = 0\n",
    "verse_counts = {25:120, 50:80, 100:64, 200:32, 400:16, 800:8, 1600:4, 3200:2, 6400:1}\n",
    "\n",
    "gnn_dataset_blinker = GNNDataset(blinker_test_dataset, blinker_verses, current_editions, blinker_verse_alignments_inter)\n",
    "\n",
    "for verse_count in verse_counts:\n",
    "    print('training with less verses: ', verse_count)\n",
    "    features = train_dataset.features\n",
    "    gnn_dataset = GNNDataset(train_dataset, train_verses[:verse_count], current_editions, verse_alignments_inter)\n",
    "    data_loader = DataLoader(gnn_dataset, batch_size=1, collate_fn=collate_fun, shuffle=True)\n",
    "\n",
    "    clean_memory()\n",
    "\n",
    "    n_head = 1\n",
    "\n",
    "    channels = 512\n",
    "\n",
    "    in_dim = sum(t.out_dim for t in features)\n",
    "    decoder_in_dim = n_head * channels * 2 + sum(t.out_dim for t in features_edge) \n",
    "    print('edge features size: ', sum(t.out_dim for t in features_edge))\n",
    "\n",
    "    decoder = Decoder(decoder_in_dim, int(decoder_in_dim/2), features_edge, n_cluster=64)\n",
    "    model = pyg_nn.GAE(Encoder(in_dim, channels, features, n_head, edge_feature_dim=len(features_edge)), decoder).to(dev)\n",
    "\n",
    "    print(\"sending input to gpu\")\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    optimizer.add_param_group({'params': word_vectors})\n",
    "\n",
    "    writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S-\") + f\"samett-{channels}chs-feat{train_dataset.num_node_features}-\")\n",
    "\n",
    "    torch.set_printoptions(edgeitems=5)\n",
    "    print(\"model params - decoder params - conv1\", sum(p.numel() for p in model.parameters()), sum(p.numel() for p in decoder.parameters()))\n",
    "\n",
    "    for epoch in range(1, verse_counts[verse_count]+1):\n",
    "        print(f\"\\n----------------epoch {epoch} ---------------\")\n",
    "        train(epoch)\n",
    "        \n",
    "    save_model(model, f'trainsize_{verse_count}_512')\n",
    "    clean_memory()\n",
    "    eval_utils.alignment_test(epoch, blinker_test_dataset.edge_index, editf12, editf22, blinker_verses, blinker_test_dataset.nodes_map,\n",
    "                dev, model, blinker_test_dataset.x, pros_blinker, surs_blinker, blinker_verse_alignments_inter, blinker_verse_alignments_gdfa, writer, gnn_dataset_blinker.verse_info)\n",
    "        \n",
    "    clean_memory()\n",
    "    \n",
    "    model = None\n",
    "    decoder = None\n",
    "    gnn_dataset_train = None\n",
    "    gnn_dataset = None\n",
    "    clean_memory()\n",
    "    \n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  1%|          | 3/250 [00:00<00:08, 28.79it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "not presented: set()\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 250/250 [00:15<00:00, 15.89it/s]\n",
      " 12%|█▏        | 3/25 [00:00<00:00, 23.20it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training with less verses:  25\n",
      "not presented: set()\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 25/25 [00:01<00:00, 16.10it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "edge features size:  0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/123 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sending input to gpu\n",
      "model params - decoder params - conv1 237901021 1574913\n",
      "\n",
      "----------------epoch 1 ---------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  1%|          | 1/123 [00:01<02:29,  1.23s/it]"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "i = sag\n",
    "batch = khar\n",
    "verse = gav\n",
    "print(i, verse)\n",
    "\n",
    "keys = list(gnn_dataset.verse_info.keys())\n",
    "\n",
    "gnn_dataset.verse_info[verse]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_loader_blinker = DataLoader(gnn_dataset_blinker, batch_size=1, collate_fn=collate_fun, shuffle=True)\n",
    "data_loader_heb = DataLoader(gnn_dataset_heb, batch_size=1, collate_fn=collate_fun, shuffle=True)\n",
    "data_loader_grc = DataLoader(gnn_dataset_grc, batch_size=1, collate_fn=collate_fun, shuffle=True)\n",
    "\n",
    "clean_memory()\n",
    "data_loader = data_loader_blinker\n",
    "gnn_dataset = gnn_dataset_blinker\n",
    "train(1)\n",
    "clean_memory()\n",
    "eval_utils.alignment_test(epoch, blinker_test_dataset.edge_index, editf12, editf22, blinker_verses[:], blinker_test_dataset.nodes_map,\n",
    "                    dev, model, blinker_test_dataset.x, pros_blinker, surs_blinker, blinker_verse_alignments_inter, blinker_verse_alignments_gdfa, writer, gnn_dataset_blinker.verse_info)\n",
    "clean_memory()\n",
    "\n",
    "data_loader = data_loader_grc\n",
    "gnn_dataset = gnn_dataset_grc\n",
    "train(1)\n",
    "clean_memory()\n",
    "eval_utils.alignment_test(epoch, grc_test_dataset.edge_index, editf_fin, editf_grc, grc_test_verses[:], grc_test_dataset.nodes_map,\n",
    "                        dev, model, grc_test_dataset.x, pros_grc, surs_grc, grc_test_verse_alignments_inter, grc_test_verse_alignments_gdfa, writer, gnn_dataset_grc.verse_info)\n",
    "clean_memory()\n",
    "\n",
    "data_loader = data_loader_heb\n",
    "gnn_dataset = gnn_dataset_heb\n",
    "train(1)\n",
    "clean_memory()\n",
    "eval_utils.alignment_test(epoch, heb_test_dataset.edge_index, editf_fin, editf_heb, heb_test_verses[:], heb_test_dataset.nodes_map,\n",
    "                        dev, model, heb_test_dataset.x, pros_heb, surs_heb, heb_test_verse_alignments_inter, heb_test_verse_alignments_gdfa, writer, gnn_dataset_heb.verse_info)\n",
    "clean_memory()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1180/1180 [00:32<00:00, 36.58it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train loss: 0\n",
      "cluster loss: 0\n",
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.783, rec: 0.754, F1: 0.768, AER: 0.231\n",
      "argmax prec: 0.94, rec: 0.673, F1: 0.784, AER: 0.213\n",
      "resnorm prec: 0.707, rec: 0.759, F1: 0.732, AER: 0.269\n",
      "itermax2-.9 prec: 0.852, rec: 0.724, F1: 0.783, AER: 0.214\n",
      "itermax2-.95 prec: 0.85, rec: 0.724, F1: 0.782, AER: 0.215\n",
      "itermax2-.8 prec: 0.854, rec: 0.725, F1: 0.784, AER: 0.214\n",
      "my_gd prec: 0.924, rec: 0.704, F1: 0.799, AER: 0.198\n",
      "my_gd_gdfa prec: 0.865, rec: 0.784, F1: 0.823, AER: 0.175\n",
      "new1 prec: 0.945, rec: 0.661, F1: 0.778, AER: 0.219\n",
      "new_mygd prec: 0.92, rec: 0.706, F1: 0.799, AER: 0.198\n",
      "new_mygd_gdfa prec: 0.863, rec: 0.786, F1: 0.823, AER: 0.176\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 43%|████▎     | 2999/7000 [07:08<09:53,  6.74it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.647, rec: 0.797, F1: 0.714, AER: 0.286\n",
      "argmax prec: 0.857, rec: 0.697, F1: 0.769, AER: 0.231\n",
      "resnorm prec: 0.679, rec: 0.779, F1: 0.726, AER: 0.275\n",
      "itermax2-.9 prec: 0.736, rec: 0.766, F1: 0.751, AER: 0.25\n",
      "itermax2-.95 prec: 0.734, rec: 0.764, F1: 0.749, AER: 0.251\n",
      "itermax2-.8 prec: 0.738, rec: 0.766, F1: 0.752, AER: 0.248\n",
      "my_gd prec: 0.838, rec: 0.731, F1: 0.781, AER: 0.22\n",
      "my_gd_gdfa prec: 0.736, rec: 0.786, F1: 0.76, AER: 0.24\n",
      "new1 prec: 0.874, rec: 0.69, F1: 0.771, AER: 0.229\n",
      "new_mygd prec: 0.835, rec: 0.732, F1: 0.78, AER: 0.22\n",
      "new_mygd_gdfa prec: 0.734, rec: 0.786, F1: 0.759, AER: 0.241\n",
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.515, rec: 0.75, F1: 0.611, AER: 0.389\n",
      "argmax prec: 0.774, rec: 0.607, F1: 0.68, AER: 0.32\n",
      "resnorm prec: 0.548, rec: 0.727, F1: 0.625, AER: 0.375\n",
      "itermax2-.9 prec: 0.643, rec: 0.693, F1: 0.667, AER: 0.333\n",
      "itermax2-.95 prec: 0.64, rec: 0.692, F1: 0.665, AER: 0.335\n",
      "itermax2-.8 prec: 0.645, rec: 0.694, F1: 0.669, AER: 0.331\n",
      "my_gd prec: 0.75, rec: 0.65, F1: 0.696, AER: 0.304\n",
      "my_gd_gdfa prec: 0.602, rec: 0.689, F1: 0.643, AER: 0.357\n",
      "new1 prec: 0.799, rec: 0.606, F1: 0.689, AER: 0.311\n",
      "new_mygd prec: 0.745, rec: 0.653, F1: 0.696, AER: 0.304\n",
      "new_mygd_gdfa prec: 0.599, rec: 0.692, F1: 0.642, AER: 0.358\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 43%|████▎     | 3001/7000 [12:08<69:33:39, 62.62s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.788, rec: 0.757, F1: 0.772, AER: 0.227\n",
      "argmax prec: 0.939, rec: 0.672, F1: 0.783, AER: 0.213\n",
      "resnorm prec: 0.711, rec: 0.767, F1: 0.738, AER: 0.264\n",
      "itermax2-.9 prec: 0.85, rec: 0.728, F1: 0.784, AER: 0.213\n",
      "itermax2-.95 prec: 0.849, rec: 0.728, F1: 0.784, AER: 0.214\n",
      "itermax2-.8 prec: 0.851, rec: 0.727, F1: 0.784, AER: 0.213\n",
      "my_gd prec: 0.919, rec: 0.708, F1: 0.8, AER: 0.197\n",
      "my_gd_gdfa prec: 0.863, rec: 0.785, F1: 0.822, AER: 0.176\n",
      "new1 prec: 0.945, rec: 0.663, F1: 0.779, AER: 0.218\n",
      "new_mygd prec: 0.914, rec: 0.709, F1: 0.799, AER: 0.198\n",
      "new_mygd_gdfa prec: 0.859, rec: 0.787, F1: 0.821, AER: 0.177\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 86%|████████▌ | 5999/7000 [18:42<02:03,  8.12it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.648, rec: 0.797, F1: 0.715, AER: 0.285\n",
      "argmax prec: 0.856, rec: 0.698, F1: 0.769, AER: 0.231\n",
      "resnorm prec: 0.669, rec: 0.781, F1: 0.721, AER: 0.279\n",
      "itermax2-.9 prec: 0.742, rec: 0.765, F1: 0.753, AER: 0.247\n",
      "itermax2-.95 prec: 0.741, rec: 0.765, F1: 0.753, AER: 0.247\n",
      "itermax2-.8 prec: 0.743, rec: 0.765, F1: 0.754, AER: 0.246\n",
      "my_gd prec: 0.838, rec: 0.731, F1: 0.781, AER: 0.219\n",
      "my_gd_gdfa prec: 0.736, rec: 0.785, F1: 0.76, AER: 0.24\n",
      "new1 prec: 0.874, rec: 0.692, F1: 0.772, AER: 0.228\n",
      "new_mygd prec: 0.836, rec: 0.732, F1: 0.781, AER: 0.22\n",
      "new_mygd_gdfa prec: 0.735, rec: 0.786, F1: 0.76, AER: 0.24\n",
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.51, rec: 0.748, F1: 0.606, AER: 0.393\n",
      "argmax prec: 0.777, rec: 0.602, F1: 0.678, AER: 0.322\n",
      "resnorm prec: 0.545, rec: 0.726, F1: 0.623, AER: 0.377\n",
      "itermax2-.9 prec: 0.649, rec: 0.69, F1: 0.669, AER: 0.332\n",
      "itermax2-.95 prec: 0.646, rec: 0.689, F1: 0.667, AER: 0.333\n",
      "itermax2-.8 prec: 0.65, rec: 0.69, F1: 0.669, AER: 0.33\n",
      "my_gd prec: 0.759, rec: 0.646, F1: 0.698, AER: 0.302\n",
      "my_gd_gdfa prec: 0.606, rec: 0.685, F1: 0.643, AER: 0.357\n",
      "new1 prec: 0.802, rec: 0.602, F1: 0.688, AER: 0.312\n",
      "new_mygd prec: 0.752, rec: 0.65, F1: 0.697, AER: 0.303\n",
      "new_mygd_gdfa prec: 0.603, rec: 0.689, F1: 0.643, AER: 0.357\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 86%|████████▌ | 6001/7000 [23:40<17:24:46, 62.75s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.785, rec: 0.757, F1: 0.771, AER: 0.229\n",
      "argmax prec: 0.941, rec: 0.671, F1: 0.783, AER: 0.213\n",
      "resnorm prec: 0.7, rec: 0.767, F1: 0.732, AER: 0.27\n",
      "itermax2-.9 prec: 0.849, rec: 0.726, F1: 0.783, AER: 0.215\n",
      "itermax2-.95 prec: 0.847, rec: 0.724, F1: 0.781, AER: 0.216\n",
      "itermax2-.8 prec: 0.85, rec: 0.726, F1: 0.783, AER: 0.214\n",
      "my_gd prec: 0.917, rec: 0.706, F1: 0.798, AER: 0.2\n",
      "my_gd_gdfa prec: 0.861, rec: 0.784, F1: 0.821, AER: 0.178\n",
      "new1 prec: 0.945, rec: 0.663, F1: 0.779, AER: 0.218\n",
      "new_mygd prec: 0.912, rec: 0.708, F1: 0.797, AER: 0.2\n",
      "new_mygd_gdfa prec: 0.857, rec: 0.787, F1: 0.821, AER: 0.178\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 7000/7000 [25:46<00:00,  4.53it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train loss: 5611854.063873291\n",
      "cluster loss: 748.3435668945312\n",
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.64, rec: 0.79, F1: 0.707, AER: 0.293\n",
      "argmax prec: 0.855, rec: 0.696, F1: 0.767, AER: 0.233\n",
      "resnorm prec: 0.666, rec: 0.773, F1: 0.716, AER: 0.285\n",
      "itermax2-.9 prec: 0.736, rec: 0.757, F1: 0.746, AER: 0.254\n",
      "itermax2-.95 prec: 0.734, rec: 0.756, F1: 0.745, AER: 0.255\n",
      "itermax2-.8 prec: 0.737, rec: 0.758, F1: 0.747, AER: 0.253\n",
      "my_gd prec: 0.836, rec: 0.729, F1: 0.779, AER: 0.221\n",
      "my_gd_gdfa prec: 0.736, rec: 0.784, F1: 0.759, AER: 0.241\n",
      "new1 prec: 0.872, rec: 0.69, F1: 0.77, AER: 0.23\n",
      "new_mygd prec: 0.833, rec: 0.73, F1: 0.778, AER: 0.222\n",
      "new_mygd_gdfa prec: 0.733, rec: 0.785, F1: 0.758, AER: 0.242\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 45%|████▌     | 2998/6635 [05:44<05:29, 11.03it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.643, rec: 0.79, F1: 0.709, AER: 0.291\n",
      "argmax prec: 0.852, rec: 0.7, F1: 0.769, AER: 0.231\n",
      "resnorm prec: 0.66, rec: 0.778, F1: 0.714, AER: 0.286\n",
      "itermax2-.9 prec: 0.738, rec: 0.76, F1: 0.749, AER: 0.251\n",
      "itermax2-.95 prec: 0.737, rec: 0.76, F1: 0.748, AER: 0.252\n",
      "itermax2-.8 prec: 0.739, rec: 0.76, F1: 0.749, AER: 0.251\n",
      "my_gd prec: 0.834, rec: 0.728, F1: 0.777, AER: 0.222\n",
      "my_gd_gdfa prec: 0.734, rec: 0.784, F1: 0.758, AER: 0.242\n",
      "new1 prec: 0.871, rec: 0.692, F1: 0.771, AER: 0.229\n",
      "new_mygd prec: 0.832, rec: 0.729, F1: 0.777, AER: 0.223\n",
      "new_mygd_gdfa prec: 0.732, rec: 0.785, F1: 0.758, AER: 0.242\n",
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.516, rec: 0.751, F1: 0.612, AER: 0.388\n",
      "argmax prec: 0.78, rec: 0.612, F1: 0.686, AER: 0.314\n",
      "resnorm prec: 0.568, rec: 0.728, F1: 0.638, AER: 0.362\n",
      "itermax2-.9 prec: 0.652, rec: 0.698, F1: 0.674, AER: 0.326\n",
      "itermax2-.95 prec: 0.649, rec: 0.696, F1: 0.672, AER: 0.328\n",
      "itermax2-.8 prec: 0.653, rec: 0.698, F1: 0.675, AER: 0.325\n",
      "my_gd prec: 0.766, rec: 0.655, F1: 0.706, AER: 0.294\n",
      "my_gd_gdfa prec: 0.611, rec: 0.693, F1: 0.649, AER: 0.351\n",
      "new1 prec: 0.807, rec: 0.611, F1: 0.695, AER: 0.304\n",
      "new_mygd prec: 0.761, rec: 0.658, F1: 0.706, AER: 0.294\n",
      "new_mygd_gdfa prec: 0.608, rec: 0.695, F1: 0.649, AER: 0.352\n",
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.784, rec: 0.752, F1: 0.768, AER: 0.231\n",
      "argmax prec: 0.939, rec: 0.674, F1: 0.785, AER: 0.212\n",
      "resnorm prec: 0.679, rec: 0.77, F1: 0.722, AER: 0.281\n",
      "itermax2-.9 prec: 0.851, rec: 0.726, F1: 0.784, AER: 0.214\n",
      "itermax2-.95 prec: 0.85, rec: 0.726, F1: 0.783, AER: 0.214\n",
      "itermax2-.8 prec: 0.853, rec: 0.726, F1: 0.784, AER: 0.213\n",
      "my_gd prec: 0.912, rec: 0.709, F1: 0.798, AER: 0.199\n",
      "my_gd_gdfa prec: 0.857, rec: 0.788, F1: 0.821, AER: 0.177\n",
      "new1 prec: 0.942, rec: 0.668, F1: 0.782, AER: 0.215\n",
      "new_mygd prec: 0.907, rec: 0.712, F1: 0.798, AER: 0.199\n",
      "new_mygd_gdfa prec: 0.853, rec: 0.79, F1: 0.82, AER: 0.178\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 90%|█████████ | 5998/6635 [15:47<01:30,  7.04it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.897, rec: 0.506, F1: 0.647, AER: 0.353\n",
      "gdfa prec: 0.733, rec: 0.671, F1: 0.701, AER: 0.299\n",
      "my_gdfa prec: 0.646, rec: 0.796, F1: 0.713, AER: 0.287\n",
      "argmax prec: 0.858, rec: 0.699, F1: 0.77, AER: 0.229\n",
      "resnorm prec: 0.609, rec: 0.791, F1: 0.688, AER: 0.312\n",
      "itermax2-.9 prec: 0.745, rec: 0.764, F1: 0.754, AER: 0.246\n",
      "itermax2-.95 prec: 0.743, rec: 0.763, F1: 0.753, AER: 0.247\n",
      "itermax2-.8 prec: 0.746, rec: 0.764, F1: 0.755, AER: 0.245\n",
      "my_gd prec: 0.831, rec: 0.738, F1: 0.782, AER: 0.218\n",
      "my_gd_gdfa prec: 0.733, rec: 0.789, F1: 0.76, AER: 0.24\n",
      "new1 prec: 0.874, rec: 0.696, F1: 0.775, AER: 0.225\n",
      "new_mygd prec: 0.829, rec: 0.741, F1: 0.783, AER: 0.218\n",
      "new_mygd_gdfa prec: 0.731, rec: 0.791, F1: 0.76, AER: 0.24\n",
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.512, rec: 0.749, F1: 0.608, AER: 0.392\n",
      "argmax prec: 0.783, rec: 0.608, F1: 0.684, AER: 0.316\n",
      "resnorm prec: 0.529, rec: 0.74, F1: 0.617, AER: 0.383\n",
      "itermax2-.9 prec: 0.649, rec: 0.695, F1: 0.671, AER: 0.329\n",
      "itermax2-.95 prec: 0.646, rec: 0.693, F1: 0.669, AER: 0.331\n",
      "itermax2-.8 prec: 0.651, rec: 0.696, F1: 0.673, AER: 0.327\n",
      "my_gd prec: 0.754, rec: 0.66, F1: 0.704, AER: 0.296\n",
      "my_gd_gdfa prec: 0.606, rec: 0.695, F1: 0.647, AER: 0.352\n",
      "new1 prec: 0.808, rec: 0.612, F1: 0.696, AER: 0.304\n",
      "new_mygd prec: 0.748, rec: 0.663, F1: 0.703, AER: 0.297\n",
      "new_mygd_gdfa prec: 0.603, rec: 0.698, F1: 0.647, AER: 0.353\n",
      "\n",
      "\n",
      "intersection prec: 0.971, rec: 0.521, F1: 0.678, AER: 0.319\n",
      "gdfa prec: 0.856, rec: 0.71, F1: 0.776, AER: 0.221\n",
      "my_gdfa prec: 0.788, rec: 0.759, F1: 0.773, AER: 0.226\n",
      "argmax prec: 0.944, rec: 0.674, F1: 0.786, AER: 0.211\n",
      "resnorm prec: 0.652, rec: 0.781, F1: 0.711, AER: 0.293\n",
      "itermax2-.9 prec: 0.853, rec: 0.728, F1: 0.786, AER: 0.212\n",
      "itermax2-.95 prec: 0.851, rec: 0.729, F1: 0.785, AER: 0.212\n",
      "itermax2-.8 prec: 0.853, rec: 0.727, F1: 0.785, AER: 0.212\n",
      "my_gd prec: 0.915, rec: 0.718, F1: 0.805, AER: 0.192\n",
      "my_gd_gdfa prec: 0.859, rec: 0.792, F1: 0.824, AER: 0.174\n",
      "new1 prec: 0.944, rec: 0.672, F1: 0.785, AER: 0.212\n",
      "new_mygd prec: 0.908, rec: 0.721, F1: 0.804, AER: 0.193\n",
      "new_mygd_gdfa prec: 0.854, rec: 0.794, F1: 0.823, AER: 0.176\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 6635/6635 [21:55<00:00,  5.04it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train loss: 3504449.5449852943\n",
      "cluster loss: 509.94970703125\n",
      "\n",
      "\n",
      "intersection prec: 0.818, rec: 0.268, F1: 0.404, AER: 0.596\n",
      "gdfa prec: 0.508, rec: 0.448, F1: 0.476, AER: 0.524\n",
      "my_gdfa prec: 0.516, rec: 0.752, F1: 0.612, AER: 0.388\n",
      "argmax prec: 0.784, rec: 0.612, F1: 0.687, AER: 0.313\n",
      "resnorm prec: 0.533, rec: 0.744, F1: 0.621, AER: 0.379\n",
      "itermax2-.9 prec: 0.655, rec: 0.7, F1: 0.677, AER: 0.324\n",
      "itermax2-.95 prec: 0.652, rec: 0.698, F1: 0.674, AER: 0.325\n",
      "itermax2-.8 prec: 0.657, rec: 0.7, F1: 0.678, AER: 0.322\n",
      "my_gd prec: 0.757, rec: 0.665, F1: 0.708, AER: 0.292\n",
      "my_gd_gdfa prec: 0.607, rec: 0.699, F1: 0.65, AER: 0.35\n",
      "new1 prec: 0.809, rec: 0.617, F1: 0.7, AER: 0.3\n",
      "new_mygd prec: 0.751, rec: 0.669, F1: 0.708, AER: 0.293\n",
      "new_mygd_gdfa prec: 0.604, rec: 0.703, F1: 0.65, AER: 0.351\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gnn_utils import eval_utils\n",
    "importlib.reload(eval_utils)\n",
    "\n",
    "clean_memory()\n",
    "eval_utils.alignment_test(epoch, heb_test_dataset.edge_index, editf_fin, editf_heb, heb_test_verses[:50], heb_test_dataset.nodes_map,\n",
    "                        dev, model, heb_test_dataset.x, pros_heb, surs_heb, heb_test_verse_alignments_inter, heb_test_verse_alignments_gdfa, writer, gnn_dataset_heb.verse_info)\n",
    "\n",
    "eval_utils.alignment_test(epoch, grc_test_dataset.edge_index, editf_fin, editf_grc, grc_test_verses[:250], grc_test_dataset.nodes_map,\n",
    "                        dev, model, grc_test_dataset.x, pros_grc, surs_grc, grc_test_verse_alignments_inter, grc_test_verse_alignments_gdfa, writer, gnn_dataset_grc.verse_info)\n",
    "\n",
    "eval_utils.alignment_test(epoch, blinker_test_dataset.edge_index, editf12, editf22, blinker_verses[:], blinker_test_dataset.nodes_map,\n",
    "                    dev, model, blinker_test_dataset.x, pros_blinker, surs_blinker, blinker_verse_alignments_inter, blinker_verse_alignments_gdfa, writer, gnn_dataset_blinker.verse_info)\n",
    "\n",
    "clean_memory()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "intersection prec: 0.858, rec: 0.272, F1: 0.413, AER: 0.587\n",
      "gdfa prec: 0.534, rec: 0.474, F1: 0.502, AER: 0.498\n",
      "my_gdfa prec: 0.521, rec: 0.763, F1: 0.619, AER: 0.381\n",
      "argmax prec: 0.806, rec: 0.634, F1: 0.71, AER: 0.29\n",
      "resnorm prec: 0.552, rec: 0.758, F1: 0.639, AER: 0.361\n",
      "itermax2-.9 prec: 0.642, rec: 0.707, F1: 0.673, AER: 0.327\n",
      "itermax2-.95 prec: 0.64, rec: 0.707, F1: 0.672, AER: 0.328\n",
      "itermax2-.8 prec: 0.645, rec: 0.71, F1: 0.676, AER: 0.324\n",
      "my_gd prec: 0.77, rec: 0.673, F1: 0.718, AER: 0.282\n",
      "my_gd_gdfa prec: 0.62, rec: 0.703, F1: 0.659, AER: 0.341\n",
      "new1 prec: 0.825, rec: 0.637, F1: 0.719, AER: 0.281\n",
      "new_mygd prec: 0.768, rec: 0.676, F1: 0.719, AER: 0.281\n",
      "new_mygd_gdfa prec: 0.619, rec: 0.706, F1: 0.66, AER: 0.34\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-ed465ebc264c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m eval_utils.alignment_test(epoch, grc_test_dataset.edge_index, editf_fin, editf_grc, grc_test_verses[:250], grc_test_dataset.nodes_map,\n\u001b[0;32m----> 9\u001b[0;31m                         dev, model, grc_test_dataset.x, pros_grc, surs_grc, grc_test_verse_alignments_inter, grc_test_verse_alignments_gdfa, writer, gnn_dataset_grc.verse_info)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m eval_utils.alignment_test(epoch, blinker_test_dataset.edge_index, editf12, editf22, blinker_verses[:], blinker_test_dataset.nodes_map,\n",
      "\u001b[0;32m~/Dokumente/code/pbc-ui-demo/gnn_utils/eval_utils.py\u001b[0m in \u001b[0;36malignment_test\u001b[0;34m(epoch, edge_index, editf1, editf2, verses, nodes_map, dev, model, x, pros, surs, alignments_inter, alignments_gdfa, writer, verses_info)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mverse_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mverse_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverse_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meditf2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mverse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mpred_aligns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_res\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverse_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meditf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meditf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_gdfa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgdfa_aligns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_inter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minter_aligns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mverse\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0;34m'GNN'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpred_aligns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'my_gd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GNN_GDFA'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpred_aligns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'my_gd_gdfa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dokumente/code/pbc-ui-demo/gnn_utils/eval_utils.py\u001b[0m in \u001b[0;36mcalc_res\u001b[0;34m(verse, verse_probs, nodes_map, editf1, editf2, align_gdfa, align_inter)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mmatrix_my_gd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmy_gd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverse_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_list_poses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_list_poses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mmatrix_mygd_gdfa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmy_gd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverse_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_list_poses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_list_poses\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mprev_gdfa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malign_gdfa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0mmatrix_gdfa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrow_diag_final_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverse_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dokumente/code/pbc-ui-demo/my_utils/align_utils.py\u001b[0m in \u001b[0;36mmy_gd\u001b[0;34m(sim_matrix, s_list_poses, t_list_poses, tresh, alignment, union, prev_gdfa, prev_inter)\u001b[0m\n\u001b[1;32m    220\u001b[0m                         if (\n\u001b[1;32m    221\u001b[0m                             \u001b[0mneighbor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munion\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                             \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0msim_fow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneighbor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mfow_tresh\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0msim_bac\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneighbor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbac_tresh\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m                             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_list_poses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me_new\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ms_list_poses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_list_poses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf_new\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_list_poses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                             \u001b[0;31m#and  (neighbor in prev_gdfa and (sim_fow[neighbor] > tresh or sim_bac[neighbor]>tresh or True))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# produce for uruba\n",
    "importlib.reload(eval_utils)\n",
    "editf_yor = 'yor-x-bible-2010'\n",
    "editf_others = ['eng-x-bible-mixed', 'deu-x-bible-newworld', 'ces-x-bible-newworld', 'fra-x-bible-louissegond', 'hin-x-bible-newworld',\n",
    "                'ita-x-bible-2009', 'prs-x-bible-goodnews', 'ron-x-bible-2006', 'spa-x-bible-newworld']\n",
    "\n",
    "def get_pruned_verse_alignments(args):\n",
    "    verse, current_editions = args\n",
    "    \n",
    "    #verse_aligns_inter = autils.get_verse_alignments(verse)\n",
    "    verse_aligns_gdfa = autils.get_verse_alignments(verse, gdfa=True)\n",
    "\n",
    "    #autils.prune_non_necessary_alignments(verse_aligns_inter, current_editions)\n",
    "    autils.prune_non_necessary_alignments(verse_aligns_gdfa, current_editions)\n",
    "\n",
    "    gc.collect()\n",
    "    return verse_aligns_gdfa\n",
    "    \n",
    "\n",
    "verse_alignments_gdfa = {}\n",
    "args = []\n",
    "editfs = editf_others[:]\n",
    "editfs.append(editf_yor)\n",
    "for i,verse in enumerate(train_verses):\n",
    "    args.append((verse, editfs))\n",
    "\n",
    "print('going to get alignments')\n",
    "with Pool(20) as p:\n",
    "    all_res = p.map(get_pruned_verse_alignments, args)\n",
    "\n",
    "for i,verse in enumerate(all_verses):\n",
    "    verse_aligns_gdfa = all_res[i]\n",
    "    \n",
    "    verse_alignments_gdfa[verse] = verse_aligns_gdfa\n",
    "\n",
    "\n",
    "\n",
    "for eidtf_t in editf_others:\n",
    "    print('going to align blinker ')\n",
    "    res = {}\n",
    "    verses = set(blinker_test_dataset.nodes_map[editf_yor].keys()).intersection(blinker_test_dataset.nodes_map[eidtf_t].keys())\n",
    "    res_ = eval_utils.alignment_test(epoch, blinker_test_dataset.edge_index, editf_yor, eidtf_t, list(verses), blinker_test_dataset.nodes_map,\n",
    "                    dev, model, blinker_test_dataset.x, pros_blinker, surs_blinker, blinker_verse_alignments_inter, blinker_verse_alignments_gdfa, writer, gnn_dataset_blinker.verse_info)\n",
    "    clean_memory()\n",
    "    res.update(res_)\n",
    "\n",
    "    print('going to align heb ')\n",
    "    verses = set(heb_test_dataset.nodes_map[editf_yor].keys()).intersection(heb_test_dataset.nodes_map[eidtf_t].keys())\n",
    "    res_ = eval_utils.alignment_test(epoch, heb_test_dataset.edge_index, editf_yor, eidtf_t, list(verses), heb_test_dataset.nodes_map,\n",
    "                    dev, model, heb_test_dataset.x, pros_heb, surs_heb, heb_test_verse_alignments_inter, heb_test_verse_alignments_gdfa, writer, gnn_dataset_heb.verse_info)\n",
    "    clean_memory()\n",
    "    res.update(res_)\n",
    "\n",
    "    print('going to align grc ')\n",
    "    verses = set(grc_test_dataset.nodes_map[editf_yor].keys()).intersection(grc_test_dataset.nodes_map[eidtf_t].keys())\n",
    "    res_ = eval_utils.alignment_test(epoch, grc_test_dataset.edge_index, editf_yor, eidtf_t, list(verses), grc_test_dataset.nodes_map,\n",
    "                    dev, model, grc_test_dataset.x, pros_grc, surs_grc, grc_test_verse_alignments_inter, grc_test_verse_alignments_gdfa, writer, gnn_dataset_grc.verse_info)\n",
    "    clean_memory()\n",
    "    res.update(res_)\n",
    "\n",
    "    print('going to align train ')\n",
    "    verses = set(train_dataset.nodes_map[editf_yor].keys()).intersection(train_dataset.nodes_map[eidtf_t].keys())\n",
    "    res_ = eval_utils.alignment_test(epoch, train_dataset.edge_index, editf_yor, eidtf_t, list(verses - set(masked_verses)), train_dataset.nodes_map,\n",
    "                    dev, model, train_dataset.x, pros, surs, verse_alignments_inter, verse_alignments_gdfa, writer, gnn_dataset_train.verse_info)\n",
    "    clean_memory()\n",
    "    res.update(res_)\n",
    "    \n",
    "    print(f'going save alignments for {eidtf_t}')\n",
    "    torch.save(res, f'/mounts/work/ayyoob/results/gnn_align/yoruba/{eidtf_t}_alignments.bin')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "going to gdfa alignments\n",
      "going to get alignments\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "global model, decoder\n",
    "#1/0\n",
    "\n",
    "decoder = None\n",
    "model = None\n",
    "\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "features = blinker_test_dataset.features[:]\n",
    "#features_edge = train_dataset.features_edge[:]\n",
    "from pprint import pprint\n",
    "#print('indim',in_dim)\n",
    "#features[-1].out_dim = 50\n",
    "for i in features:\n",
    "    #if i.type==3:\n",
    "    #    i.out_dim=4\n",
    "    print(vars(i))\n",
    "\n",
    "sum(p.out_dim for p in features)\n",
    "#train_dataset.features.pop()\n",
    "#train_dataset.features[0] = afeatures.OneHotFeature(20, 83, 'editf')\n",
    "#train_dataset.features[1] = afeatures.OneHotFeature(32, 150, 'position')\n",
    "#train_dataset.features[2] = afeatures.FloatFeature(4, 'degree_centrality')\n",
    "#train_dataset.features[3] = afeatures.FloatFeature(4, 'closeness_centrality')\n",
    "#train_dataset.features[4] = afeatures.FloatFeature(4, 'betweenness_centrality')\n",
    "#train_dataset.features[5] = afeatures.FloatFeature(4, 'load_centrality')\n",
    "#train_dataset.features[6] = afeatures.FloatFeature(4, 'harmonic_centrality')\n",
    "#train_dataset.features[7] = afeatures.OneHotFeature(32, 250, 'greedy_modularity_community')\n",
    "##train_dataset.features.append(afeatures.MappingFeature(100, 'word'))\n",
    "#torch.save(train_dataset, \"/mounts/work/ayyoob/models/gnn/dataset_helfi_train_community_word.pickle\")\n",
    "#torch.save(train_dataset.features[-3], \"./features.tmp\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# count number of deleted edges by each community detection method\n",
    "# from networkx.algorithms.community import greedy_modularity_communities, asyn_lpa_communities, label_propagation_communities, asyn_fluidc\n",
    "\n",
    "# tmp_verses = [all_verses[2]]\n",
    "# tmp_editions = small_editions[:10]\n",
    "# tmp_dataset, tmp_nodes_map = create_dataset(tmp_verses, verse_alignments_inter, tmp_editions)\n",
    "\n",
    "# tmp_g = pyg_utils.convert.to_networkx(tmp_dataset, to_undirected=True)\n",
    "# def count_deleted_edges(tmp_dataset, c):\n",
    "#     deleted_edges = 0\n",
    "#     for i in range(0, len(tmp_dataset.edge_index[0]), 2):\n",
    "#         for comp in c:\n",
    "#             if tmp_dataset.edge_index[0][i].item() in comp and tmp_dataset.edge_index[1][i].item() not in comp:\n",
    "#                 deleted_edges += 1\n",
    "    \n",
    "#     return deleted_edges\n",
    "\n",
    "# print(\"eng token count: \", tmp_nodes_map['eng-x-bible-mixed'][tmp_verses[0]])\n",
    "# print(\"original connected components\",nx.number_connected_components(tmp_g))\n",
    "\n",
    "# c = list(greedy_modularity_communities(tmp_g))\n",
    "# print(\"new connected_components\", len(c))\n",
    "# print(\"deleted edges: \", count_deleted_edges(tmp_dataset, c))\n",
    "\n",
    "# c = list(asyn_lpa_communities(tmp_g))\n",
    "# print(\"asyn_lpa_communities number of components\", len(c))\n",
    "# print(\"deleted edges: \", count_deleted_edges(tmp_dataset, c))\n",
    "\n",
    "# c = list(label_propagation_communities(tmp_g))\n",
    "# print(\"label_propagation_communities number of components\", len(c))\n",
    "# print(\"deleted edges: \", count_deleted_edges(tmp_dataset, c))\n",
    "\n",
    "# cents = nx.edge_betweenness_centrality(tmp_g)\n",
    "# vals = sorted(list(cents.values()))\n",
    "# print(vals[0], vals[10], vals[100], vals[1000], vals[2000], vals[3000], vals[10000])\n",
    "# print(vals[-1], vals[-10], vals[-100], vals[-1000], vals[-2000], vals[-3000], vals[-10000])\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# measure different community detection algorithms\n",
    "# from networkx.algorithms.community import greedy_modularity_communities, asyn_lpa_communities, label_propagation_communities, asyn_fluidc\n",
    "\n",
    "# def remove_bad_community_edges(nodes_map, verses, edition_files, alignments):\n",
    "#     edges_tmp = [[],[]]\n",
    "#     res_edges = [[],[]]\n",
    "#     for verse in verses:\n",
    "#         utils.LOG.info(f\"extracting edge features for {verse}\")\n",
    "#         for i,editf1 in enumerate(edition_files):\n",
    "#             for j,editf2 in enumerate(edition_files[i+1:]):\n",
    "#                 aligns = autils.get_aligns(editf1, editf2, alignments[verse])\n",
    "#                 if aligns != None:\n",
    "#                     for align in aligns:\n",
    "#                         n1, node_count = node_nom(verse, editf1, align[0], 0, nodes_map, None, None)\n",
    "#                         n2, node_count = node_nom(verse, editf2, align[1], 0, nodes_map, None, None)\n",
    "#                         edges_tmp[0].extend([n1, n2])\n",
    "#                         edges_tmp[1].extend([n2, n1])\n",
    "\n",
    "#         gnx = convert_to_netx(edges_tmp)\n",
    "#         print('detecting communities')\n",
    "#         coms = greedy_modularity_communities(gnx)\n",
    "\n",
    "#         print('finding good edges')\n",
    "#         for i in range(0, len(edges_tmp[0]), 2):\n",
    "#             for c in coms:\n",
    "#                 if edges_tmp[0][i] in c and edges_tmp[0][i+1] in c:\n",
    "#                     res_edges[0].extend([edges_tmp[0][i], edges_tmp[0][i+1]])\n",
    "#                     res_edges[1].extend([edges_tmp[0][i+1], edges_tmp[0][i]])\n",
    "#         edges_tmp = [[],[]]\n",
    "#     print('to keep edges:', len(res_edges[0]))\n",
    "#     return torch.tensor(res_edges, dtype=torch.long)\n",
    "\n",
    "# # old_edge_index = train_dataset.edge_index\n",
    "# # new_edge_index = remove_bad_community_edges(train_dataset.nodes_map, train_verses, small_editions, verse_alignments_inter)\n",
    "# # train_dataset.edge_index = new_edge_index\n",
    "\n",
    "# # with open(\"./dataset_greedy_modularity_communities.pickle\", 'rb') as inf:\n",
    "# #     train_dataset = pickle.load(inf)\n",
    "\n",
    "# test_dataset = train_dataset\n",
    "\n",
    "# print('orig edge count', old_edge_index.shape)\n",
    "# print('new edge count', train_dataset.edge_index.shape)\n",
    "# print(\"done\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nodes_map = train_dataset.nodes_map\n",
    "bad_edition_files = []\n",
    "for edit in nodes_map:\n",
    "    bad_count = 0\n",
    "    for verse in nodes_map[edit]:\n",
    "        if len(nodes_map[edit][verse].keys()) < 2:\n",
    "            bad_count += 1\n",
    "        if bad_count > 1:\n",
    "            bad_edition_files.append(edit)\n",
    "            break\n",
    "print(bad_edition_files)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_japanese_nodes = set()\n",
    "nodes_map = train_dataset.nodes_map\n",
    "\n",
    "for bad_editionf in bad_edition_files:\n",
    "    for verse in nodes_map[bad_editionf]:\n",
    "        for item in nodes_map[bad_editionf][verse].items():\n",
    "            all_japanese_nodes.add(item[1])\n",
    "\n",
    "print(\" all japansese nodes: \", len(all_japanese_nodes))\n",
    "edge_index = train_dataset.edge_index.to('cpu')\n",
    "remaining_edges_index = []\n",
    "for i in tqdm(range(0, edge_index.shape[1], 2)):\n",
    "    if edge_index[0, i].item() not in all_japanese_nodes and edge_index[0, i+1].item() not in all_japanese_nodes:\n",
    "        remaining_edges_index.extend([i, i+1])\n",
    "\n",
    "print('original total edges count', edge_index.shape)\n",
    "print('remaining edge count', len(remaining_edges_index))\n",
    "train_dataset.edge_index = edge_index[:, remaining_edges_index]\n",
    "train_dataset.edge_index.shape\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from networkx.algorithms.community import greedy_modularity_communities, asyn_lpa_communities, label_propagation_communities, asyn_fluidc\n",
    "\n",
    "def get_community_edges(c, verse):\n",
    "    res = []\n",
    "    for n1 in all_nodes_map[editf1][verse].items():\n",
    "        for n2 in all_nodes_map[editf2][verse].items():\n",
    "            for com in c:\n",
    "                if n1[1] in com and n2[1] in com:\n",
    "                    res.append((n1[0], n2[0]))\n",
    "    \n",
    "    return res\n",
    "\n",
    "def intersect(e1, e2):\n",
    "    res = set()\n",
    "    for item in e1:\n",
    "        if item in e2:\n",
    "            res.add(item)\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "measures = {}\n",
    "measures['intersection']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "measures['gdfa']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "measures['c1_all']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "measures['c1_inter']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "measures['c2_all']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "measures['c2_inter']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "measures['c3_all']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "measures['c3_inter']= {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "numbers = [100, 300, 600, 1000, 1300, 1600, 2000, 3000, 4000, 5000]\n",
    "for i in numbers:\n",
    "    measures[i] = {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "    measures[i+1] = {\"p_hit_count\": 0, \"s_hit_count\": 0, \"total_hit_count\": 0, \"gold_s_hit_count\": 0, \"prec\": 0, \"rec\": 0, \"f1\": 0, \"aer\": 0}\n",
    "coms = {}\n",
    "for verse in test_verses:\n",
    "    inter_edges = autils.get_aligns(editf1, editf2, verse_alignments_inter[verse])\n",
    "\n",
    "    all_dataset, all_nodes_map = create_dataset([verse], verse_alignments_inter, small_editions)\n",
    "    print(\"converting\")\n",
    "    g = pyg_utils.convert.to_networkx(all_dataset, to_undirected=True)\n",
    "    print(\"detecting community 1\")\n",
    "    c1 = list(greedy_modularity_communities(g))\n",
    "    print(\"detecting community 2\")\n",
    "    c2 = list(asyn_lpa_communities(g))\n",
    "    print(\"detecting community 3\")\n",
    "    c3 = list(label_propagation_communities(g))\n",
    "\n",
    "    print(\"detecting community 4\")\n",
    "    edge_betweenness = [y[0] for y in sorted(nx.edge_betweenness_centrality(g).items(), key=lambda x: x[1], reverse=True)]\n",
    "    \n",
    "    print('orig communities', nx.number_connected_components(g))\n",
    "    prev_i = 0\n",
    "    for i in numbers:\n",
    "        for j in range(prev_i, i):\n",
    "            g.remove_edge(*edge_betweenness[j])\n",
    "        prev_i = i\n",
    "        com = list(nx.connected_components(g))\n",
    "        edges = get_community_edges(com, verse)\n",
    "        autils.calc_and_update_alignment_score(edges, pros[verse], surs[verse], measures[i])\n",
    "        autils.calc_and_update_alignment_score(intersect(edges, inter_edges), pros[verse], surs[verse], measures[i+1]) \n",
    "        print(f'communities {i}', nx.number_connected_components(g))\n",
    "\n",
    "    c1_edges = get_community_edges(c1, verse)\n",
    "    c2_edges = get_community_edges(c2, verse)\n",
    "    c3_edges = get_community_edges(c3, verse)\n",
    "    print('c1 communities', len(c1))\n",
    "    print('c2 communities', len(c2))\n",
    "    print('c3 communities', len(c3))\n",
    "\n",
    "\n",
    "\n",
    "    autils.calc_and_update_alignment_score(inter_edges, pros[verse], surs[verse], measures['intersection'])\n",
    "    autils.calc_and_update_alignment_score(autils.get_aligns(editf1, editf2, verse_alignments_gdfa[verse]), pros[verse], surs[verse], measures['gdfa'])\n",
    "\n",
    "    autils.calc_and_update_alignment_score(c1_edges, pros[verse], surs[verse], measures['c1_all'])\n",
    "    autils.calc_and_update_alignment_score(c2_edges, pros[verse], surs[verse], measures['c2_all'])\n",
    "    autils.calc_and_update_alignment_score(c3_edges, pros[verse], surs[verse], measures['c3_all'])\n",
    "\n",
    "    autils.calc_and_update_alignment_score(intersect(c1_edges, inter_edges), pros[verse], surs[verse], measures['c1_inter'])\n",
    "    autils.calc_and_update_alignment_score(intersect(c2_edges, inter_edges), pros[verse], surs[verse], measures['c2_inter'])\n",
    "    autils.calc_and_update_alignment_score(intersect(c3_edges, inter_edges), pros[verse], surs[verse], measures['c3_inter'])\n",
    "\n",
    "    for item in measures:\n",
    "        print(item, measures[item])\n",
    "    \n",
    "    \n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}